{"questions": {"fd4bdbd0-b628-4ae7-987d-33c92fd319c2": "What are some key elements that panelists believe are necessary for the governance of technologies to protect against potential harms?", "5ab071ed-0759-4ee0-ad5d-24baaf7d6e0d": "How do panelists suggest ensuring that technological use cases are relevant and effective for their intended goals?", "a9ec15d8-8a45-47e2-a562-aa1afee80094": "What are the key characteristics of trustworthiness that the NIST framework aims to address in AI products and services?", "eb7536a8-6916-4f11-bd1b-cf50e84118b2": "How is the NIST framework being developed to ensure transparency and collaboration in its creation?", "4bac333f-8319-4287-87d9-1153aa7d1b28": "What measures have federal government agencies been developing to prevent bias in automated systems?", "2ac3d3c6-8f85-4ddb-8c5f-e98010d73740": "Why is it important to include digital lives in the safeguards against algorithmic discrimination?", "ad93c0a3-b882-4283-8f78-a86db484c3d4": "What types of Indigenous communities are mentioned in the context regarding organizational ties?", "f0aeaa8c-2ba9-4872-8ea4-874b61e696f7": "How does the Blueprint for an AI Bill of Rights propose to address the harms of automated systems?", "6da3578c-9dcc-4893-b9e5-c20bee16679e": "What is the focus of the National Science Foundation's program mentioned in the context?", "e0c7eec8-0a80-49e3-b03d-4a01053cf722": "How does automatic signature verification software potentially impact U.S. voters according to the provided articles?", "9fb7208c-62c9-4923-a106-f7c7fdfe4785": "What are the five principles identified by the White House Office of Science and Technology Policy to guide the design and use of automated systems?", "b63d18bb-b4cc-4b9e-ac1b-1036891d2e97": "How does the Blueprint for an AI Bill of Rights aim to protect the American public from threats posed by artificial intelligence?", "3866f7a5-d728-47bb-b489-bc2e411f1eae": "What are some activities that can be performed using AI in organizational settings or the public domain?", "e63f4751-39ae-4160-9691-107c09fa44cd": "What governance tools and protocols can be applied to GAI systems according to the provided context?", "24730d48-a402-4ddb-a784-2f81c8f152a1": "How do strong safety regulations impact innovation in the context of complex technologies like motor vehicles?", "e8d561c3-070f-4cc3-a3a0-fe3fff40c897": "What role does the National Highway Traffic Safety Administration play in ensuring vehicle safety while allowing for manufacturer innovation?", "e47098c0-5ca2-47d1-9d70-a57338168552": "What types of defenses are mentioned in relation to enforceability against the United States?", "07cff927-38a3-4be5-9d1f-c9900de290d6": "Does the document constitute a waiver of sovereign immunity?", "0534c3ba-7306-4fb7-a39d-6ae31ff3f6ba": "What are the key components of the training process for the pre-trained model, including hyperparameters and training duration?", "238d500c-df74-4c22-a9c2-fa20ccc1516e": "How do content filters function to prevent the generation of inappropriate or harmful content in GAI applications?", "aa19ddb4-af38-42bc-904c-72456883cede": "What criteria should be used to determine the level of explanation provided by a system based on risk level?", "44b40ddc-f911-4cc1-bb02-a27cfc26a46b": "How should simplifications in the explanation of a system's decision-making be approached to ensure they are scientifically supportable?", "29dffbef-61f4-4061-9a3d-680c51166bda": "What themes were explored during the panel discussions regarding automated systems and their impact on society?", "ab6e13c2-dd07-473d-a048-c1602aa7af82": "How did the panel discussion on Consumer Rights and Protections address the challenges faced by individuals in an AI-enabled ecosystem?", "407f78de-6e5c-4c71-a0cb-24d14d6d9646": "What is the purpose of verifying the GAI system training data and TEVV data provenance according to MS-2.5-005?", "020bf608-9a7a-49da-9efc-4cd499a04d62": "Why is it important to regularly review security and safety guardrails for the GAI system as stated in MS-2.5-006?", "33ae2d79-5ff6-4762-aeea-1b7c9a61fb6b": "What are the implications of surveillance technologies on the rights and opportunities of underserved communities?", "75b65d1a-21ba-434b-8d96-050d748bbe48": "How does the definition of \"underserved communities\" relate to the impact of government and commercial surveillance?", "bad2b413-54eb-4c13-8edd-ab07bf683225": "What are the key components that should be included in the ongoing monitoring procedures for automated systems?", "918e6f86-db8c-4cef-a967-931a1e200462": "Why is it important to have fallback mechanisms in place for automated systems?", "3e5d4444-e720-4d82-a077-344097f50d66": "What is the purpose of the National Artificial Intelligence Initiative Office's public input request regarding biometric technologies?", "080cc9fe-d96c-4547-89a0-b4a96df54802": "Who are the authors of the synopsis of responses to the OSTP\u2019s request for information on biometric technologies?", "3f4acee0-7bdc-4fd6-b6cf-9c6ee08d43ec": "What approaches can be used to assess algorithmic discrimination in automated systems?", "db4e3cb2-31b2-4dc1-bd30-3bccc28f8609": "How often should riskier and higher-impact systems be monitored for algorithmic discrimination?", "b732ca88-de42-4a7a-be0a-8b9650c52060": "What are the key areas of focus in enhancing techniques for GAI development to ensure data privacy and integrity?", "51be062d-c176-4c80-8d1c-10fc823ef024": "How should organizations respond to and recover from previously unknown risks associated with GAI systems?", "0297ed84-8edb-463a-9744-abdf18b4db93": "What is the focus of the paper by Padmakumar et al. (2024) regarding language models and content diversity?", "37dc9345-b042-4f97-b2c8-c77a40e99c9b": "What are the main topics covered in the survey by Park et al. (2024) on AI deception?", "3e865b5c-c660-41a0-ac1a-4ebcfd2c3654": "What are the key components that should be monitored in the performance of automated systems?", "d8cee951-6d5b-4919-9c47-387eed78c367": "Why is it important to have manual, human-led monitoring in addition to automated monitoring systems?", "dd86b3bd-5333-48cb-b614-c51cf21f9f75": "What is the deadline for the implementation of biometric identifying technology in schools according to the law?", "5b3153a2-ba52-4cbd-9fe7-204f2fa7bf1c": "What must employers report when engaging in workplace surveillance related to a labor dispute?", "18e6d82d-0788-4bad-a178-cc48c2f1562c": "What should entities allow regarding the withdrawal of data access consent and the deletion of user data?", "ebb28ebc-d167-4d55-a346-fa3781b23b69": "What capabilities should entities establish for individuals to manage consent, access, and control decisions in automated systems?", "ba3f4dc7-6583-4ff5-9afb-9012089b4fd7": "What role do healthcare navigators play in assisting consumers with health coverage options?", "d2b6f193-c41e-4bc9-93a9-d10a9d6f9796": "How did the Biden-Harris Administration support the training and certification of healthcare navigators for the 2022 plan year?", "aa986d3b-c193-49ca-9b37-8ccd0ef435db": "What are some of the key risks associated with using AI in high-stakes settings as emphasized by the panelists?", "5bc514f6-21e5-45fc-86a5-3dcc819a0b25": "What interventions and key needs did the panelists suggest for the future design of critical AI systems?", "cdfb6235-a046-43e6-adde-de769b57da7b": "What are the nine principles that federal agencies must adhere to when using AI according to Executive Order 13960?", "db3365a1-7e85-4f0e-9358-eecc8eda81a8": "How does Executive Order 13960 address the use of AI in sensitive law enforcement contexts compared to private sector use?", "27f9b1a1-5392-4478-950d-fa62b0521d31": "What should the documentation of the system include to ensure it is understandable to the public?", "a725b499-1731-4c80-911a-c1121ba394bc": "How should users be informed about the use of automated systems according to the context provided?", "c5474e88-7992-4f84-a17c-1c03b6673df7": "What are the requirements for lenders regarding notice and explanation to consumers in the context of credit?", "c87bcd24-c8d2-4174-a430-55ed4c738f25": "How are innovative companies and researchers addressing the need for better explanations of automated systems that impact consumers?", "ba74a626-060a-4a64-878d-fab00fc2c2d1": "What methods are suggested for assessing the accuracy and reliability of GAI output?", "c7bef1c4-d361-4b87-89a3-c79f2aa06d8c": "Why is it important to compare GAI output to a set of known ground truth data?", "25237588-69a6-41dd-8561-b933c9a20b89": "What are some potential consequences of incorrect presumptions about performance in AI systems?", "0f1b03c0-33c1-4871-8aa5-3d47e078f6c5": "How can human-AI configuration lead to emotional entanglement with generative AI systems?", "1783fd8b-9c51-4de2-9e2e-e4392c811407": "What should be considered as a performance baseline for the algorithm before deployment?", "c3197cde-04b1-4559-b1b3-b9a0eed0ad53": "What types of risks should be identified and mitigated before deploying the automated system?", "068721c8-fbf0-4e9b-aed3-74fb35523fdd": "What criteria should be used to determine the appropriateness of opting out of automated systems in favor of human alternatives?", "698176c3-9239-4c25-97cf-6c6735813192": "What are the key features that human consideration and fallback processes should possess to ensure effectiveness and accessibility?", "2bb6971d-4ccd-4a04-a315-f3b58968bfe1": "What mechanisms are suggested to monitor the effectiveness of risk controls and mitigation plans in information security?", "b83aa106-7b5f-4b42-b154-77849e407150": "How should organizations assess the outputs of GAI systems in relation to their risk tolerance and guidelines?", "d417d84b-7a9d-48a9-8c1a-b752703b4300": "What is the purpose of the Blueprint for an AI Bill of Rights as mentioned in the context?", "be207439-5677-4a75-a522-4f628caf6059": "How does the Technical Companion relate to the development of technical standards and practices in various sectors?", "ee33d792-b391-44d3-b958-599437ae0358": "What are the expectations for automated systems that handle sensitive data according to the context provided?", "3b7c48a6-c5aa-4639-9696-878247589308": "Why is it important for the American public to have assurances regarding the protection and use of sensitive data?", "7cc6cfb6-87d0-4054-9c7c-33c161d41169": "What types of entities are required to provide public reports regarding data security lapses or breaches related to sensitive data?", "39e21754-b568-43a8-8116-3b4404c645c7": "What information should be included in the public reports concerning the handling of sensitive data?", "e7bc84af-3891-48e9-bf97-726f1fba46b9": "What are some examples of content types where creative generation of non-factual content can be a desired behavior?", "b1580adf-6240-48b8-85ac-f45ad3f587ce": "How do legal confabulations manifest in current state-of-the-art language models?", "e151225f-5aa9-4821-a1bf-326937e08e1c": "What challenges are associated with providing explanations for the decision-making processes of automated systems?", "a7857d9b-8c9d-4342-8dcc-27df927ea1f8": "Why is it important to provide clear and valid explanations for decisions made by automated systems?", "4ff54218-d098-4b79-b7fe-e4a9b6382b55": "What methods did the White House Office of Science and Technology Policy use to gather input from the public on algorithmic and data-driven harms?", "9d70d5af-dfe6-4f31-b332-2ac4f4f5d03f": "Who were the key stakeholders involved in the yearlong process led by the OSTP regarding the Blueprint for an AI Bill of Rights?", "10c22a14-1d0f-4551-a12d-ffe323547624": "How do sociodemographic variables in healthcare clinical algorithms potentially contribute to race-based health inequities?", "07799984-0b23-418e-bd7b-e94ec5266d3c": "What role do physicians play in utilizing clinical algorithms that may include adjustments based on a patient's race or ethnicity?", "0f463179-3fd1-4998-9d6d-ade8ec477894": "What are the main privacy protection measures mentioned in Apple's guidelines?", "e5d52e03-7469-49df-a03d-70691210e26e": "How do algorithms contribute to poverty, according to Karen Hao's article in MIT Tech Review?", "30c2fc7e-76e5-44dd-9f1a-d73ee7ee6850": "What are the main purposes of the Privacy Protection Act of 1998 and the Confidential Information Protection and Statistical Efficiency Act (CIPSEA)?", "59a3d01f-114d-4f0a-90fb-847ac6b349e2": "How have cheating-detection companies profited during the pandemic according to Drew Harwell's article?", "b0cafe6b-068c-4235-9a49-a27726b6f369": "What factors can influence the likelihood of risks materializing in a given context?", "b7172b7e-71e2-454b-b671-3eb28e051a5e": "How do GAI risks differ from traditional software risks?", "f4805d01-7059-4438-9eb2-5057539eda71": "What are the anticipated environmental impacts that should be documented during model development, maintenance, and deployment in product design decisions?", "f3093f20-9daf-4135-bbfd-ff6d9ace169a": "How can the effectiveness of carbon capture or offset programs for GAI training and applications be verified?", "69e0567e-faa4-49dd-868d-31900f9feba1": "What are the key principles outlined in the Privacy Act of 1974 regarding data retention and individual access to personal information?", "af764b10-4378-43e4-ab54-fb7794c96878": "How does the Privacy Act of 1974 illustrate the principle of limiting the scope of data retention in federal records systems?", "c7a5ecc6-0ca1-4bc7-aefe-312469f80327": "What are the known and potential vulnerabilities associated with GAI systems as outlined in the context?", "2ac0f3d5-0865-4772-83b1-681037efbdf7": "How do GAI systems impact affected individuals and communities according to the information security framework?", "24a34cc0-0b1f-4b8d-bf8a-8ddf4e04112b": "What are some proactive measures suggested to protect individuals and communities from algorithmic discrimination?", "5d9a4630-cbb1-4452-99dd-e6b37e78a8cc": "Why is it important to conduct independent evaluations and algorithmic impact assessments in the context of system design?", "b440546c-8275-4852-a093-16c80d9331fd": "What types of data are used to train machine learning models within the organization, and how are these data sources processed and interpreted?", "ff40a976-42c3-441a-a193-a4dafdf91350": "How does the organization identify and manage risks associated with its business processes, and what steps are taken to mitigate potential harms?", "6e6dae6b-23af-491a-808e-fbd5bf05f5e4": "What are the potential risks associated with generative AI (GAI) in the context of information security?", "8bbaefb2-af59-4d11-99e7-e389cccd8a5a": "How can generative AI models impact the creation of fraudulent content on social media platforms?", "bc6ca361-c4d9-4c44-be28-487c9cbb46d8": "What is the focus of the National Institute of Standards and Technology's AI Risk Management Framework as outlined in Appendix B?", "12d89dd9-2572-4405-85dc-0aaa6a04c67d": "Where can one find the glossary of terms related to trustworthy AI provided by the National Institute of Standards and Technology?", "d4b712d1-69a5-43e8-9972-7cf75a17080c": "How can organizations maximize the utility of provenance data in their risk management efforts?", "9ef86bd5-aa0c-400d-a0a1-df9434893a5b": "What role does direct input from end users play in enhancing content provenance compared to automated error collection systems?"}, "relevant_contexts": {"fd4bdbd0-b628-4ae7-987d-33c92fd319c2": ["560c316b-b85c-4908-900b-114508dbeda1"], "5ab071ed-0759-4ee0-ad5d-24baaf7d6e0d": ["560c316b-b85c-4908-900b-114508dbeda1"], "a9ec15d8-8a45-47e2-a562-aa1afee80094": ["68a8cb79-b085-4aa7-a822-7a6fe91efaf5"], "eb7536a8-6916-4f11-bd1b-cf50e84118b2": ["68a8cb79-b085-4aa7-a822-7a6fe91efaf5"], "4bac333f-8319-4287-87d9-1153aa7d1b28": ["89e3e37f-a555-47a5-9d99-c4c12c871a74"], "2ac3d3c6-8f85-4ddb-8c5f-e98010d73740": ["89e3e37f-a555-47a5-9d99-c4c12c871a74"], "ad93c0a3-b882-4283-8f78-a86db484c3d4": ["4877eae0-98c2-405d-a609-e2b4b6b043b8"], "f0aeaa8c-2ba9-4872-8ea4-874b61e696f7": ["4877eae0-98c2-405d-a609-e2b4b6b043b8"], "6da3578c-9dcc-4893-b9e5-c20bee16679e": ["2fd912b7-48c5-4d32-b453-ef5b22179648"], "e0c7eec8-0a80-49e3-b03d-4a01053cf722": ["2fd912b7-48c5-4d32-b453-ef5b22179648"], "9fb7208c-62c9-4923-a106-f7c7fdfe4785": ["f4b531ac-2549-4da9-9756-0bdf32a95008"], "b63d18bb-b4cc-4b9e-ac1b-1036891d2e97": ["f4b531ac-2549-4da9-9756-0bdf32a95008"], "3866f7a5-d728-47bb-b489-bc2e411f1eae": ["ad3277f6-9b01-4a15-85ee-0547b6cdef81"], "e63f4751-39ae-4160-9691-107c09fa44cd": ["ad3277f6-9b01-4a15-85ee-0547b6cdef81"], "24730d48-a402-4ddb-a784-2f81c8f152a1": ["7a97c9a4-8b02-4940-b7ad-dccb212bceff"], "e8d561c3-070f-4cc3-a3a0-fe3fff40c897": ["7a97c9a4-8b02-4940-b7ad-dccb212bceff"], "e47098c0-5ca2-47d1-9d70-a57338168552": ["20b4ecee-f877-41bf-94e6-6f7d2bdcdd95"], "07cff927-38a3-4be5-9d1f-c9900de290d6": ["20b4ecee-f877-41bf-94e6-6f7d2bdcdd95"], "0534c3ba-7306-4fb7-a39d-6ae31ff3f6ba": ["fede1846-3a31-46de-acc5-944032788823"], "238d500c-df74-4c22-a9c2-fa20ccc1516e": ["fede1846-3a31-46de-acc5-944032788823"], "aa19ddb4-af38-42bc-904c-72456883cede": ["7c98ce98-ef06-45d6-b674-b51ce70d0b0b"], "44b40ddc-f911-4cc1-bb02-a27cfc26a46b": ["7c98ce98-ef06-45d6-b674-b51ce70d0b0b"], "29dffbef-61f4-4061-9a3d-680c51166bda": ["1e320b68-4737-4d89-be43-8b9ff8bcfc7f"], "ab6e13c2-dd07-473d-a048-c1602aa7af82": ["1e320b68-4737-4d89-be43-8b9ff8bcfc7f"], "407f78de-6e5c-4c71-a0cb-24d14d6d9646": ["85df67c4-01eb-4daf-b75c-803d4f9f7e2c"], "020bf608-9a7a-49da-9efc-4cd499a04d62": ["85df67c4-01eb-4daf-b75c-803d4f9f7e2c"], "33ae2d79-5ff6-4762-aeea-1b7c9a61fb6b": ["fabb8048-84be-4544-b4bb-69c6353bb66c"], "75b65d1a-21ba-434b-8d96-050d748bbe48": ["fabb8048-84be-4544-b4bb-69c6353bb66c"], "bad2b413-54eb-4c13-8edd-ab07bf683225": ["1e82d8d2-9fb4-45ab-a88a-315f048f4082"], "918e6f86-db8c-4cef-a967-931a1e200462": ["1e82d8d2-9fb4-45ab-a88a-315f048f4082"], "3e5d4444-e720-4d82-a077-344097f50d66": ["bf9159b4-2816-403b-aada-24bf8412ddb5"], "080cc9fe-d96c-4547-89a0-b4a96df54802": ["bf9159b4-2816-403b-aada-24bf8412ddb5"], "3f4acee0-7bdc-4fd6-b6cf-9c6ee08d43ec": ["006e62b4-5fd5-453d-ac13-990e5ef4ce33"], "db4e3cb2-31b2-4dc1-bd30-3bccc28f8609": ["006e62b4-5fd5-453d-ac13-990e5ef4ce33"], "b732ca88-de42-4a7a-be0a-8b9650c52060": ["6807109d-3853-473c-8ba1-21c9f0aec268"], "51be062d-c176-4c80-8d1c-10fc823ef024": ["6807109d-3853-473c-8ba1-21c9f0aec268"], "0297ed84-8edb-463a-9744-abdf18b4db93": ["d3d96f8c-38e5-458e-9c1c-371ca2c181ac"], "37dc9345-b042-4f97-b2c8-c77a40e99c9b": ["d3d96f8c-38e5-458e-9c1c-371ca2c181ac"], "3e865b5c-c660-41a0-ac1a-4ebcfd2c3654": ["657e6e3d-cd4b-4528-ac1d-50d0f1a8ac2c"], "d8cee951-6d5b-4919-9c47-387eed78c367": ["657e6e3d-cd4b-4528-ac1d-50d0f1a8ac2c"], "dd86b3bd-5333-48cb-b614-c51cf21f9f75": ["a5db6001-ac93-4b97-afa0-6b170643ffeb"], "5b3153a2-ba52-4cbd-9fe7-204f2fa7bf1c": ["a5db6001-ac93-4b97-afa0-6b170643ffeb"], "18e6d82d-0788-4bad-a178-cc48c2f1562c": ["347d53c5-b382-4a95-b296-5727a3c2ce38"], "ebb28ebc-d167-4d55-a346-fa3781b23b69": ["347d53c5-b382-4a95-b296-5727a3c2ce38"], "ba3f4dc7-6583-4ff5-9afb-9012089b4fd7": ["1ceaf856-b64a-470b-9836-f07d99a08ace"], "d2b6f193-c41e-4bc9-93a9-d10a9d6f9796": ["1ceaf856-b64a-470b-9836-f07d99a08ace"], "aa986d3b-c193-49ca-9b37-8ccd0ef435db": ["c644be03-0039-4466-aab2-6432319ac6e5"], "5bc514f6-21e5-45fc-86a5-3dcc819a0b25": ["c644be03-0039-4466-aab2-6432319ac6e5"], "cdfb6235-a046-43e6-adde-de769b57da7b": ["2fcfbe54-2897-4e2e-8da2-4b9089f68683"], "db3365a1-7e85-4f0e-9358-eecc8eda81a8": ["2fcfbe54-2897-4e2e-8da2-4b9089f68683"], "27f9b1a1-5392-4478-950d-fa62b0521d31": ["868816e4-73ce-4e0c-93bb-25a6cc6e6ced"], "a725b499-1731-4c80-911a-c1121ba394bc": ["868816e4-73ce-4e0c-93bb-25a6cc6e6ced"], "c5474e88-7992-4f84-a17c-1c03b6673df7": ["9d03e4e9-9969-4a03-9d62-1b7bf051fe41"], "c87bcd24-c8d2-4174-a430-55ed4c738f25": ["9d03e4e9-9969-4a03-9d62-1b7bf051fe41"], "ba74a626-060a-4a64-878d-fab00fc2c2d1": ["5d5280b8-c456-4b8e-bb23-2c18d489c7b0"], "c7bef1c4-d361-4b87-89a3-c79f2aa06d8c": ["5d5280b8-c456-4b8e-bb23-2c18d489c7b0"], "25237588-69a6-41dd-8561-b933c9a20b89": ["c6c5d608-62a3-4472-abb7-60eabfdbb58f"], "0f1b03c0-33c1-4871-8aa5-3d47e078f6c5": ["c6c5d608-62a3-4472-abb7-60eabfdbb58f"], "1783fd8b-9c51-4de2-9e2e-e4392c811407": ["e9550b70-6ec3-442c-8821-0c3586e8af26"], "c3197cde-04b1-4559-b1b3-b9a0eed0ad53": ["e9550b70-6ec3-442c-8821-0c3586e8af26"], "068721c8-fbf0-4e9b-aed3-74fb35523fdd": ["580f4522-df99-42bc-b50e-1982319b9c07"], "698176c3-9239-4c25-97cf-6c6735813192": ["580f4522-df99-42bc-b50e-1982319b9c07"], "2bb6971d-4ccd-4a04-a315-f3b58968bfe1": ["78deb994-1b18-4b76-a735-1fc150351f17"], "b83aa106-7b5f-4b42-b154-77849e407150": ["78deb994-1b18-4b76-a735-1fc150351f17"], "d417d84b-7a9d-48a9-8c1a-b752703b4300": ["b26f8a5b-2e2d-4b86-bcb7-6e43390bb816"], "be207439-5677-4a75-a522-4f628caf6059": ["b26f8a5b-2e2d-4b86-bcb7-6e43390bb816"], "ee33d792-b391-44d3-b958-599437ae0358": ["d51d3101-2ad2-4f77-aa6e-91a7055fc090"], "3b7c48a6-c5aa-4639-9696-878247589308": ["d51d3101-2ad2-4f77-aa6e-91a7055fc090"], "7cc6cfb6-87d0-4054-9c7c-33c161d41169": ["33383e76-a128-4724-8046-211f375ddb92"], "39e21754-b568-43a8-8116-3b4404c645c7": ["33383e76-a128-4724-8046-211f375ddb92"], "e7bc84af-3891-48e9-bf97-726f1fba46b9": ["02f6bbd1-8324-4dc7-8fea-10892b80f70c"], "b1580adf-6240-48b8-85ac-f45ad3f587ce": ["02f6bbd1-8324-4dc7-8fea-10892b80f70c"], "e151225f-5aa9-4821-a1bf-326937e08e1c": ["09e51cd3-ba38-4e17-9305-6e6487c71c9e"], "a7857d9b-8c9d-4342-8dcc-27df927ea1f8": ["09e51cd3-ba38-4e17-9305-6e6487c71c9e"], "4ff54218-d098-4b79-b7fe-e4a9b6382b55": ["e0c54e69-71d8-4fc1-8077-2bb945cab5ce"], "9d70d5af-dfe6-4f31-b332-2ac4f4f5d03f": ["e0c54e69-71d8-4fc1-8077-2bb945cab5ce"], "10c22a14-1d0f-4551-a12d-ffe323547624": ["f5290356-e1d6-4090-a053-03a0d1421092"], "07799984-0b23-418e-bd7b-e94ec5266d3c": ["f5290356-e1d6-4090-a053-03a0d1421092"], "0f463179-3fd1-4998-9d6d-ade8ec477894": ["e37183d1-6e73-4079-a423-7127d5703478"], "e5d52e03-7469-49df-a03d-70691210e26e": ["e37183d1-6e73-4079-a423-7127d5703478"], "30c2fc7e-76e5-44dd-9f1a-d73ee7ee6850": ["19ccf0ba-6916-47fd-8666-071ed9b1aacd"], "59a3d01f-114d-4f0a-90fb-847ac6b349e2": ["19ccf0ba-6916-47fd-8666-071ed9b1aacd"], "b0cafe6b-068c-4235-9a49-a27726b6f369": ["1a36240a-eb70-41a6-a643-11204fd4eead"], "b7172b7e-71e2-454b-b671-3eb28e051a5e": ["1a36240a-eb70-41a6-a643-11204fd4eead"], "f4805d01-7059-4438-9eb2-5057539eda71": ["dbab1a70-cac4-43b2-846b-b73320b58b21"], "f3093f20-9daf-4135-bbfd-ff6d9ace169a": ["dbab1a70-cac4-43b2-846b-b73320b58b21"], "69e0567e-faa4-49dd-868d-31900f9feba1": ["e42fa73e-53d2-49d9-9eb6-7df762d3ac54"], "af764b10-4378-43e4-ab54-fb7794c96878": ["e42fa73e-53d2-49d9-9eb6-7df762d3ac54"], "c7a5ecc6-0ca1-4bc7-aefe-312469f80327": ["2d9afe9d-133e-468b-9d02-f35043900a34"], "2ac0f3d5-0865-4772-83b1-681037efbdf7": ["2d9afe9d-133e-468b-9d02-f35043900a34"], "24a34cc0-0b1f-4b8d-bf8a-8ddf4e04112b": ["64f53976-ef4c-497e-abd7-cef6c70f91a6"], "5d9a4630-cbb1-4452-99dd-e6b37e78a8cc": ["64f53976-ef4c-497e-abd7-cef6c70f91a6"], "b440546c-8275-4852-a093-16c80d9331fd": ["a9393c60-af5c-4aa9-b8a3-37e4df00282a"], "ff40a976-42c3-441a-a193-a4dafdf91350": ["a9393c60-af5c-4aa9-b8a3-37e4df00282a"], "6e6dae6b-23af-491a-808e-fbd5bf05f5e4": ["f94e0d0b-837f-4fed-8e99-48ae404a9c65"], "8bbaefb2-af59-4d11-99e7-e389cccd8a5a": ["f94e0d0b-837f-4fed-8e99-48ae404a9c65"], "bc6ca361-c4d9-4c44-be28-487c9cbb46d8": ["a96d1521-fbdf-4d83-b284-51d909f82a38"], "12d89dd9-2572-4405-85dc-0aaa6a04c67d": ["a96d1521-fbdf-4d83-b284-51d909f82a38"], "d4b712d1-69a5-43e8-9972-7cf75a17080c": ["4fc58dbe-b778-4fd1-a776-2a6bf0342ab5"], "9ef86bd5-aa0c-400d-a0a1-df9434893a5b": ["4fc58dbe-b778-4fd1-a776-2a6bf0342ab5"]}, "corpus": {"560c316b-b85c-4908-900b-114508dbeda1": "whether they are genuinely helpful in solving an identified problem. \nIn discussion of technical and governance interventions that that are needed to protect against the harms of \nthese technologies, panelists individually described the importance of: receiving community input into the \ndesign and use of technologies, public reporting on crucial elements of these systems, better notice and consent \nprocedures that ensure privacy based on context and use case, ability to opt-out of using these systems and \nreceive a fallback to a human process, providing explanations of decisions and how these systems work, the \nneed for governance including training in using these systems, ensuring the technological use cases are \ngenuinely related to the goal task and are locally validated to work, and the need for institution and protection \nof third party audits to ensure systems continue to be accountable and valid. \n57", "68a8cb79-b085-4aa7-a822-7a6fe91efaf5": "voluntary use to help incorporate trustworthiness considerations into the design, development, use, and \nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \ninput. The NIST framework aims to foster the development of innovative approaches to address \ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, \nrobustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of \nharmful \nuses. \nThe \nNIST \nframework \nwill \nconsider \nand \nencompass \nprinciples \nsuch \nas \ntransparency, accountability, and fairness during pre-design, design and development, deployment, use, \nand testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23. \n21", "89e3e37f-a555-47a5-9d99-c4c12c871a74": "launched, preventing harm to the public. Federal government agencies have been developing standards and guidance \nfor the use of automated systems in order to help prevent bias. Non-profits and companies have developed best \npractices for audits and impact assessments to help identify potential algorithmic discrimination and provide \ntransparency to the public in the mitigation of such biases. \nBut there is much more work to do to protect the public from algorithmic discrimination to use and design \nautomated systems in an equitable way. The guardrails protecting the public from discrimination in their daily \nlives should include their digital lives and impacts\u2014basic safeguards against abuse, bias, and discrimination to \nensure that all people are treated fairly when automated systems are used. This includes all dimensions of their \nlives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal", "4877eae0-98c2-405d-a609-e2b4b6b043b8": "zational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AI \nand other data-driven automated systems most directly collect data on, make inferences about, and may cause \nharm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of com-\nmunities. Accordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights. \nUnited States law and policy have long employed approaches for protecting the rights of individuals, but exist-\ning frameworks have sometimes struggled to provide protections when effects manifest most clearly at a com-\nmunity level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automated \nsystems should be evaluated, protected against, and redressed at both the individual and community levels. \nEQUITY: \u201cEquity\u201d means the consistent and systematic fair, just, and impartial treatment of all individuals.", "2fd912b7-48c5-4d32-b453-ef5b22179648": "ENDNOTES\n96. National Science Foundation. NSF Program on Fairness in Artificial Intelligence in Collaboration\nwith Amazon (FAI). Accessed July 20, 2022.\nhttps://www.nsf.gov/pubs/2021/nsf21585/nsf21585.htm\n97. Kyle Wiggers. Automatic signature verification software threatens to disenfranchise U.S. voters.\nVentureBeat. Oct. 25, 2020.\nhttps://venturebeat.com/2020/10/25/automatic-signature-verification-software-threatens-to\u00ad\ndisenfranchise-u-s-voters/\n98. Ballotpedia. Cure period for absentee and mail-in ballots. Article retrieved Apr 18, 2022.\nhttps://ballotpedia.org/Cure_period_for_absentee_and_mail-in_ballots\n99. Larry Buchanan and Alicia Parlapiano. Two of these Mail Ballot Signatures are by the Same Person.\nWhich Ones? New York Times. Oct. 7, 2020.\nhttps://www.nytimes.com/interactive/2020/10/07/upshot/mail-voting-ballots-signature\u00ad\nmatching.html\n100. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020.", "f4b531ac-2549-4da9-9756-0bdf32a95008": "basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this \ncountry.\u201d2\nTo advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified \nfive principles that should guide the design, use, and deployment of automated systems to protect the American \npublic in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that \nprotects all people from these threats\u2014and uses technologies in ways that reinforce our highest values. \nResponding to the experiences of the American public, and informed by insights from researchers, \ntechnologists, advocates, journalists, and policymakers, this framework is accompanied by a technical \ncompanion\u2014a handbook for anyone seeking to incorporate these protections into policy and practice, including \ndetailed steps toward actualizing these principles in the technological design process. These principles help", "ad3277f6-9b01-4a15-85ee-0547b6cdef81": "models, content moderation, code generation and review, text generation and editing, image and video \ngeneration, summarization, search, and chat. These activities can take place within organizational \nsettings or in the public domain. \nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that con\ufb02ict \nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \nsystems can be applied to GAI systems. These plans and actions include: \n\u2022 Accessibility and reasonable \naccommodations \n\u2022 AI actor credentials and quali\ufb01cations  \n\u2022 Alignment to organizational values \n\u2022 Auditing and assessment \n\u2022 Change-management controls \n\u2022 Commercial use \n\u2022 Data provenance", "7a97c9a4-8b02-4940-b7ad-dccb212bceff": "The law and policy landscape for motor vehicles shows that strong safety regulations\u2014and \nmeasures to address harms when they occur\u2014can enhance innovation in the context of com-\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components. \nThe National Highway Traffic Safety Administration,14 through its rigorous standards and independent \nevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers\u2019 ability to \ninnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \nrequirements on drivers, such as slowing down near schools or playgrounds.16\nFrom large companies to start-ups, industry is providing innovative solutions that allow \norganizations to mitigate risks to the safety and efficacy of AI systems, both before \ndeployment and through monitoring over time.17 These innovative solutions include risk", "20b4ecee-f877-41bf-94e6-6f7d2bdcdd95": "defense, substantive or procedural, enforceable at law or in equity by any party against the United States, its \ndepartments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a \nwaiver of sovereign immunity. \nCopyright Information \nThis document is a work of the United States Government and is in the public domain (see 17 U.S.C. \u00a7105). \n2", "fede1846-3a31-46de-acc5-944032788823": "architecture, training process of the pre-trained model including information on \nhyperparameters, training duration, and any \ufb01ne-tuning or retrieval-augmented \ngeneration processes applied. \nInformation Integrity; Harmful Bias \nand Homogenization; Intellectual \nProperty \nMG-3.2-004 Evaluate user reported problematic content and integrate feedback into system \nupdates. \nHuman-AI Con\ufb01guration, \nDangerous, Violent, or Hateful \nContent \nMG-3.2-005 \nImplement content \ufb01lters to prevent the generation of inappropriate, harmful, \nfalse, illegal, or violent content related to the GAI application, including for CSAM \nand NCII. These \ufb01lters can be rule-based or leverage additional machine learning \nmodels to \ufb02ag problematic inputs and outputs. \nInformation Integrity; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMG-3.2-006 \nImplement real-time monitoring processes for analyzing generated content", "7c98ce98-ef06-45d6-b674-b51ce70d0b0b": "be built into the system design so that the system\u2019s full behavior can be explained in advance (i.e., only fully \ntransparent models should be used), rather than as an after-the-decision interpretation. In other settings, the \nextent of explanation provided should be tailored to the risk level. \nValid. The explanation provided by a system should accurately reflect the factors and the influences that led \nto a particular decision, and should be meaningful for the particular customization based on purpose, target, \nand level of risk. While approximation and simplification may be necessary for the system to succeed based on \nthe explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns \nrelated to revealing decision-making information, such simplifications should be done in a scientifically \nsupportable way. Where appropriate based on the explanatory system, error ranges for the explanation should", "1e320b68-4737-4d89-be43-8b9ff8bcfc7f": "and federal government officials to offer insights and analysis on the risks, harms, benefits, and \npolicy opportunities of automated systems. Each panel discussion was organized around a wide-ranging \ntheme, exploring current challenges and concerns and considering what an automated society that \nrespects democratic values should look like. These discussions focused on the topics of consumer \nrights and protections, the criminal justice system, equal opportunities and civil justice, artificial \nintelligence and democratic values, social welfare and development, and the healthcare system. \nSummaries of Panel Discussions: \nPanel 1: Consumer Rights and Protections. This event explored the opportunities and challenges for \nindividual consumers and communities in the context of a growing ecosystem of AI-enabled consumer \nproducts, advanced platforms and services, \u201cInternet of Things\u201d (IoT) devices, and smart city products and \nservices. \nWelcome:\n\u2022", "85df67c4-01eb-4daf-b75c-803d4f9f7e2c": "MS-2.5-005 Verify GAI system training data and TEVV data provenance, and that \ufb01ne-tuning \nor retrieval-augmented generation data is grounded. \nInformation Integrity \nMS-2.5-006 \nRegularly review security and safety guardrails, especially if the GAI system is \nbeing operated in novel circumstances. This includes reviewing reasons why the \nGAI system was initially assessed as being safe to deploy.  \nInformation Security; Dangerous, \nViolent, or Hateful Content \nAI Actor Tasks: Domain Experts, TEVV", "fabb8048-84be-4544-b4bb-69c6353bb66c": "limits its focus to both government and commercial use of surveillance technologies when juxtaposed with \nreal-time or subsequent automated analysis and when such systems have a potential for meaningful impact \non individuals\u2019 or communities\u2019 rights, opportunities, or access. \nUNDERSERVED COMMUNITIES: The term \u201cunderserved communities\u201d refers to communities that have \nbeen systematically denied a full opportunity to participate in aspects of economic, social, and civic life, as \nexemplified by the list in the preceding definition of \u201cequity.\u201d \n11", "1e82d8d2-9fb4-45ab-a88a-315f048f4082": "SAFE AND EFFECTIVE \nSYSTEMS \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nOngoing monitoring. Automated systems should have ongoing monitoring procedures, including recalibra\u00ad\ntion procedures, in place to ensure that their performance does not fall below an acceptable level over time, \nbased on changing real-world conditions or deployment contexts, post-deployment modification, or unexpect\u00ad\ned conditions. This ongoing monitoring should include continuous evaluation of performance metrics and \nharm assessments, updates of any systems, and retraining of any machine learning models as necessary, as well \nas ensuring that fallback mechanisms are in place to allow reversion to a previously working system. Monitor\u00ad", "bf9159b4-2816-403b-aada-24bf8412ddb5": "information-rfi-on-public-and-private-sector-uses-of-biometric-technologies\n114. National Artificial Intelligence Initiative Office. Public Input on Public and Private Sector Uses of\nBiometric Technologies. Accessed Apr. 19, 2022.\nhttps://www.ai.gov/86-fr-56300-responses/\n115. Thomas D. Olszewski, Lisa M. Van Pay, Javier F. Ortiz, Sarah E. Swiersz, and Laurie A. Dacus.\nSynopsis of Responses to OSTP\u2019s Request for Information on the Use and Governance of Biometric\nTechnologies in the Public and Private Sectors. Science and Technology Policy Institute. Mar. 2022.\nhttps://www.ida.org/-/media/feature/publications/s/sy/synopsis-of-responses-to-request-for\u00ad\ninformation-on-the-use-and-governance-of-biometric-technologies/ida-document-d-33070.ashx\n73", "006e62b4-5fd5-453d-ac13-990e5ef4ce33": "deploying or using the automated system to examine whether the system has led to algorithmic discrimina\u00ad\ntion when deployed. This assessment should be performed regularly and whenever a pattern of unusual \nresults is occurring. It can be performed using a variety of approaches, taking into account whether and how \ndemographic information of impacted people is available, for example via testing with a sample of users or via \nqualitative user experience research. Riskier and higher-impact systems should be monitored and assessed \nmore frequently. Outcomes of this assessment should include additional disparity mitigation, if needed, or \nfallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, and \nprior mechanisms provide better adherence to equity standards. \n27\nAlgorithmic \nDiscrimination \nProtections", "6807109d-3853-473c-8ba1-21c9f0aec268": "enhancing techniques in GAI development, where appropriate and applicable, \nmatch the statistical properties of real-world data without disclosing personally \nidenti\ufb01able information or contributing to homogenization. \nData Privacy; Intellectual Property; \nInformation Integrity; \nConfabulation; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identi\ufb01ed. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.3-001 \nDevelop and update GAI system incident response and recovery plans and \nprocedures to address the following: Review and maintenance of policies and \nprocedures to account for newly encountered uses; Review and maintenance of \npolicies and procedures for detection of unanticipated uses; Verify response \nand recovery plans account for the GAI system value chain; Verify response and", "d3d96f8c-38e5-458e-9c1c-371ca2c181ac": "Publishing, Paris. https://doi.org/10.1787/d1a8d965-en \nOpenAI (2023) GPT-4 System Card. https://cdn.openai.com/papers/gpt-4-system-card.pdf \nOpenAI (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774 \nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \nhttps://arxiv.org/pdf/2309.05196 \nPark, P. et. al. (2024) AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). \narXiv. https://arxiv.org/pdf/2308.14752 \nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect \nDisclosure. https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1-\nindirect-disclosure/ \nQu, Y. et al. (2023) Unsafe Di\ufb00usion: On the Generation of Unsafe Images and Hateful Memes From Text-\nTo-Image Models. arXiv. https://arxiv.org/pdf/2305.13873 \nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model", "657e6e3d-cd4b-4528-ac1d-50d0f1a8ac2c": "harm assessments, updates of any systems, and retraining of any machine learning models as necessary, as well \nas ensuring that fallback mechanisms are in place to allow reversion to a previously working system. Monitor\u00ad\ning should take into account the performance of both technical system components (the algorithm as well as \nany hardware components, data inputs, etc.) and human operators. It should include mechanisms for testing \nthe actual accuracy of any predictions or recommendations generated by a system, not just a human operator\u2019s \ndetermination of their accuracy. Ongoing monitoring procedures should include manual, human-led monitor\u00ad\ning as a check in the event there are shortcomings in automated monitoring systems. These monitoring proce\u00ad\ndures should be in place for the lifespan of the deployed automated system. \nClear organizational oversight. Entities responsible for the development or use of automated systems", "a5db6001-ac93-4b97-afa0-6b170643ffeb": "\u201cbiometric identifying technology\u201d in schools until July 1, 2022.80 The law additionally requires that a report on \nthe privacy, civil rights, and civil liberties implications of the use of such technologies be issued before \nbiometric identification technologies can be used in New York schools. \nFederal law requires employers, and any consultants they may retain, to report the costs \nof surveilling employees in the context of a labor dispute, providing a transparency \nmechanism to help protect worker organizing. Employers engaging in workplace surveillance \"where \nan object there-of, directly or indirectly, is [\u2026] to obtain information concerning the activities of employees or a \nlabor organization in connection with a labor dispute\" must report expenditures relating to this surveillance to \nthe Department of Labor Office of Labor-Management Standards, and consultants who employers retain for \nthese purposes must also file reports regarding their activities.81", "347d53c5-b382-4a95-b296-5727a3c2ce38": "Consent withdrawal and data deletion. Entities should allow (to the extent legally permissible) with\u00ad\ndrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of \ntheir data from any systems (e.g., machine learning models) derived from that data.68\nAutomated system support. Entities designing, developing, and deploying automated systems should \nestablish and maintain the capabilities that will allow individuals to use their own automated systems to help \nthem make consent, access, and control decisions in a complex data ecosystem. Capabilities include machine \nreadable data, standardized data formats, metadata or tags for expressing data processing permissions and \npreferences and data provenance and lineage, context of use and access-specific tags, and training models for \nassessing privacy risk. \nDemonstrate that data privacy and user control are protected", "1ceaf856-b64a-470b-9836-f07d99a08ace": "HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \nHealthcare \u201cnavigators\u201d help people find their way through online signup forms to choose \nand obtain healthcare. A Navigator is \u201can individual or organization that's trained and able to help \nconsumers, small businesses, and their employees as they look for health coverage options through the \nMarketplace (a government web site), including completing eligibility and enrollment forms.\u201d106 For \nthe 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could \n\u201ctrain and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensive \nhealth coverage.\u201d107", "c644be03-0039-4466-aab2-6432319ac6e5": "Moderator: Kathy Pham Evans, Deputy Chief Technology Officer for Product and Engineering, U.S \nFederal Trade Commission. \nPanelists: \n\u2022\nLiz O\u2019Sullivan, CEO, Parity AI\n\u2022\nTimnit Gebru, Independent Scholar\n\u2022\nJennifer Wortman Vaughan, Senior Principal Researcher, Microsoft Research, New York City\n\u2022\nPamela Wisniewski, Associate Professor of Computer Science, University of Central Florida; Director,\nSocio-technical Interaction Research (STIR) Lab\n\u2022\nSeny Kamara, Associate Professor of Computer Science, Brown University\nEach panelist individually emphasized the risks of using AI in high-stakes settings, including the potential for \nbiased data and discriminatory outcomes, opaque decision-making processes, and lack of public trust and \nunderstanding of the algorithmic systems. The interventions and key needs various panelists put forward as \nnecessary to the future design of critical AI systems included ongoing transparency, value sensitive and", "2fcfbe54-2897-4e2e-8da2-4b9089f68683": "SAFE AND EFFECTIVE \nSYSTEMS \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \u00ad\u00ad\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \nFederal Government requires that certain federal agencies adhere to nine principles when \ndesigning, developing, acquiring, or using AI for purposes other than national security or \ndefense. These principles\u2014while taking into account the sensitive law enforcement and other contexts in which \nthe federal government may use AI, as opposed to private sector use of AI\u2014require that AI is: (a) lawful and \nrespectful of our Nation\u2019s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d)", "868816e4-73ce-4e0c-93bb-25a6cc6e6ced": "system should ensure that documentation describing the overall system (including any human components) is \npublic and easy to find. The documentation should describe, in plain language, how the system works and how \nany automated component is used to determine an action or decision. It should also include expectations about \nreporting described throughout this framework, such as the algorithmic impact assessments described as \npart of Algorithmic Discrimination Protections. \nAccountable. Notices should clearly identify the entity responsible for designing each component of the \nsystem and the entity using it. \nTimely and up-to-date. Users should receive notice of the use of automated systems in advance of using or \nwhile being impacted by the technology. An explanation should be available with the decision itself, or soon \nthereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use case \nor key functionality changes.", "9d03e4e9-9969-4a03-9d62-1b7bf051fe41": "as credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate the \nprocess of explaining such systems are under active research and improvement and such explanations can take many \nforms. Innovative companies and researchers are rising to the challenge and creating and deploying explanatory \nsystems that can help the public better understand decisions that impact them. \nWhile notice and explanation requirements are already in place in some sectors or situations, the American public \ndeserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights, \nopportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in the \nvalidity and reasonable use of automated systems. \n\u2022\nA lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home", "5d5280b8-c456-4b8e-bb23-2c18d489c7b0": "design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct \nvalidation \nAction ID \nSuggested Action \nGAI Risks \nMP-2.3-001 \nAssess the accuracy, quality, reliability, and authenticity of GAI output by \ncomparing it to a set of known ground truth data and by using a variety of \nevaluation methods (e.g., human oversight and automated evaluation, proven \ncryptographic techniques, review of content inputs). \nInformation Integrity", "c6c5d608-62a3-4472-abb7-60eabfdbb58f": "incorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Con\ufb01guration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or \ufb01ction or acknowledge \nuncertainties, or could be leveraged for large-scale dis- and mis-information campaigns. \n9. Information Security: Lowered barriers for o\ufb00ensive cyber capabilities, including via automated \ndiscovery and exploitation of vulnerabilities to ease hacking, malware, phishing, o\ufb00ensive cyber", "e9550b70-6ec3-442c-8821-0c3586e8af26": "performance should be compared with the in-place, potentially human-driven, status quo procedures, with \nexisting human performance considered as a performance baseline for the algorithm to meet pre-deployment, \nand as a lifecycle minimum performance standard. Decision possibilities resulting from performance testing \nshould include the possibility of not deploying the system. \nRisk identification and mitigation. Before deployment, and in a proactive and ongoing manner, poten\u00ad\ntial risks of the automated system should be identified and mitigated. Identified risks should focus on the \npotential for meaningful impact on people\u2019s rights, opportunities, or access and include those to impacted \ncommunities that may not be direct users of the automated system, risks resulting from purposeful misuse of \nthe system, and other concerns identified via the consultation process. Assessment and, where possible, mea\u00ad", "580f4522-df99-42bc-b50e-1982319b9c07": "SECTION TITLE\nHUMAN ALTERNATIVES, CONSIDERATION, AND FALLBACK\nYou should be able to opt out, where appropriate, and have access to a person who can quickly \nconsider and remedy problems you encounter. You should be able to opt out from automated systems in \nfavor of a human alternative, where appropriate. Appropriateness should be determined based on reasonable \nexpectations in a given context and with a focus on ensuring broad accessibility and protecting the public from \nespecially harmful impacts. In some cases, a human or other alternative may be required by law. You should have \naccess to timely human consideration and remedy by a fallback and escalation process if an automated system \nfails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and \nfallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and", "78deb994-1b18-4b76-a735-1fc150351f17": "Information Security \nMG-1.3-002 \nMonitor the robustness and e\ufb00ectiveness of risk controls and mitigation plans \n(e.g., via red-teaming, \ufb01eld testing, participatory engagements, performance \nassessments, user feedback mechanisms). \nHuman-AI Con\ufb01guration \nAI Actor Tasks: AI Development, AI Deployment, AI Impact Assessment, Operation and Monitoring \n \nMANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.2-001 \nCompare GAI system outputs against pre-de\ufb01ned organization risk tolerance, \nguidelines, and principles, and review and test AI-generated content against \nthese guidelines. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content \nMG-2.2-002 \nDocument training data sources to trace the origin and provenance of AI-\ngenerated content. \nInformation Integrity \nMG-2.2-003", "b26f8a5b-2e2d-4b86-bcb7-6e43390bb816": "enforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, \nand privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in \nthis framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in \nmoving principles into practice. \nThe expectations given in the Technical Companion are meant to serve as a blueprint for the development of \nadditional technical standards and practices that should be tailored for particular sectors and contexts. While \nexisting laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail \nthose laws beyond providing them as examples, where appropriate, of existing protective measures. This \nframework instead shares a broad, forward-leaning vision of recommended principles for automated system", "d51d3101-2ad2-4f77-aa6e-91a7055fc090": "before; as such, the protections afforded by current legal guidelines may be inadequate. The American public \ndeserves assurances that data related to such sensitive domains is protected and used appropriately and only in \nnarrowly defined contexts with clear benefits to the individual and/or society. \nTo this end, automated systems that collect, use, share, or store data related to these sensitive domains should meet \nadditional expectations. Data and metadata are sensitive if they pertain to an individual in a sensitive domain (defined \nbelow); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or \nsensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, \ngeolocation data, data related to interaction with the criminal justice system, relationship history and legal status such", "33383e76-a128-4724-8046-211f375ddb92": "those who are less proximate do not (e.g., a teacher has access to their students\u2019 daily progress data while a \nsuperintendent does not). \nReporting. In addition to the reporting on data privacy (as listed above for non-sensitive data), entities devel-\noping technologies related to a sensitive domain and those collecting, using, storing, or sharing sensitive data \nshould, whenever appropriate, regularly provide public reports describing: any data security lapses or breaches \nthat resulted in sensitive data leaks; the number, type, and outcomes of ethical pre-reviews undertaken; a \ndescription of any data sold, shared, or made public, and how that data was assessed to determine it did not pres-\nent a sensitive data risk; and ongoing risk identification and management procedures, and any mitigation added \nbased on these procedures. Reporting should be provided in a clear and machine-readable manner. \n38", "02f6bbd1-8324-4dc7-8fea-10892b80f70c": "violent recommendations, and some models have generated actionable instructions for dangerous or \n \n \n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \ncontent, creative generation of non-factual content can be a desired behavior.  \n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \ne.g.,", "09e51cd3-ba38-4e17-9305-6e6487c71c9e": "particular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore, \nunaccountable, whether by design or by omission. These factors can make explanations both more challenging and \nmore important, and should not be used as a pretext to avoid explaining important decisions to the people impacted \nby those choices. In the context of automated systems, clear and valid explanations should be recognized as a baseline \nrequirement. \nProviding notice has long been a standard practice, and in many cases is a legal requirement, when, for example, \nmaking a video recording of someone (outside of a law enforcement or national security context). In some cases, such \nas credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate the \nprocess of explaining such systems are under active research and improvement and such explanations can take many", "e0c54e69-71d8-4fc1-8077-2bb945cab5ce": "SECTION TITLE\nAPPENDIX\nListening to the American People \nThe White House Office of Science and Technology Policy (OSTP) led a yearlong process to seek and distill \ninput from people across the country \u2013 from impacted communities to industry stakeholders to \ntechnology developers to other experts across fields and sectors, as well as policymakers across the Federal \ngovernment \u2013 on the issue of algorithmic and data-driven harms and potential remedies. Through panel \ndiscussions, public listening sessions, private meetings, a formal request for information, and input to a \npublicly accessible and widely-publicized email address, people across the United States spoke up about \nboth the promises and potential harms of these technologies, and played a central role in shaping the \nBlueprint for an AI Bill of Rights. \nPanel Discussions to Inform the Blueprint for An AI Bill of Rights", "f5290356-e1d6-4090-a053-03a0d1421092": "healthcare clinical algorithms that are used by physicians to guide clinical decisions may include\nsociodemographic variables that adjust or \u201ccorrect\u201d the algorithm\u2019s output on the basis of a patient\u2019s race or\nethnicity, which can lead to race-based health inequities.47\n25\nAlgorithmic \nDiscrimination \nProtections", "e37183d1-6e73-4079-a423-7127d5703478": "www.dol.gov/sites/dolgov/files/OLMS/regs/compliance/LM-10_factsheet.pdf\n82. See, e.g., Apple. Protecting the User\u2019s Privacy. Accessed May 2, 2022.\nhttps://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy; Google Developers.\nDesign for Safety: Android is secure by default and private by design. Accessed May 3, 2022.\nhttps://developer.android.com/design-for-safety\n83. Karen Hao. The coming war on the hidden algorithms that trap people in poverty. MIT Tech Review.\nDec. 4, 2020.\nhttps://www.technologyreview.com/2020/12/04/1013068/algorithms-create-a-poverty-trap-lawyers\u00ad\nfight-back/\n84. Anjana Samant, Aaron Horowitz, Kath Xu, and Sophie Beiers. Family Surveillance by Algorithm.\nACLU. Accessed May 2, 2022.\nhttps://www.aclu.org/fact-sheet/family-surveillance-algorithm\n70", "19ccf0ba-6916-47fd-8666-071ed9b1aacd": "Privacy Protection Act of 1998, 15 U.S.C. 6501\u20136505, and Confidential Information Protection and\nStatistical Efficiency Act (CIPSEA) (116 Stat. 2899)\n70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.\n21, 2018.\nhttps://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true\n71. Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\n72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology\nSchools are Using to Monitor Students. ProPublica. Jun. 25, 2019.\nhttps://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology\u00ad\nschools-are-using-to-monitor-students/\n73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\nfighting back. Washington Post. Nov. 12, 2020.", "1a36240a-eb70-41a6-a643-11204fd4eead": "Some risks can be assessed as likely to materialize in a given context, particularly those that have been \nempirically demonstrated in similar contexts. Other risks may be unlikely to materialize in a given \ncontext, or may be more speculative and therefore uncertain. \nAI risks can di\ufb00er from or intensify traditional software risks. Likewise, GAI can exacerbate existing AI \nrisks, and creates unique risks. GAI risks can vary along many dimensions: \n\u2022 \nStage of the AI lifecycle: Risks can arise during design, development, deployment, operation, \nand/or decommissioning. \n\u2022 \nScope: Risks may exist at individual model or system levels, at the application or implementation \nlevels (i.e., for a speci\ufb01c use case), or at the ecosystem level \u2013 that is, beyond a single system or \norganizational context. Examples of the latter include the expansion of \u201calgorithmic \nmonocultures,3\u201d resulting from repeated use of the same model, or impacts on access to", "dbab1a70-cac4-43b2-846b-b73320b58b21": "Dangerous, Violent, or Hateful \nContent \nMS-2.12-002 Document anticipated environmental impacts of model development, \nmaintenance, and deployment in product design decisions. \nEnvironmental \nMS-2.12-003 \nMeasure or estimate environmental impacts (e.g., energy and water \nconsumption) for training, \ufb01ne tuning, and deploying models: Verify tradeo\ufb00s \nbetween resources used at inference time versus additional resources required \nat training time. \nEnvironmental \nMS-2.12-004 Verify e\ufb00ectiveness of carbon capture or o\ufb00set programs for GAI training and \napplications, and address green-washing concerns. \nEnvironmental \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV", "e42fa73e-53d2-49d9-9eb6-7df762d3ac54": "DATA PRIVACY \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \nThe Privacy Act of 1974 requires privacy protections for personal information in federal \nrecords systems, including limits on data retention, and also provides individuals a general \nright to access and correct their data. Among other things, the Privacy Act limits the storage of individual \ninformation in federal systems of records, illustrating the principle of limiting the scope of data retention. Under \nthe Privacy Act, federal agencies may only retain data about an individual that is \u201crelevant and necessary\u201d to \naccomplish an agency\u2019s statutory purpose or to comply with an Executive Order of the President. The law allows", "2d9afe9d-133e-468b-9d02-f35043900a34": "Information Security \nMP-5.1-006 \nPro\ufb01le threats and negative impacts arising from GAI systems interacting with, \nmanipulating, or generating content, and outlining known and potential \nvulnerabilities and the likelihood of their occurrence. \nInformation Security \nAI Actor Tasks: AI Deployment, AI Design, AI Development, AI Impact Assessment, A\ufb00ected Individuals and Communities, End-\nUsers, Operation and Monitoring", "64f53976-ef4c-497e-abd7-cef6c70f91a6": "proactive \nand \ncontinuous \nmeasures \nto \nprotect \nindividuals \nand \ncommunities \nfrom algorithmic \ndiscrimination and to use and design systems in an equitable way. This protection should include proactive \nequity assessments as part of the system design, use of representative data and protection against proxies \nfor demographic features, ensuring accessibility for people with disabilities in design and development, \npre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent \nevaluation and plain language reporting in the form of an algorithmic impact assessment, including \ndisparity testing results and mitigation information, should be performed and made public whenever \npossible to confirm these protections. \n5", "a9393c60-af5c-4aa9-b8a3-37e4df00282a": "organization\u2019s business processes or other activities, system goals, any human-run procedures that form a \npart of the system, and specific performance expectations; a description of any data used to train machine \nlearning models or for other purposes, including how data sources were processed and interpreted, a \nsummary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the \nresults of public consultation such as concerns raised and any decisions made due to these concerns; risk \nidentification and management assessments and any steps taken to mitigate potential harms; the results of \nperformance testing including, but not limited to, accuracy, differential demographic impact, resulting \nerror rates (overall and per demographic group), and comparisons to previously deployed systems; \nongoing monitoring procedures and regular performance testing reports, including monitoring frequency,", "f94e0d0b-837f-4fed-8e99-48ae404a9c65": "campaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and \nengagement on social media platforms. Additionally, generative AI models can assist malicious actors in \ncreating fraudulent content intended to impersonate others. \nTrustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and \nExplainable \n2.9. Information Security \nInformation security for computer systems and data is a mature \ufb01eld with widely accepted and \nstandardized practices for o\ufb00ensive and defensive cyber capabilities. GAI-based systems present two \nprimary information security risks: GAI could potentially discover or enable new cybersecurity risks by \nlowering the barriers for or easing automated exercise of o\ufb00ensive capabilities; simultaneously, it \nexpands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data \npoisoning.", "a96d1521-fbdf-4d83-b284-51d909f82a38": "57 \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \nHow AI Risks Di\ufb00er from Traditional Software Risks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B \nNational Institute of Standards and Technology (2023) AI RMF Playbook. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook \nNational Institue of Standards and Technology (2023) Framing Risk \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1-sec-risk \nNational Institute of Standards and Technology (2023) The Language of Trustworthy AI: An In-Depth \nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary \nNational Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \nBias in Arti\ufb01cial Intelligence https://www.nist.gov/publications/towards-standard-identifying-and-\nmanaging-bias-arti\ufb01cial-intelligence", "4fc58dbe-b778-4fd1-a776-2a6bf0342ab5": "origins and modi\ufb01cations. Further narrowing of GAI task de\ufb01nitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management e\ufb00orts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback \nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.  \nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and \ncorresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback \u2013 before"}}
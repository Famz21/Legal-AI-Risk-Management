{"questions": {"9359ec2a-755e-4f38-ab9b-a959faade87c": "What are the responsibilities of AI Actors in monitoring reported issues related to GAI system performance?", "7951764e-0adf-4275-96ec-66506ab80ada": "How are measurable activities for continual improvements integrated into AI system updates according to the context provided?", "e05e9fc0-5672-4ed8-b2a3-eb5820dede84": "What is the main function of the app discussed in Samantha Cole's article from June 26, 2019?", "53085e80-ee55-412f-ad31-87b71b19b291": "What issue is highlighted in Lauren Kaori Gurley's article regarding Amazon's AI cameras?", "4bb1fe70-04c4-4914-80d2-bbc3262c2ea9": "What are the suggested actions provided in the document to help organizations manage risks associated with Generative AI (GAI)?", "30af0e74-6e5b-4ba3-b475-c15a6c0ce965": "How does EO 14110 define Generative AI, and what types of content can it generate?", "cf83ff2d-a13d-4dd4-acef-354940f0c5d5": "What are the expectations for automated systems regarding the handling of sensitive data?", "7f7cbad6-8db9-4604-b47e-9610d018534c": "Under what circumstances may consent for sensitive data need to be acquired from a guardian and/or child?", "6cc44cb7-c062-4eb3-9909-7f9b7bd2ff5b": "What types of requests for consideration are mentioned in the context?", "77aee64b-31c2-4f93-9d1c-2a0028a803ba": "What information should be included in the reporting for systems used in sensitive domains?", "a6bfcc82-f6b5-4353-ace0-db9f6639ad13": "What are the potential consequences of school audio surveillance systems monitoring student conversations?", "3a279837-3633-45f1-bfe4-4543ff5fc090": "How might location data from data brokers impact individuals who visit abortion clinics?", "67c59510-aa48-42b7-9a40-a17d83383699": "What rights do applicants have if their application for credit is denied according to the CFPB?", "f4663d20-a072-45a3-ab51-0a9ed245c4d4": "What requirements does California law impose on warehouse employers regarding employee quotas?", "5f8685ee-2106-4af5-ab7a-82031c9f457e": "What is the term used to describe the issue of LLMs revealing sensitive information from their training data during adversarial attacks?", "6be2b5d4-6566-4e26-afa3-bb5f15e800d6": "How can GAI models infer PII or sensitive data that was not included in their training data?", "e83ea684-71b2-44e1-be69-b11d2956d159": "What are the suggested actions for conducting impact assessments on AI-generated content according to the provided context?", "21eec31c-f0e1-47cf-865b-0a2eb2f04d57": "How can potential biases and stereotypes in AI-generated content be evaluated based on the methodologies mentioned?", "de1912dc-75a2-4145-9340-7de4d68eb0f0": "What are the main concerns addressed in the paper by Qu et al. (2023) regarding text-to-image models?", "4c0e6f0b-1312-41e4-ba12-883825b96e33": "How does the research by Rafat et al. (2023) contribute to reducing the carbon footprint in deep learning model compression?", "8c73a115-dd73-47b5-9c01-68b0b99cd25a": "What are the main concerns discussed in the paper by Luccioni et al. (2023) regarding the cost of AI deployment?", "7e4b1def-af67-4ecc-be8e-1d895e9226ee": "How does the National Institute of Standards and Technology (2023) address AI risks and trustworthiness in their AI Risk Management Framework?", "aa0d05d8-639d-4438-b7b2-95dd14b860b5": "What does \"equity\" mean in the context of treatment for individuals from underserved communities?", "05051823-b0c9-404b-a96c-613a8826857a": "Which groups are identified as being adversely affected by persistent poverty or inequality in the context provided?", "37356948-60d9-4493-b459-590b701db73f": "What measures should be taken to ensure that surveillance does not infringe upon civil liberties and rights?", "48cfbe42-de1d-4557-82f8-838f40e26612": "How can surveillance systems be designed to avoid algorithmic discrimination based on identity-related information?", "d0b46f77-ef1f-46fd-be09-9f2aecd247db": "What is the role of the Center for Research and Education on Accessible Technology and Experiences at the University of Washington?", "9834ff3c-dbab-4399-96c4-592d6dc34021": "How does the City of Portland Smart City PDX Program relate to technology initiatives?", "8bbbb635-9ae8-4396-83c5-a84e1b09c39f": "What are some reasons people may prefer not to use an automated system according to the context provided?", "14d358f8-d913-4b04-9a50-f40d9f739ad8": "How does the lack of human reconsideration impact individuals' access to rights, opportunities, and benefits?", "d36bca4b-2e3b-4d1f-a70e-37f6eda7349e": "What are the potential harms that can be caused by AI incidents as defined in the context?", "20cec3ab-1c2b-4ecc-826f-420577c0100e": "Are there currently formal channels available for reporting and documenting AI incidents?", "aa660712-ab3d-4265-8080-b6598fff34ae": "What foundational American principles has President Biden affirmed as a cornerstone of his Administration?", "bab92b97-da80-4aa1-af12-8eff44756bd7": "What actions did President Biden take on his first day in office to address inequity and civil rights?", "3b619b6f-78d7-4911-995e-09f9f01dc0b1": "What criteria determine the appropriateness of opting out of automated systems in favor of human alternatives?", "a4167b3e-2e87-4138-9ee3-aae00129f33f": "What should be included in the fallback and escalation process if an automated system fails?", "c6eacea8-70df-43a7-9634-d45da6e24d8d": "What are some potential mental health harms associated with the use of surveillance technologies in schools and workplaces?", "ad32eb95-d42d-402b-8d5d-bddbd49eaf96": "How does the aggregation of personal data by data brokers affect communities?", "ecdc997f-fcaf-4754-90f9-e7218f607a82": "What are the key expectations for automated systems regarding surveillance and monitoring?", "bcb0b164-73c1-456c-a4c9-3ebb08b1d0ab": "Why is it important to assess potential harms of surveillance systems before deployment?", "b008ea2e-0fa6-4704-bcfe-63926e370b62": "What are some existing regulatory safety requirements mentioned in the context for medical devices?", "b82511ce-0113-4ad5-b176-702cbd10c7e9": "In what situations might exceptions to the principles of the AI Bill of Rights be necessary?", "931c42a0-82d3-40db-940f-d0fb3548d0e8": "What are the suggested actions for addressing privacy risks associated with AI-generated content?", "005c0ac3-f057-41f1-ab95-3309b1c37cf1": "How should organizations respond to potential intellectual property infringement claims according to the provided context?", "018689b1-aca8-4d06-ba46-5d5f5f9f266c": "What issues does the automated sentiment analyzer's bias against Jews and gay people highlight?", "fe0ae106-1da5-4a0a-8895-8c43da752054": "How has the related company responded to the concerns about bias in their sentiment analysis tool?", "57a1ea6b-52e3-48fb-83d0-3b3dbbdd80ee": "What are the built-in protections that should be in place to safeguard individuals from abusive data practices?", "2719ce52-1169-4489-ad53-008763fe4588": "How should designers, developers, and deployers of automated systems approach user consent regarding data collection and usage?", "24f5399a-0435-40e6-a88f-f3ff09352629": "How do LLMs compare to traditional search engine queries in providing assistance for biological threat creation and attack planning?", "529587e1-9907-4f83-a6aa-7ab7b86ef74d": "What factors will influence the impact of GAI on the misuse of chemical or biological agents?", "04aaf309-dcdd-462a-9566-025e5cd73079": "What are the main ethical and social risks associated with language models as discussed by Weidinger et al. (2021)?", "e20a3f65-c076-42b7-a2ae-0689d2f336bf": "How does the research by Yin et al. (2024) highlight racial bias in OpenAI's GPT in the context of recruitment?", "939f291a-ab95-40b4-a0ce-19e5a29d9d98": "What are the key tasks involved in the AI Actor's responsibilities as outlined in the context?", "055bf158-d20d-4ff1-81bf-84c82b0cf748": "How are the most significant AI risks selected for implementation according to MEASURE 1.1?", "fb92684b-aea6-4dd5-9ab4-7b3fae408439": "What are some of the challenges posed to democracy by technology and automated systems, as mentioned in the foreword?", "26844282-e776-44b5-a21e-e3e71a5f599c": "How have algorithms in hiring and credit decisions been found to impact existing inequities, according to the context?", "c0d7b5d4-3668-4077-97c0-26a9be864434": "How does the local police department utilize videos from the community in relation to facial recognition software?", "4d44a2ef-705b-431c-bba6-6eff589eb19b": "In what ways do companies use surveillance software to monitor employee discussions about union activity?", "59651941-5e49-41d8-bc3d-9c5636f97522": "What are the suggested actions for establishing organizational roles and procedures related to GAI incidents?", "bb7a66b6-4d1d-417b-b7ca-a931d048c453": "How are the roles and responsibilities for managing AI risks communicated within an organization?", "b4a84cb9-3cfe-4b1c-9f6b-2a75c372584e": "What are the five principles outlined in the Blueprint for an AI Bill of Rights?", "8495c971-2834-4aa4-b622-e2416649d841": "How does the Blueprint for an AI Bill of Rights aim to protect the rights of the American public?", "68f00089-09f3-426d-8645-396bf9a16bf3": "What criteria must federal agencies meet to retain data about an individual under the Privacy Act?", "56cd8376-c476-43e6-ad2e-20ba1b8426f5": "What rights do individuals have regarding their personal information stored in federal systems of records according to the Privacy Act?", "03b543bb-1ef8-4064-b5cc-4293df0ba9e8": "What are the benefits of involving large groups of AI red-teamers in exercises?", "be72edea-ff94-4257-8723-12f6a992e902": "In what situations might AI red-teaming exercises need to combine both specialists and non-specialists?", "8f9fcbd0-7981-4444-8641-710f5d8a582e": "What measures have been taken by the related company to address the bias in social media comment blocking?", "3af76725-6591-4dc5-8e40-a2e7707d4767": "How do search engine results for terms like \"Black girls,\" \"Asian girls,\" and \"Latina girls\" reflect issues of sexualization and representation?", "d556e7c5-a187-4f35-b570-1e64a065f204": "What are some of the extraordinary benefits brought about by automated systems mentioned in the context?", "884e3c8b-c0e7-4e16-ba22-abfe38f1a3e9": "How does the context suggest that technological progress should align with civil rights and democratic values?", "0cf8120a-66d8-4673-9b9b-5eff48a1ab54": "What are the potential sources of latent systemic bias in GAI system training data?", "fd891bc7-9f6b-481b-8bc2-c823d3b7d811": "How might the digital divide affect the representativeness of demographic groups in GAI system training and TEVV data?", "583ed22f-9d24-4cf3-89d4-d8e63bcae3da": "What are the potential risks associated with increased attack surfaces for targeted cyberattacks mentioned in the context?", "982dbab6-0bad-4036-8fd8-a876896630a0": "How does the context describe the impact of eased production or replication on intellectual property rights?", "11b668f1-c709-4076-ac60-01cd89e83e56": "What are the suggested actions to prevent GAI systems from generating content that violates the law?", "b2b6189f-a512-439d-896b-d714c85405d1": "How should organizations establish transparent acceptable use policies for GAI?", "a28131e5-5ca5-4c33-b431-8a5e293605a9": "What are the key considerations for assessing GAI vendors and tools against incident or vulnerability databases?", "5bf61bad-d4c8-4f1b-b050-10b9eb8067f1": "How should GAI acceptable use policies be updated to address the involvement of third-party personnel?", "a53eaae0-5966-4fb5-973e-f52aed4504e0": "What are the potential harms associated with GAI-generated obscene or degrading content?", "d14205f7-d215-4670-8056-5cc6fda7d594": "How does the prevalence of generated non-consensual intimate imagery (NCII) affect efforts to find real-world victims of child sexual abuse material (CSAM)?", "fc133689-db4f-45ba-9583-137d2c106f4e": "What are some examples of structured public feedback methods used to evaluate GAI systems?", "2f81df52-3a00-43d2-903c-71a2a59e158b": "How might gaps between benchmarks and real-world use of GAI systems be affected by prompt sensitivity?", "13f01366-a968-4eee-8550-1d1823c737ea": "What are the risks associated with the use of third-party components in GAI value chains?", "d149ebac-bb46-48a4-aa2b-f9cb607f7e1f": "How has the generation of synthetic NCII and CSAM evolved in terms of its presence on the internet?", "4a2da819-5658-4163-8119-3376a4630aa3": "What are the key components that should be included in incident response plans for third-party GAI technologies?", "51b62ad3-7982-4bf7-b987-d82a4f642e6f": "How can organizations improve their incident response plans based on retrospective learning?", "5e49492b-34e0-47fa-8b06-f867e531ba1e": "What are the key organizational processes and procedures that need to be established for post-deployment monitoring of GAI systems?", "7a63a1f9-9f76-49fc-9aed-b57b23daa7bb": "How can sentiment analysis be utilized to assess user sentiment regarding GAI content performance?", "641f7cd1-e718-4a9d-9c76-14f2396e2cec": "What measures should entities take to ensure the accuracy and completeness of their data?", "b5aad58a-6580-4ee1-a776-e9b77316afdd": "How should access to sensitive data be determined according to the principle of local control?", "0ef9b874-ecb2-4ef0-8185-a799a89df06b": "What are the main energy and carbon considerations discussed in Wang, X. et al. (2023) regarding the fine-tuning of BERT?", "2829aa3a-2e25-45f1-93f7-a6f2e4eab423": "How does the dataset \"Do-Not-Answer\" contribute to evaluating safeguards in large language models, as presented by Wang, Y. et al. (2023)?", "47bae431-a432-4721-805d-d99841bc074b": "What are the suggested actions for managing AI risks associated with third-party GAI resources?", "e465f445-bfa3-4e73-af48-6db571aa7455": "How should organizations apply risk tolerances when utilizing third-party datasets and models in AI deployment?", "7ae3fa17-3050-4aaa-91dd-5976bba1d7d2": "What is the purpose of the technical companion mentioned in the context?", "a379ce8c-b367-43ef-868c-ca8b373932b3": "How does the section \"WHY THIS PRINCIPLE IS IMPORTANT\" contribute to understanding the principles related to automated systems?", "e4fcd729-4f79-4d1c-b2e6-d2ee57b90127": "What are some examples of sensitive domains that require enhanced data protections?", "93fa3ce9-5961-4363-98d7-a73f82ab9c5f": "Why is it important to provide extra protections for data related to sensitive domains?", "8733fab2-2cb6-4c79-b549-9f87f590f6f3": "What actions should be taken to monitor GAI systems and report on their performance?", "e63d3e63-4040-4324-8b5f-9db902f554b1": "How can organizations address and prevent the generation of inappropriate or harmful content in GAI systems?", "649b6fba-6665-465f-b616-e10b0aa87a9a": "What is the main focus of the Federal Trade Commission's report titled \"Data Brokers: A Call for Transparency and Accountability\" published in May 2014?", "77b1b087-d26e-42a6-86e2-e73ee64d2264": "How does Shoshana Zuboff's book \"The Age of Surveillance Capitalism\" relate to the concept of social media surveillance by the U.S. Government?", "99a25e0f-f613-4906-bfa3-1f59f7578b41": "What processes are defined, assessed, and documented to ensure operator and practitioner proficiency with AI system performance and trustworthiness?", "5af0b0d7-19a9-477a-8c71-210158174b72": "How can existing training programs be adapted to enhance understanding of digital content transparency?", "18280e9c-b6b9-43e1-a72c-2c8c777e2ecc": "What are the main findings of Zhang et al. (2023) regarding people's perceptions of generative AI compared to human experts?", "a0417d68-27e1-4f69-b8e6-5c4d4d3bce0b": "What is the focus of Zhao et al. (2023) in their research on AI-generated text?", "f847bd16-3186-4fe0-b0da-8fcb4328190d": "What should be monitored and documented regarding human operators or other systems in relation to the GAI's decisions?", "3590d8be-1272-435c-aa15-82ef17fbc630": "How should the results of structured public feedback exercises be incorporated into the decision-making processes for AI deployment and monitoring?", "7bcf1232-6a4b-4e5d-a59f-1dd83a8f27f1": "What are the three categories of bias in artificial intelligence identified by NIST's Special Publication 1270?", "cfaed751-ceb1-4391-bf53-f69d7c995acc": "What is the purpose of the Access Board\u2019s Section 508 regulations in the context of technology design processes?", "da0ce0db-5de0-46ea-9590-960fc8f3255f": "What are the concerns raised about the impact of virtual testing on disabled students?", "e428c5fb-d0f5-451f-8853-6196d4f0efd4": "How have cheating-detection companies profited during the pandemic according to the Washington Post article?", "1882074f-f8fa-4b42-ae0d-623f85b10328": "What are the five principles outlined in the Blueprint for an AI Bill of Rights?", "35a2027e-8c4e-420b-a6cd-b5c8981aa1ec": "How can communities and governments implement the practices suggested in the technical companion to protect the rights of the American public?", "5f68ea59-6563-4780-8657-6cac24c7258c": "How does the creation and spread of NCII disproportionately impact women and sexual minorities?", "7c6e11b5-7a96-43da-8dd0-413ed3195ee6": "What are some of the negative consequences associated with the spread of NCII?", "6a4861ae-cc49-4bb2-93e4-a26056835633": "What is the purpose of the effort mentioned in the White House article regarding a bill of rights for an automated society?", "008b3082-a083-4c80-87a0-e2d326ea1b90": "What are some key documents referenced in the context that relate to the rights of citizens and the protection of personal data?", "7b336f26-dda0-40ef-9d8f-6add6391f80c": "What are proxies in the context of algorithmic discrimination, and why should they be avoided?", "91c6a460-c1f5-46da-8f90-b96979abc2b3": "What steps should organizations take if a proxy is identified in their system design or data?", "ec1dd89b-0430-4db3-b8ea-5773a62a467b": "What roles do the panelists play in the context of social welfare systems and social development programs?", "eb143311-e998-4f66-8f05-3f71b40d30fd": "How might the insights from the panel discussion impact the implementation of systems that affect life chances?", "234bfb1e-dec6-4b4e-9591-aaaa6adf8960": "What are some examples of automated systems that can produce inequitable outcomes?", "32cc9210-3b28-4ff0-a863-a2064a2030d2": "How can systemic biases in American society affect the data used in algorithmic decision-making?", "e53f71d6-2f8e-49d3-bf6d-6f5752b82330": "What types of data are considered sensitive when generated by individuals who are not yet legal adults?", "8ac67b6a-ad42-43f0-89c7-2bb5374b1314": "Which domains are identified as sensitive and deserving of enhanced data protections?", "5b2a4346-1ea7-4042-b11c-79e76d515db5": "What are the potential risks associated with revealing sensitive information in the context of membership inference?", "30cc89d4-6920-44bd-8bd3-1e7d56108a3f": "How can feedback from end-users and stakeholders be utilized to improve provenance data-tracking techniques?", "a39f37c1-2f9c-4f7f-b589-5dcaeeae9406": "What procedures must be established for escalating GAI system incidents to the organizational risk management authority?", "799f814c-2bb7-4504-bf31-81da7898dff5": "How often should the specific criteria for the deactivation of GAI systems be reviewed in relation to risk tolerances and appetites?", "fe1c6cf7-70ba-49f5-ba55-3395951dccab": "What are the expectations for automated systems in sensitive domains according to the provided context?", "77298fee-c5dd-4c64-b7db-5ad2641d983a": "Why is human oversight important for automated systems used in areas like criminal justice and education?", "7e8df5fa-97a9-417a-a530-4ff97c938f2e": "What are the suggested actions for addressing the risks associated with GAI content provenance?", "26ed11e7-65d2-4671-8875-cbbd905966d5": "How should organizations rank the potential harms of GAI, and what factors should be considered in this evaluation?", "ced547f5-5dd0-4dff-9055-8a7c953aa687": "What types of unacceptable use should be identified through stakeholder input according to GV-1.3-004?", "238caa23-8d02-4cb5-8eb0-cc21d074dda5": "How should organizations reevaluate their risk tolerances in relation to GAI negative risks as outlined in GV-1.3-006?", "707e05bd-211f-495b-a35b-d0aa5b59bbb3": "What types of systems are used in mortgage underwriting and home insurance to estimate home values?", "fb0f2683-c2aa-43a4-97fb-1b92bf4f9ae4": "How do workplace algorithms influence employment terms such as pay, promotion, and hiring?", "c6bf4149-3386-4219-8a8a-8c86e7d4b567": "What are the potential risks associated with the impersonation and cyber-attacks mentioned in the context?", "117b8b57-3c7b-4948-9e7c-7f9ef12cb676": "How can regular evaluations of GAI system vulnerabilities contribute to enhancing information security?", "46096ec7-2d54-45a3-8f22-4e978d0c557c": "What is the importance of training for individuals interacting with automated systems?", "8c617963-336f-46d9-bc51-90663a4e118c": "How can ongoing assessment help mitigate the risks associated with human involvement in automated systems?", "a286d90b-8777-4add-b96c-bd9459e86ac2": "What are the potential risks associated with the design, training, or operation of GAI models?", "2bbe3b74-88c0-4890-ae60-839a9ec6c50b": "How can the phenomenon of \"algorithmic monocultures\" impact decision-making in employment and lending?", "0918e42e-634f-4863-9b7c-baa048a809f2": "What percentage did Google cut racy results for searches like 'Latina teenager' according to the Reuters article?", "475b8773-8cce-4d3e-abf6-91b38d5aa9f4": "Who is the author of the book \"Algorithms of Oppression: How Search Engines Reinforce Racism\"?", "ecc74d6c-998a-4034-8a19-612e477bdb8d": "What procedures should be established to engage teams for GAI system incident response according to GV-2.1-002?", "3bbf1ab7-380c-4c5a-b37f-e4f9d04f9e1f": "How can organizations ensure that AI Actors involved in GAI incident response possess the necessary skills and training as outlined in GV-2.1-003?", "8f2966b5-3c77-4464-98c2-cac2e663fefe": "What are the novel methods and technologies being evaluated for measuring GAI-related risks?", "bff58f34-4182-4a36-a611-2850128479fe": "How do the models ensure valid, reliable, and factually accurate outputs while assessing risks in content provenance and offensive cyber?", "d9284a2f-ec74-4904-9e0e-f9372fe19e54": "What organizations are associated with legal defense and civil rights in the provided context?", "01828659-89dd-4d87-af34-9f6ef54c8d69": "Which technology companies are mentioned in the context?", "dcedfb96-97f5-414f-8966-a192a5e8cf20": "What commitment is included in the Action Plan to Advance Property Appraisal and Valuation Equity regarding Automated Valuation Models?", "84ef7097-cd99-4fa9-9d5d-b94f706fbec5": "How can the use of AI and automated systems by employers lead to discrimination against job applicants and employees with disabilities?", "dfdb907a-1d3f-483b-a3fb-40d375f9ae67": "What are the legal requirements and judicial oversight related to seizures?", "f9b97959-12b9-4f34-a80a-a4b15dbd8b4f": "How do civil rights laws protect the American people against discrimination in the context of criminal investigations?", "88972501-7b50-4d8f-bb1c-5f6465dd3588": "What are the potential cybersecurity risks associated with querying a closed production model in GAI systems?", "2c30006f-83fd-42ba-be5d-2b158ea469d2": "How can data poisoning affect the outputs or operation of a GAI model?", "99065c49-5993-4a5e-859e-38fe3549d822": "What concerns have parents and education experts raised regarding the collection of sensitive data related to students?", "4b913cd1-f8c2-4d2b-bc43-4670d8ffc69d": "How can the transfer of employee data to third party job verification services impact former employees?", "bb2ab5d2-5d33-46bf-bf8d-adc62375d2e2": "What protocols are suggested to ensure GAI systems can be deactivated when necessary?", "75af3581-8e4b-4a5e-a0a5-b487c8be009b": "What factors should be considered when decommissioning GAI systems?", "5509b57b-6088-48e4-85ec-d948b231ad5c": "What are the two types of discrimination mentioned in the context?", "34029bf8-f63d-4308-a8bc-2e0c68139750": "Why do formal channels for reporting AI incidents currently not exist?", "9ca51517-5f29-4fa0-aad5-1ddc9d958b1e": "What are the special requirements that adversaries are often subject to regarding classified information and protected data?", "a8429f34-ca7e-493f-aa4e-f422232da1ce": "How can the implementation of the Blueprint for an AI Bill of Rights inform national security and defense activities?", "f7dcacbd-9ed9-4887-b50f-c6d0cd992107": "What information should be included in a machine-readable report about a user's data and metadata?", "f3e38776-3894-446e-b04e-44d0351bcd2d": "Why is identity verification necessary before providing a report to users when a login is not available?", "c5e8a4f8-ed05-473f-ac39-d9063b675bf8": "What are the key considerations for establishing policies related to data collection and retention as mentioned in the context?", "c09fba32-4fd4-4db0-b28b-cd6d4668ba86": "How does the context address the risks associated with training data imbalances and harmful biases?", "95ce6f6a-7c3a-41f4-94a3-ba8cdb5c1637": "What is the main focus of the article by Duhigg (2012) regarding shopping habits?", "0c5e9a81-12af-4d80-9d33-5ba7f3af0f49": "How do the findings of Elsayed et al. (2024) relate to the influence of altered images on human perception?", "784931e2-64cf-43b8-8686-c4d0290268ad": "What are some examples of how automated systems impact people's lives in areas such as employment and the courtroom?", "0b95f225-39ed-4da4-9334-f94e3e84f43e": "Why is it important for individuals to have knowledge about the automated systems that affect their opportunities and decisions?", "24124845-94a1-4719-9c89-65afbe4b5606": "What are some proposals mentioned for designing algorithmic impact assessments?", "c000979d-175a-44f8-963c-73766bd0b5c0": "Who are the authors of the report titled \"Assembling Accountability: Algorithmic Impact Assessment for the Public Interest\"?", "c9d8b913-470e-43a1-8492-3e0be609ef01": "What factors should be assessed to determine the expected and acceptable GAI system context of use in collaboration with socio-cultural and other domain experts?", "d08e03b2-19ea-4bbc-b0c5-499d79447d07": "What elements should be included in risk measurement plans to address identified risks associated with GAI systems?", "64a2d688-a503-478b-9f9e-c29e4804ce31": "What is the purpose of documenting how the system relies on upstream data sources?", "5ca7dbe2-e28e-458d-a95c-1deb0595e2cd": "What potential risks are associated with the GAI system's interaction with external networks?", "0e41f2c3-6d31-41ef-ab65-f9119582cfdb": "What is the focus of Section 2 in the Table of Contents?", "3a47e1f6-c619-41a3-b589-24cc61613716": "Where can one find the Suggested Actions to Manage GAI Risks in the document?", "0e4753cb-8424-477b-9ce5-babba7652abc": "What are the potential consequences of overly homogenized outputs in AI models?", "85f5d0a6-14f4-4472-8fa6-bbe7b8185f76": "How can model collapse affect the robustness of a model and its outputs?", "71cdaad0-bf94-4c3a-b1e0-fcba011f6c44": "What mechanisms should individuals have to control access to their data according to the context provided?", "5afb1559-6342-4bc2-9407-c6294526fbcd": "How should automated systems be designed to ensure privacy is protected by default?", "37dfda02-c719-4590-924a-edc49d84f5fa": "What factors should be considered when assessing the level of risk in relation to explanations?", "437dfbc6-f3f7-4e74-bfa0-d33e98e78d90": "How can individualized profile information be made more accessible and understandable to recipients?", "3970a859-0021-420d-bc71-20ce93ace873": "What are the potential benefits of combining GAI-led red-teaming with human teams?", "b2bf361a-9861-4116-b32b-48a52c3c39cc": "How can digital transparency mechanisms like provenance data tracking help manage risks associated with AI-generated synthetic content?", "f98c5248-24d4-4997-9b8f-fdc917f811d2": "What organizations are listed in the provided context that focus on technology and privacy?", "5cabd937-7d69-4608-a461-772664ca8684": "Which individuals mentioned in the context are associated with academic institutions?", "4e1953d7-6982-4c53-8c2a-7b03daf7d7b5": "What are the expectations for automated systems intended to achieve in terms of technical standards and practices?", "fa705d1a-2487-4214-b296-0d7e42b0eed1": "Why should derived data be viewed as potentially high-risk inputs in automated systems?", "9b67dc74-75c1-4ea0-af82-3b4475fd02ef": "What records should be maintained to promote content provenance according to GV-6.1-008?", "a9fd266e-fd5c-4450-9d82-217382310904": "What aspects should be included in the updated due diligence processes for GAI acquisition and procurement vendor assessments as outlined in GV-6.1-009?", "c8f0f9d9-4dd1-4194-aca3-52b6fab15963": "What are the key expectations for automated systems as outlined in the context?", "fa96eadb-1fab-4b9e-ba49-cd43bf8bf7ba": "Why is it important for the fallback and escalation system to provide equitable access to underserved communities?", "8c64893a-146b-439a-89b5-4e4750acd04e": "What are the key components of the certification programs mentioned in MP-3.4-003 for managing GAI risks?", "862afdc5-1ca1-4a87-a45c-10a511b3965c": "How does MP-3.4-006 suggest involving end-users in the testing of GAI systems?", "45e7f413-1e12-4cbd-9e75-feec63a84920": "What information should designers, developers, and deployers of automated systems provide to ensure transparency and understanding for users?", "313bf6c8-c5ab-4fe3-8d77-26dda05b27ea": "Why is it important for individuals impacted by automated systems to be notified of significant use case or key functionality changes?", "121f9c82-7903-4809-898b-8c426fc28bdc": "What measures should be implemented to mitigate identified risks associated with automated systems?", "e07d6280-119e-411b-b6af-e0451b343fbe": "Who are the independent evaluators that should have access to the automated systems for evaluation purposes?", "4e92b2f3-df8c-4936-9d5b-e3bceebbf7e8": "What are the concerns raised by panelists regarding the impact of surveillance on student expression and privacy?", "62120fd7-38e6-433e-b947-a099ec35b5cd": "How does the use of automated tenant background screening and workplace surveillance potentially lead to discrimination?", "022119c8-675b-4f23-949a-f653e8082a2d": "What is the focus of the article published by The Markup on January 11, 2022, regarding data collection on children?", "d3d8dfc7-cc00-4a04-9c62-9a88959616aa": "How does the job verification process for former Apple employees affect their job titles, according to the article by Reed Albergotti in The Washington Post?", "8759afea-2213-407d-8530-165dab52c701": "What methods are suggested for assessing the impact of AI-generated content according to the provided context?", "4dacfc71-3361-4d75-80ff-5040c7b24d02": "How can structured feedback mechanisms be utilized to ensure alignment of AI-generated content with community and societal values?", "440d320b-5312-4654-b068-bcc9414a8c64": "What are some of the education-related concerns mentioned regarding the use of AI systems?", "3ff43819-bbb3-4c9d-880b-005110a5d27e": "How are AI systems being used in housing-related contexts according to the panelists?", "6ef4accc-35cb-4fdd-bbc4-e5b7ec44b8f7": "What processes are suggested to mitigate risks associated with unexplainable GAI systems?", "1ec1885c-3c1b-4e4e-893c-199c7f143158": "Why is it important to document the sources and types of training data used for GAI applications?", "ccd2d100-7136-410b-a3e9-75d2e7df65c4": "What is the purpose of the Executive Order on Advancing Racial Equity and Support for Underserved Communities?", "d380a44b-f522-4256-8a6c-bbcbe3bf9fd5": "What was President Biden's response to the Supreme Court's decision to overturn Roe v. Wade?", "d7d70876-8523-4c11-b423-b58bde1682e0": "What steps should be taken if a disparity assessment identifies a disparity against an assessed group?", "f4ca6361-9dfd-4774-8b7b-648d8658a665": "Why is it important to separate demographic data collected for disparity assessment from data used for the automated system?", "c64e0ab3-e23e-4c53-9817-d44f69b94258": "What are some appropriate responses to identified privacy risks according to the context?", "bf45fdf0-76d0-4f8b-a480-58147905bf39": "How should entities ensure that data and metadata do not leak beyond the specific consented use case?", "dc9e137d-d095-41a2-93e4-c4e8afaf5f81": "What are some of the activities that AI Actors may engage in when interacting with GAI systems?", "7d9b39c3-7cc3-4470-8877-f18c1a1d2ae2": "Why might organizations need to implement additional human review and oversight when using GAI systems?", "c8373bf3-02d7-42a2-8702-c5c5688a1481": "What are some known past incidents and failure modes associated with GAI systems?", "037654f0-c70b-47b5-809f-1c4d15e85c3a": "How can organizations identify and document foreseeable illegal uses of GAI systems that exceed their risk tolerances?", "af90f57b-c0de-4a66-af9a-8037ec52d099": "What are the potential risks associated with the emotional entanglement and aversion in GAI systems?", "e6ebb88c-b398-4b91-bb03-cc70c6d85b80": "How should deployment approval processes address the reliability and adaptability of GAI system performance over time?", "eb57c278-22b2-4f86-9a60-f01f11c861dd": "How can generative AI models contribute to the creation of disinformation and misinformation?", "c95c78ab-a17a-42ec-a0ce-c8ef2c7106c8": "What impact did a synthetic image of a Pentagon blast have on the stock market?", "a08c38e9-d179-4027-ab4b-298bf9297e56": "What are the implications of digital surveillance in a post-Roe world as discussed by Sam Sabin?", "29d179a7-4799-4b6c-8bcd-7ccbca8f2e0d": "What actions did the Federal Trade Commission take against Kochava regarding data tracking at sensitive locations?", "21827305-0823-497f-b15b-599ccb2a839a": "What should entities responsible for automated systems provide regarding algorithmic impact assessments?", "c82f1f3a-fb42-449d-b898-cb7b068f91c7": "How can access to evaluation data be achieved without compromising individual privacy?", "879eea4c-1d4b-45dd-834a-cfe6195dfe0b": "What are the key expectations for automated systems as outlined in the context?", "63c0c80f-78df-4c6f-978c-35fd98b2f548": "Why is it important for the documentation of automated systems to be in plain language?", "c2da4324-ea8c-4489-9f85-4f656f87a83b": "What are the key principles outlined by the U.S. Intelligence Community for the ethical use of AI in national security and defense activities?", "5b540966-663f-4d68-90d9-12953e58080b": "How does the National Science Foundation contribute to the development of safe and effective automated systems?", "56c9f95e-7504-4971-ade0-7e8bb9af0a70": "What measures should be taken to protect the public from algorithmic discrimination during the design phase of technology development?", "aa41d8b7-3f96-47e1-9e49-b70c8c028303": "Why is it important to conduct proactive equity assessments in the development and deployment of automated systems?", "6caf288d-335e-47a2-ae83-ce828c8726ac": "What is the role of the Information Technology and Innovation Foundation in relation to the International Corporation?", "5f0826be-9e91-4723-8c49-238a63f1ee75": "Who are some of the individuals associated with the context provided, such as Jennifer K. Wagner and Joy Buolamwini?", "a0e8e0fa-b5ba-4212-8ca4-e11f772ec3cd": "What roles do the panelists play in the discussion of AI-enabled consumer products and smart city services?", "bd0df280-49a6-46ff-a08e-e2fd030dedae": "How does the context of individual consumers and communities relate to the advancements in IoT devices?", "ffbbebbf-8490-4b4f-82e0-fa7bc3326dc6": "What was the purpose of the Blueprint for an AI Bill of Rights published by the White House Office of Science and Technology Policy?", "427a26a8-9d70-44ba-98c7-900a134e9647": "When was the Office of Science and Technology Policy established, and what is its primary function?", "05e99477-e456-498c-b40e-1438e37d559b": "What are the key considerations for ensuring accessibility in the design, development, and deployment of automated systems?", "482ad7b6-78a3-4465-822c-7619b62a6239": "Why is it important to conduct disparity assessments for automated systems before and after deployment?", "17e92a1e-b7d8-471f-8761-c48a2585ce48": "What types of harmful content should be analyzed in GAI output according to the context?", "9758304b-6b26-4c4c-a495-cf0db71fff15": "How does the context address the issue of harmful bias and homogenization in generated content?", "6685a18c-d7cf-4533-a22f-5403166a2fe8": "How does automated test proctoring software discriminate against disabled students according to the provided context?", "0a2c3b6d-6a54-4ac4-b9b0-f13a48fd4de2": "What is the significance of the study by Ziad Obermeyer et al. regarding racial bias in algorithms used for health management?", "d812ff42-89e2-4ec2-88db-65fb862e260e": "How can inaccuracies in labels affect the robustness of benchmarks in GAI model selection?", "08502bf6-3cc8-43cf-a6d4-fbb9dd4bc94e": "What are some suggested actions to manage risks unique to GAI systems?", "c1ba6a8a-f2c2-4e31-9c7e-87cb44353015": "What are some examples of technical or model risks associated with advanced AI as mentioned in the report?", "5e6b5891-f31d-40dd-9de9-a689975eb1d6": "How can advanced AI be misused by humans, according to the scientific report?", "eef23962-ae8c-4048-aa9b-a42e6934d7f0": "What is the main concern highlighted by Andrew Thompson regarding Google's Sentiment Analyzer?", "2e1b4f00-30d4-4781-883d-07335ae17f31": "How does the research by Lucas Dixon and colleagues address the issue of unintended bias in text classification?", "ace5291b-d405-4032-bce1-2192a8287c53": "What concerns did the panelists raise regarding the validity of data-driven systems in relation to safety?", "92b6ba26-57a0-4144-99e4-46847b9db0ad": "How might the use of automated systems impact individuals and communities, according to the panelists?", "54662c3c-3b4d-4b68-822e-302640e48167": "What is the purpose of the AI Bill of Rights mentioned in the context?", "4f17f355-e531-4dfd-a69e-d17f5127eb47": "When was the Blueprint for an AI Bill of Rights published?", "d3015184-067a-4f85-828d-9419f38ebe7d": "What impact does the lack of notice about data collection have on the validation of risk assessments in child maltreatment cases?", "b9db7225-3aab-4890-b433-9163917b2cbd": "How does the absence of an explanation regarding data usage affect parents' ability to contest decisions made in child maltreatment assessments?", "00eb2463-330c-4b4c-a333-af8ea394402a": "What information should designers, developers, and deployers of automated systems provide to ensure transparency about the system's functioning and its impact on individuals?", "562255f8-2171-4089-9ac6-e94959d127d7": "Why is it important for individuals impacted by automated systems to be notified of significant use case or key functionality changes?", "b0c27726-d570-4f03-8c13-8d09fe49138d": "What mechanisms should automated systems provide to allow individuals to opt out in favor of a human alternative?", "217093c3-c905-4a5b-8785-667c6d76c50a": "Why is human oversight and training important in the context of automated systems, especially in sensitive domains?", "91ebb5f1-6ac6-4f16-ac97-d9758ae20b12": "What was the purpose of the series of panel discussions co-hosted by OSTP?", "dd0b92f6-85e4-45b7-bfd4-1f462556d878": "Who were the collaborators involved in the panel discussions for the Blueprint for an AI Bill of Rights?", "6f806534-7869-4fb6-8e06-0354094e316f": "What frameworks have U.S. government agencies developed for the ethical use of AI systems?", "f6b3d5a2-7890-4977-81d9-eaf14525e139": "How does the Department of Energy's AI Advancement Council contribute to the implementation of the DOE AI Strategy?", "0a1c8be2-2819-4e53-9e67-484f320af15b": "What is the purpose of the National Artificial Intelligence Initiative Office as mentioned in the context?", "12fb84d3-58a4-4149-910e-c65c4c60bf1c": "How does the U.S. Department of Transportation describe the benefits of traffic calming measures?", "aa81ac5e-a134-46e2-9095-8d17698fc227": "What are some of the significant negative impacts associated with the design, development, and deployment of GAI systems?", "cd1fd036-4c79-45e2-abd0-6b59198164f3": "How can organizations ensure that their risk management processes for GAI are transparent and aligned with their risk priorities?", "e66ea9b6-691f-4df9-b796-b98a9e6d23bd": "What are some potential harms associated with the reliance on unproven technologies?", "f7ba5f62-d047-4b12-a8b9-8204de59843c": "How can historical data impact the decision-making process in automated systems?", "ab7d779b-083d-40e9-b1bf-3d921bec998f": "What methods did the White House Office of Science and Technology Policy use to gather input from the public regarding algorithmic and data-driven harms?", "19251687-f898-4279-9132-c2a5940cdfce": "How did the contributions from various stakeholders influence the development of the Blueprint for an AI Bill of Rights?", "388c31f3-d681-485f-9cc6-ca65bc53c856": "What is the purpose of the NIST AI Risk Management Framework?", "1307a1ac-da37-457b-a59d-20498d2a4bd2": "How does the OMB suggest improving stakeholder engagement in program and service design?", "cc9da629-4b96-4240-82f0-af0c190e0ceb": "What should be the level of responsibility for establishing governance procedures within an organization?", "026ccc65-6bb2-4bbc-a725-c2416f258492": "Why is it important to conduct an independent ethics review before the deployment of certain use cases?", "1fbcb8ba-e9ce-4904-ad0b-587ac0ca27ce": "How are the risks associated with GAI categorized according to their outcomes, objects, or sources?", "ac69bdaa-4d02-4dec-91e0-f39355715fec": "What is the purpose of mapping risks to relevant Trustworthy AI Characteristics in the AI RMF?", "68a41bf6-0d87-46bd-91bd-7f2fb4edfc55": "What is the purpose of the expectations for automated systems as mentioned in the context?", "2a6f16db-12a8-4e6c-9f9f-2edcf928370b": "How should independent evaluations of algorithmic discrimination be handled in the public sector according to the provided context?", "6b3c9c23-eb38-461a-94f9-07a7f6965229": "What are some sensitive domains where automated systems are used, and what risks do they pose without appropriate safeguards?", "16cf78bb-c4d0-4144-8d10-e6dbf976f4e0": "How do existing human processes complement automated systems in providing public access to government benefits?", "7a090519-47bd-4909-a436-068b8936a9fc": "What is the title of the document published by the U.S Department of Defense in June 2022 regarding artificial intelligence?", "63fbc7f8-f4b7-4c3a-a80b-e5dfcc25d2a3": "Where can the Principles of Artificial Intelligence Ethics for the Intelligence Community be found?", "37be8050-9dc9-43fc-bcb1-8fe0b7f32687": "What is the focus of the Canadian Centre for Cyber Security's guidance on generative artificial intelligence?", "22b42e66-fad3-4c65-acf9-b1f9dcc1cd01": "What are the main themes explored in the paper by Chandra et al. (2023) regarding Chinese influence operations?", "f25bf8d4-4535-4d77-8166-083f5a935d4c": "What are some potential risks associated with the measurement gaps in assessing GAI systems in real-world conditions?", "179ad3a6-1cf4-4f3b-8fe1-9a6e2ceb1bf7": "How might prompt sensitivity and the heterogeneity of contexts affect the validity of GAI system assessments?", "bab93157-ef62-43c6-8018-20ff0edb8689": "What steps should be taken to mitigate or eliminate disparities in automated systems?", "445b3544-ceff-4332-a602-54b32165a543": "Why is it important to evaluate multiple models when designing an automated system?", "3e608b4f-e830-4765-b9de-59b7efa36964": "What are the implications of the principles described in the context for U.S. government international negotiations?", "4153e7e9-83d0-45a2-8149-60453ae3473e": "In what circumstances might the application of the principles in the white paper be deemed inappropriate?", "4990a25e-ff36-4112-9653-0361ff5119e8": "What are the main focus areas of the National AI Research Institutes?", "01051c46-d951-4b59-a96f-42e4a1d12fc1": "How do state legislatures influence the use of algorithmic pretrial risk assessments?", "9546a589-c675-4224-91fa-60446c99f3a9": "What is the purpose of the equity assessment in the context of the safety and efficacy review?", "00334ec7-eb04-4c2e-93eb-88f350be5685": "Why is it important for data used in system development to be representative of local communities?", "b67c46ce-3c43-4127-acc6-e31e06a44421": "What methods can be used to measure the reliability of content authentication techniques in relation to content provenance?", "7956197e-9ca3-4a9b-9571-fd584a27af8d": "How can the rate of false positives and false negatives in content provenance verification be evaluated?", "2c042492-9062-44ca-80f2-3cb7781ad055": "What requirements does Idaho Code Section 19-1910 impose on pretrial risk assessments before they can be used in the state?", "e5513860-660f-423f-90a6-af0fd082a932": "How do civil rights groups view the use of algorithmic pretrial risk assessments?", "e54fa07e-3e07-4f1a-af95-238db8f73285": "What types of systems are mentioned that impact the safety of communities?", "c57d39eb-607d-4c41-a5fe-f4a3a36fb313": "How do systems related to access to benefits or services assist decision-makers?", "617f86d8-c8da-425d-a175-22bd7a077c22": "What are the Fair Information Practice Principles (FIPPs) and how do they relate to the AI Bill of Rights?", "92949a62-8971-45ec-84b2-7b29baa30396": "How does Executive Order 13985 influence the principles outlined in the AI Bill of Rights?", "209e3cbb-a414-4733-a855-3ad2990a57aa": "What is the purpose of the AI Risk Management Framework (AI RMF) for Generative AI as outlined in the document?", "b1caf441-4206-4bcb-8539-e94ae8e1d427": "How does the AI RMF pro\ufb01le assist organizations in managing AI risks?", "c138f871-ee8c-4f1a-a56f-9ac1d5b6bab6": "What measures should be taken to ensure that automated systems do not endanger safety?", "5db37c4b-566f-42fb-8980-de127941c908": "Why is independent evaluation and reporting important in the design and deployment of automated systems?", "871398e8-debb-42ea-be42-fa7343670b8e": "What are the methodologies mentioned for measuring the effectiveness of content provenance in GAI risks?", "19ae23dd-fca5-4dd2-a904-c7b3022a8b36": "What minimum set of criteria is necessary for GAI system incident reporting according to the provided context?", "7b1e72ef-6fac-4dfd-baa9-116b76741bbf": "What is the purpose of the White House Office of Science and Technology Policy's initiative regarding a Bill of Rights for an Automated Society?", "ca802ba8-80cc-4d3c-916f-4bb4da90857b": "When was the Notice of Request for Information (RFI) on Public and Private Sector Uses of Biometric Technologies issued?", "c0812497-006d-433b-9793-bed7ef0ed900": "What are the emerging areas of study related to the potential deception of humans by LLMs?", "c0118b13-51c9-4edd-a1dd-f6c71f7f7919": "How do GAI systems compare to other technologies in terms of producing dangerous or violent content?", "fe67901b-170c-4b55-82e5-e42f6dbb4141": "What are the suggested actions for managing AI risks that do not surpass organizational risk tolerance?", "ff3ac23f-c5b2-4075-9465-e290331fa4a4": "How should organizations monitor the effectiveness of their risk controls and mitigation plans?", "c8c5653b-1645-45d5-b580-8c2472a5cf52": "What should organizations leverage when deploying GAI applications and using third-party pre-trained models?", "5357ba14-cdfd-428e-988c-83f2294fbdc5": "How should human moderation systems be utilized in relation to generated content and human-AI configuration policies?", "f86f5106-d2b9-4129-beb1-5c6dfbd75f4e": "What are the main themes discussed in the paper \"Algorithmic monoculture and social welfare\" by Kleinberg et al.?", "b789df15-89b6-4057-a27c-ed90ba21939e": "How does the report \"Data Poisoning: The Exploitation of Generative AI\" by Lenaerts-Bergmans address the risks associated with generative AI?", "f0c44885-a26d-4f90-a581-2d38fb40074c": "What is the title of the article by Mick Dumke and Frank Main published in the Chicago Sun Times on May 18, 2017?", "15446d6f-74b6-4174-842e-72b4e89501be": "What legislation is referenced in the context that became effective on October 3, 2008, regarding biometric information privacy?", "dd2eac10-e764-4b4a-9032-8ff57e3c6b73": "What are some potential biases associated with human-based systems?", "d3d6b54e-fb46-4e28-90e1-b953afa93dc2": "How can governance structures help mitigate the effects of bias in human-based systems?", "2ae0d18e-e434-4ce6-9686-cf59fb6b3092": "What processes are suggested for identifying emergent GAI system risks according to MEASURE 3.2?", "8d5f03ff-8c34-499b-a4e1-d8157029ee0f": "How are feedback processes for end users and impacted communities integrated into AI system evaluation metrics as outlined in MEASURE 3.3?", "65bb1d8a-fd18-4afc-8b62-6ad095a61e8a": "What is the main focus of Tirrell's (2017) work on discursive harm?", "afa72550-84ad-45e4-a029-7c5e2d17b4e7": "How do Tufekci's (2015) findings relate to the challenges of computational agency beyond major tech platforms?", "0573cf74-c634-44d5-ba70-af3350e0bbca": "What factors should be considered when calculating error ranges for decision-making explanations?", "3f032e60-78c1-49e2-a6da-dc2cce29bfb0": "How should the clarity and timeliness of notice be assessed in the context of reporting on decision-making information?", "a34c6721-e7fe-426b-8d12-dcf1c65faee3": "What is the focus of the model cards framework mentioned in the context?", "2f71f30b-36ab-4786-9205-0bd781b5a408": "Who are the authors of the paper \"Model Cards for Model Reporting\"?", "28d50e27-0577-48c4-b4c2-18d92edbe514": "What is the purpose of utilizing a testing environment like NIST Dioptra in evaluating GAI trustworthy characteristics?", "c3b0bbc5-7fd2-4d2c-b36c-51e3b6ccd88a": "How should the limitations of generalizability for the AI system be documented according to MEASURE 2.5?", "cac32498-70ce-4097-8315-7caf5147c141": "What benchmarks are suggested for quantifying systemic bias in GAI system outputs?", "7a88bfa7-50b0-401d-8ca3-47601dc1cbdb": "How should fairness assessments be conducted to measure systemic bias across demographic groups?", "4d584453-9d84-4560-9948-05662b0fcbb5": "What steps should be taken to assess the intellectual property and privacy risks associated with training data use?", "284f5fbd-7f7b-4580-8d6a-4047300640db": "How can the likelihood and magnitude of identified impacts from AI systems be documented and evaluated?", "6b3b55b8-dfd0-47a8-9fec-c1b57ad6267f": "What is the purpose of maintaining a document retention policy in the context of GAI?", "9e2647bc-80f5-449c-87bc-d56c9646f370": "How should organizational policies address inventory exemptions for GAI systems embedded in application software?", "f06bc859-c6e9-4df7-8d4e-634dd2074454": "What does the article by Maia Szalavitz discuss regarding the treatment of chronic pain and the response of doctors?", "a1351a2a-f2a3-4a86-9f05-8fd9b9ff4a42": "How does the Bloomberg article by Spencer Soper illustrate the impact of automation on employment at Amazon?", "6b6e0162-a818-4f70-8674-d54933989ad9": "What types of organizations can take concrete steps to uphold the values outlined in the framework for responsible use of automated systems?", "43379da9-6bad-49b5-9c91-b4f0f88e2310": "How does the Blueprint for an AI Bill of Rights inform policy decisions where existing laws or policies do not provide guidance?", "df684eeb-3334-4b3c-950b-22367a3ade89": "What are the recommended practices for obtaining user consent for data collection?", "61856785-5c4e-49e7-89e6-4050a55fa96c": "Why is it important to have enhanced protections for data related to sensitive domains?", "90d869b2-cee9-4c56-915c-654fda9f227a": "What are the criteria for the kinds of queries that GAI applications should refuse to respond to according to the acceptable use policies?", "c1f75d6c-27ed-40b9-befc-ff1e65902b17": "How should user feedback mechanisms for GAI systems be structured to include instructions and recourse options?", "7b44d3b4-c8dd-4a73-86f4-c0d5ba039c75": "What measures should be taken to review training data for CBRN information and intellectual property?", "20bbee03-1540-4db2-afcc-b591cdd559be": "How should organizations reassess model risks after fine-tuning or implementing retrieval-augmented generation for third-party GAI models?", "86761d95-7806-4069-b9d1-bb0ae160b5ed": "What is the purpose of the expectations for automated systems as mentioned in the context?", "a43a5b18-9f0f-4c42-ad67-43380c0d49a2": "How should the level of explanation for automated systems be determined according to the risk assessment?", "7cc29955-e340-4430-b146-a7611e709c88": "What should be established before deploying an automated system to ensure proper oversight and governance?", "f4305e3d-9a16-4b92-85bc-27cf25adb3c9": "Who should be involved in the process of establishing governance procedures for automated systems?", "9c482bbc-b948-48c5-b071-cc7f6fafcbee": "What does the Biometric Information Privacy Act in Illinois require from private entities regarding the use of biometric information?", "04f91e5c-4ff4-40c4-b2af-bc0a75bbf9a3": "How are major technology companies attempting to improve communication with the public about biometric data usage?", "45a8e235-2618-469c-9f66-d93a6a38d3fa": "What are the key processes involved in monitoring system capabilities and limitations in GAI deployment?", "a867b1e7-46bb-46fb-b6f7-03e6d5e53658": "How can organizations utilize provenance data to improve risk management efforts in GAI systems?", "19227b4b-3748-401c-82e1-f65eec7e1987": "What issues arise from the automation of performance evaluations in large corporations?", "9201bc25-550e-4250-999b-748722a2eb6e": "How did a software error impact a patient's access to pain medication in a hospital setting?", "35100d31-258b-4927-ae84-c2e1acd29f05": "What techniques are suggested to assess and manage statistical biases related to GAI content provenance?", "5688741c-26ca-4bc5-975e-44e6aea8cc04": "How should content provenance data be documented to ensure the privacy and security of human subjects?", "8e8033af-1acc-4cd6-890a-87f249ea26fd": "What factors should be considered when disclosing the use of GAI to end users?", "91993f90-2129-4899-9e6e-5629d9218594": "What methods can be employed to identify unforeseen failure modes in GAI systems?", "b0e16fa8-d18a-4960-b739-124315bc8b80": "What mechanisms should be in place to allow individuals to opt out of automated systems in favor of human alternatives?", "80c51600-c396-4fbb-a9af-397b1bf4df10": "How should the notice and instructions for opting out of automated systems be presented to those impacted?", "00e38210-0e59-468c-bed2-e1861fc8fa72": "What are the potential trade-offs involved in selecting a watermarking model that prioritizes robustness?", "f2499075-4456-4b99-843f-be61fa42fd87": "How can organizations enhance content provenance in GAI systems according to the provided context?", "568d5e9b-2a53-4d65-a2a4-c25447a264d4": "What role does the Technology Engagement Center play in relation to Uber Technologies and the University of Pittsburgh?", "d649feeb-3824-45fc-8760-22830fe7bd1e": "How many participants attended the listening sessions conducted by OSTP regarding the RFI?", "d1c718e8-56d1-46c2-858d-117077b73507": "What technologies were discussed by the panelists in relation to the criminal justice system?", "da8c32e1-3387-4723-84ae-7496f0798bfd": "What was the primary focus of the panelists' discussion regarding community safety?", "8302b95b-a3d7-4d82-a1f7-bfdc4a562ae4": "What types of innovative solutions are being provided by industry to mitigate risks associated with AI systems?", "91947ba0-a268-40c7-84d0-3d12a7ad3861": "How does the Office of Management and Budget (OMB) propose to enhance stakeholder engagement in program and service design?", "7ba61c64-6d80-4f16-986a-6d98866e17ec": "What factors influence the implementation of suggested actions for managing GAI risks according to the AI RMF and its Playbook?", "0ea09853-e524-4fbf-90c7-efc92f19cf2a": "Are all subcategories of the AI RMF included in the document discussing suggested actions for GAI risk management?", "54dcd851-b53a-4dec-9d4e-25959431298c": "What is the main focus of AB 701 as discussed in the Zaller Law Group California Employment Law Report?", "0932885d-9262-4c08-8138-90adae0cacf3": "How does the National Institute of Standards and Technology define the concept of explainability in AI research?", "7dced792-124c-4fc4-acb1-14acf6e0aba7": "What is the definition of \"confabulation\" in the context of GAI systems?", "4601d51c-2533-4e35-95c7-0e7ac23df4b0": "How do confabulations relate to the design and functioning of generative models?", "98574c22-9001-4181-b94f-0174572e69a0": "What processes are in place for tracking and responding to incidents and errors in the GAI system?", "73eb8344-5811-4b80-9bd3-2908daa20b14": "How are after-action assessments conducted to verify the effectiveness of incident response and recovery processes in GAI systems?", "d367fca5-e6f5-4172-997c-22933e02d5d6": "What are the necessary functions for which sensitive data can be used without consent?", "b018df58-62ee-4950-aedf-6f4face1a456": "What conditions must be met for consent to be considered appropriate when using sensitive data for non-necessary functions?", "eed35e80-0c64-4fe2-96d6-e62fbde504a7": "What issues does the principle of human alternatives seek to address in the context of unemployment benefits and fraud detection systems?", "ca2fbff7-bd7f-4eb6-89e2-509315f4f026": "How did the reliance on technology in the examples provided lead to negative outcomes for individuals seeking benefits or medical care?", "670d4ad9-791e-43d2-b50d-ce315ec93bff": "What types of systems are mentioned in the context that could lead to algorithmic discrimination in education?", "c6986ee6-197d-4d0d-b5c3-2273014ca4a8": "How might tenant screening algorithms impact equal opportunities in housing?", "8fdb5621-c22f-4ba9-8fe6-542f5140f8fc": "What are the key requirements for AI use by the federal government as outlined in the context?", "f4475605-1469-4f7a-86e5-5c6c4dfff062": "How does the law and policy landscape for motor vehicles relate to the regulation of AI technologies?", "58ef895e-9d3a-48b4-bc65-2d7ce3e624ad": "What is the title of the paper by Acemoglu published in 2024 regarding the macroeconomics of AI?", "9dc5d2ac-6501-4c9b-8fc7-8781ac526be4": "Which database is referenced for incidents related to deepfakes and child safety?", "3f342f9a-9dfa-4cfb-a805-f03ea30936d0": "What are the three categories of bias in AI identified in the context?", "8272595b-9c68-488d-9dc8-23317f95e0f0": "What are the three broad challenges for mitigating bias in AI mentioned in the context?", "433a16fd-c540-4978-a0f2-7d52f44d9a34": "What are the key components that should be included in contracts and service level agreements (SLAs) for GAI systems?", "4040256f-1654-4824-a930-9d34a3f7d9e8": "How can organizations measure the success of content provenance management efforts with third parties?", "8bb8bbab-3dc8-41a3-833c-85d16690c27f": "What are the main topics discussed in the paper by Boyarskaya et al. (2020) regarding AI system development and deployment?", "9e26333d-0470-42fe-b3d9-7748eb9e8d6b": "According to the article by Burgess (2024), what is identified as the biggest security flaw in generative AI?", "1a4406b0-3ea8-47ff-a6b9-cb3cb90f44fa": "What challenges arise when establishing a baseline scenario to assess harm caused by disparities in AI behaviors among different groups?", "a9e5b9fe-1741-41d2-b973-8ecbdb34b7fe": "How does the presence of a GAI model potentially worsen situations for certain subgroups compared to a scenario without any AI system?", "2bc3f62b-40bd-4601-8a92-a5f61c7fd080": "What surveillance technology is mentioned in the context as being used by Amazon and Walmart to monitor unions?", "262b7d02-b974-4b62-905f-1f49f44388e0": "Which federal acts are referenced in relation to privacy and data protection in the provided context?", "d5bfa5ef-3ed2-4ffc-ad48-5d322b8e5744": "What are the expectations for data and inferences in sensitive domains according to the provided context?", "5686b77d-627a-4a7a-a842-b53a5d23f801": "Why is human oversight important in the deployment of automated systems in sensitive domains?", "3a62d74d-a065-43f1-a498-9282e52023b7": "What is the main focus of the report titled \"The Public Health Crisis Hidden in Amazon Warehouses\" by Human Impact Partners and WWRC?", "588df61b-8d7b-47a0-abc2-20c19b242ec1": "How are surveillance programs affecting contract lawyers according to Drew Harwell's article in The Washington Post?", "3ed2350b-1301-4669-9615-7cd29418e762": "What groups of individuals are specifically mentioned as being adversely affected by persistent poverty or inequality?", "d9c7d599-b536-4465-9bf9-f57012d08166": "What types of rights and opportunities are encompassed within the framework described in the context?", "522f20a1-260e-46fa-9bea-6d9279815bef": "What are the requirements for consumers who are denied credit under the Fair Credit Reporting Act and the Equal Credit Opportunity Act?", "f9b0f71b-2368-4e94-8fde-02023651655f": "What must lenders do under the risk-based pricing rule when informing borrowers about their credit score?", "75d5081d-922c-4e2b-9eb9-bbbb8fe09786": "What are the potential consequences for individuals labeled as \"high risk\" by automated systems regarding their bail?", "d05613fc-32a9-4f0c-b8d8-505ae888caa9": "Why is clear and understandable notice important for the American public in relation to automated systems?", "c9ac2658-baaa-4935-80ec-1ed96d504d4e": "What policies and practices should be implemented to protect third-party intellectual property and training data?", "ab5dc45a-355c-4963-99c5-7e8a2c9032ee": "Why is it important to re-evaluate risks when adapting GAI models to new domains?", "4da428b4-93fd-4e59-9f96-0b5383fd29d2": "What disparities are observed in the risk assessment scores for Black students compared to their white peers regarding the likelihood of dropping out?", "d3a5fd8a-9875-488d-a700-c0379f7d0840": "How does the risk assessment tool for predicting recidivism show evidence of disparity among different racial groups?", "d50f8538-352a-4ffc-a882-dcb8959384d1": "What mechanisms are in place to deactivate AI systems that show inconsistent performance or outcomes?", "53fbb287-5e44-48b7-a706-283c47c73bfb": "How are communication plans established to inform AI stakeholders during the deactivation process of a GAI system?", "e50b831a-3a29-4d74-9492-ccf5331cb3cc": "What are the key aspects of information security for GAI models and systems?", "77cdcb7e-7ed1-4618-93ab-632568a6bd0c": "Why is it important to maintain the integrity and confidentiality of GAI code, training data, and model weights?", "7abbd449-5cde-4df7-adca-dac356f7cdf7": "What measures should be taken to ensure that surveillance technologies are subject to heightened oversight?", "3668dbf3-6749-4772-969d-a108c1f04903": "In what contexts should continuous surveillance and monitoring be avoided to protect rights and opportunities?", "f3b608e7-0779-4388-ad78-bacbecd4a672": "What constitutes algorithmic discrimination according to the provided context?", "5f7a632c-6bdc-4240-8f17-300f93998569": "What measures should designers, developers, and deployers of automated systems take to prevent algorithmic discrimination?", "1e006d60-237a-42f0-8076-f38eb25bf6d4": "What mechanisms are suggested for addressing the risks associated with a lack of explainability and transparency in GAI systems?", "69405fb0-0810-4ea4-9e75-29ef608f91b2": "How does the organizational policy GOVERN 4.1 aim to minimize potential negative impacts in the design and deployment of AI systems?", "d3431fc6-1a26-431c-9df6-045368fcc00e": "What are some vulnerabilities that GAI systems are susceptible to, as mentioned in the context?", "421fd769-39b2-45c3-9457-5e38ffd58634": "How might sophisticated threat actors utilize GAI-powered security co-pilots in cyber attacks?", "efe55b57-94c8-492b-a75a-a10d7d31b1c4": "What metrics are used to measure the implementation rate of recommendations from security checks and incidents in AI systems?", "2a046917-5086-4bdb-a5d7-b559ccc6fdea": "How does AI red-teaming assess resilience against various types of attacks, such as malicious code generation and prompt injection?", "07a64697-1323-4a24-a6d1-3eb4bb5075e1": "How can diverse backgrounds and lived experiences contribute to effective risk measurement and management functions in an enterprise?", "2447398e-12b0-48d1-93dd-80c1093f2175": "What steps should be taken to ensure that data used in risk measurement is representative of diverse user populations?", "7e1ef5db-ea16-4d72-8652-83e2d2c2796b": "How do new surveillance technologies in education disproportionately affect disabled individuals?", "e9c32f82-0ab4-437c-84bf-46179845b8f6": "What are the implications of surveillance technologies in policing for people with disabilities?", "eb6f2339-5835-448a-af94-f8236ba90e11": "What should automated systems undergo before deployment to ensure they are safe and effective?", "0570cbed-3520-4585-82eb-30a22a104d5d": "How should automated systems be designed in relation to the safety of individuals and communities?", "bcf24575-8d84-4edf-9cce-48ddc5ee0faf": "What are some methods mentioned for capturing user feedback regarding GAI systems and digital content transparency?", "d27f6e98-19f3-4073-9f10-9e1312f69406": "How can organizations track and document the provenance of datasets to address performance issues in GAI systems?", "818a4dd9-5054-4f4a-bebd-a7cfe737486f": "What is the main focus of the article by Darshali A. Vyas et al. regarding race correction in clinical algorithms?", "ee5e7804-d5dc-4f30-9d9c-943c4b2624b0": "Where can one find the definitions of 'equity' and 'underserved communities' as mentioned in the context?", "aa301ad2-a144-4e28-aba5-32a66523436b": "What measures should be included in the disparity assessment of automated systems to evaluate their effectiveness across different demographics?", "ec76ec19-a322-49b9-99a6-7d7e8957842a": "Why is it important to separate demographic data collected for disparity assessment from the data used for the automated system?", "ee38f986-4f8f-4997-94f4-e7121ff0936d": "What criteria must be met for the use of sensitive data to undergo ethical review and monitoring?", "46edbdac-ca2c-45f3-8983-2bb2703cdaec": "In what situations might an ethical review determine that sensitive data should not be used or shared, even with consent?", "d9fba9bf-ff32-40f9-be90-533fd00dee09": "How might the adoption of GAI systems impact the preservation of endangered languages?", "23403185-4e9b-4048-a7bd-42a5162f7eff": "What challenges do lower-resource languages face in the context of GAI system performance?", "afdad59a-8d9b-47ee-9f15-e4aa7ad24e8e": "What are the legal and regulatory requirements for reporting GAI incidents mentioned in the context?", "6f0b6103-d318-4d3e-aac7-99121dbf385d": "Which organizations' reporting requirements are referenced in relation to HIPAA and autonomous vehicle crashes?", "ced5cadc-1eff-4975-97a0-10782e9f4729": "What tools are suggested for analyzing content provenance and detecting data anomalies?", "ed295a73-f263-433e-957f-de3169eba5ae": "How can evaluation metrics be disaggregated to identify discrepancies in content provenance mechanisms across different populations?", "de8a52e4-3e82-4b72-a933-5c3bf00e665b": "What are the implications of facial recognition technology on civil rights, as highlighted by the wrongful arrests mentioned in the context?", "6c2f0573-89cd-41f6-8d3f-372db5e94d1e": "How do the works of Cathy O\u2019Neil and Ruha Benjamin contribute to the discussion of data discrimination and its impact on marginalized communities?", "19914be9-45bd-42a7-a1e3-14d827646633": "What are the potential risks associated with relying solely on automated systems in sensitive domains like criminal justice and healthcare?", "1c60f5c8-42e6-49e0-9bf4-a00e9e540fb8": "Why is it important for the American public to have access to a human fallback system in the event of automated system failures?", "7dca4d20-4ef0-49fe-9266-0ffc5ee147ea": "How do GAI systems contribute to the unintentional production of misinformation?", "e01c37e8-616f-410c-8f84-f47c80b79c3c": "In what ways can GAI systems facilitate the deliberate dissemination of disinformation by malicious actors?", "3f7bac5d-800c-42e2-8d5a-ba7971e8ab41": "What determines the applicability of suggested actions to relevant AI actors in the context of GAI systems?", "97fb7019-e43d-4a5e-be8c-5338a22f1e77": "How are the Action IDs structured in relation to the AI RMF functions and subcategories?", "f523c7db-d0dd-4254-becd-e0d446266102": "What are the potential risks associated with third-party GAI integrations mentioned in the context?", "9f7e08cf-1101-4a54-9126-c48e4a29dbc4": "How can organizations enhance their risk management processes when interacting with external GAI technologies or service providers?", "f07213a8-088b-4e64-bb5c-13087794bc0c": "What are the key considerations for organizations when applying governance principles to generative AI models?", "4d9cb278-709d-4d86-a9d3-0166e56bba4f": "How might organizations need to adjust their existing risk tiering to address the unique risks associated with generative AI?", "bdfa454e-8811-40e2-a1b8-8612efacf83e": "What types of algorithms are mentioned in relation to health insurance and risk assessments?", "1d0ea6bd-7eb9-490d-9d71-e216b294a2df": "How do financial system algorithms determine loan allocation and credit scoring?", "0946b374-67f1-4ad2-82b1-a1a600e2cfe3": "What are the key topics covered in the technical companion to the AI Bill of Rights?", "bcf0d8ab-d72a-41c9-89f7-c2c9f5201643": "How does the document address algorithmic discrimination protections?", "d9157e6f-9928-4a37-9a79-fd09d73844c8": "What were the four primary considerations relevant to Generative AI that the GAI PWG focused on?", "2aa7c8f2-95cc-4328-9096-654352b7dd2c": "How will future revisions of the profile address the evolving risks associated with Generative AI?", "437b6fa5-ead7-4392-9a1a-3967e3f6c814": "What measures should be taken to ensure that surveillance technologies do not infringe on privacy and civil liberties?", "6bb5e6e1-2c40-4ca5-84ce-0bb7aa44c640": "Why is it important to have pre-deployment assessments for surveillance technologies?", "49cf1b46-1b5f-4b12-b624-f6395f1bb465": "What issues are associated with the tool designed to help low-risk federal prisoners win early release, as mentioned in the NPR article by Carrie Johnson?", "5d1bee94-fe36-4cfe-b172-f56d60f2d973": "How is the Justice Department addressing racial bias in the decision-making process for prison releases, according to Carrie Johnson's report?", "54d3ebcc-992c-47ac-b26c-73b41f4b1066": "What are some implications of using third-party GAI models and systems for an organization?", "00f8d243-5dcc-4fb0-8274-69adb80f424e": "Why is there a need for clear guidelines regarding transparency and risk management in third-party GAI integrations?", "a24ae91e-1a03-4814-bc53-5c1d945dd7fe": "What organizations are involved in the Digital Welfare State & Human Rights Project at New York University School of Law?", "60bf9e79-1458-44f5-9f8b-7aefbcbfd556": "Who is associated with the Institute for Human-Centered Artificial Intelligence at Stanford University?", "713a991b-8a8f-4972-9275-6b7b4ed7983b": "What is the significance of the ACLU of New York's publication regarding the temporary ban on facial recognition in schools?", "5a21399e-3e68-4267-ab90-984b0dd68440": "What legislative changes were made to the Education Law in New York State on December 22, 2020?", "10df46e0-aa53-4cc8-ab1c-38bdeee23ed8": "What were the reasons individuals were denied benefits in the system mentioned?", "cc72cd6a-a029-4422-932b-ad3892f12949": "How did the lack of explanation regarding the system's criteria impact the correction of errors?", "a46f1765-4104-4060-bf4b-cc3a9a87c169": "What are some potential harms that can arise from the exposure of sensitive information, according to the context?", "d2f22508-8074-4588-a9b2-30e3b9c9bb1b": "How can predictive inferences made by GAI models based on PII lead to adverse decisions for individuals or groups?", "7a4943ad-0b2a-47ea-9345-d7117b715e89": "What topics were explored during the panel on Equal Opportunities and Civil Justice?", "efa24a20-746c-4452-a068-793a94874939": "Who served as the moderator for the event, and what is her role?", "76acfb70-0b8f-4d02-9d0c-ed71a08126f0": "What are the key elements that panelists believe are necessary for fostering responsible innovation in companies?", "854e290d-18b6-436a-9d00-1da4abb72bf2": "How does the event address the impact of technology on public safety and democratic values within the criminal justice system?", "3a30161e-eb65-4c76-8b82-9c96d692ac8c": "What are the ethical considerations that organizations must adhere to when conducting human subject experimentation?", "63edbe35-9e61-4194-b0ed-3ece00033578": "Why is it important for entities to conduct regular, independent audits of their data in sensitive domains?", "a2165b66-b277-403a-ac4a-df5f42045bfc": "What is the main focus of the article \"The Low Down on Ballot Curing\" by Rachel Orey and Owen Bacskai?", "979b9120-8c5b-47e0-a244-81f9e1822750": "How does the unemployment benefits system discussed by Andrew Kenney highlight issues related to the digital divide?", "fa6a6cbd-f7a5-4049-949a-444c36c9922f": "What measures should be taken to assess the risks associated with unreliable downstream decision-making in GAI systems?", "c1c2b6bf-ff54-431d-8c7d-abbfccb8c1e5": "How can GAI system architecture ensure recovery from security anomalies and threats?", "f715d51d-7856-4b22-bfcb-696c03bb19b6": "What measures should be taken to ensure that those conducting structured human feedback exercises are not involved in the development of the same GAI model?", "340a53a9-fc52-453c-b055-b820f2b2c01f": "How do harmful bias and homogenization relate to the context of human-AI configuration and data privacy?", "acc3f8fd-6a30-429a-be35-22ce9826f73a": "What initiative has the Department of Justice launched to combat discrimination in mortgage lending?", "0d1515be-0297-47d3-be57-23e493924ffe": "How are federal agencies collaborating to address issues related to redlining and mortgage lending discrimination?", "852a8805-c66e-40c4-86ee-4ce374f0c2a0": "What types of data are mentioned as being sensitive or privileged in the context?", "f9f20f2d-faaa-4a0f-989a-1bcd6bcf0bf1": "What are some of the key considerations related to AI Actor Tasks as outlined in the context?", "98f87900-cbae-49e5-8f1f-0639ce47f976": "What safety metrics are used to evaluate the AI system's reliability and robustness?", "5d4c5e99-bcef-4ba3-8bda-4caffa863a53": "What actions are suggested to assess adverse impacts on AI actors exposed to harmful content during GAI training?", "0c7fa675-b8ec-478e-95a7-500958f22157": "What are some purposes that feedback activities can serve in organizations?", "a239a6da-dc7d-427d-9ba5-5d4934386201": "What best practices should organizations follow when implementing feedback activities?", "42f8e9e1-f6ca-4695-a0a3-0e692840f3f4": "What are the main predictors of non-consensual dissemination of intimate images according to Karasavva et al. (2021)?", "59b9097c-5525-4b82-8443-eccefd9b1cf3": "How does Khan et al. (2024) describe the value chain analysis of generative AI?", "e85147b2-414a-4c8d-ac16-18762e3c46dd": "What are the key concerns and risks associated with the deployment of automated systems?", "f9ecb98e-a971-4c38-b8aa-e240e7db75e3": "How can ongoing monitoring help in the identification and mitigation of potential harms from automated systems?", "4e7e96e3-ce71-4afd-a7d5-58e5e44dc7ce": "What issues are raised by the use of facial recognition technology in relation to wrongful arrests?", "58a0a2ee-a693-4494-97a6-c6e7874c8c75": "How did Amazon's AI recruiting tool demonstrate bias against women, leading to its discontinuation?", "a2baf73b-8409-4f49-b089-0bc567906f60": "What are some practices mentioned that can help decrease risks associated with the misuse of GAI in human-AI teaming settings?", "c3415e0f-3913-4413-8ec5-86caf09b9b58": "Why is it important to establish acceptable use policies for GAI in different levels of human-AI configurations?", "4f3f76b3-1772-43ec-8f80-4c4300870e3a": "What are some key lessons learned from technological diffusion in urban planning that could inform the integration of AI technologies?", "63ac3a5e-ae8f-4b27-a77b-d7ed080454ff": "How do panelists suggest addressing the representation of marginalized voices in the development and integration of AI-enabled systems?", "3fd117d1-a7d5-41c6-bca9-7ca566527140": "What is the focus of the startups mentioned in Karen Hao's article regarding AI ethics?", "39f074ed-9605-4b37-a9e0-ecb02c51dbb8": "What type of report did the Office of Management and Budget release in August 2021?", "b6dfb9e9-a49f-4dc7-8f9e-2fd1e456d733": "What are some potential sources of harmful bias in Generative AI (GAI) models?", "8f636506-005c-48bc-b882-09d722d92484": "How can disparities in GAI model performance affect different subgroups or languages?", "6f99fd41-dbdd-4202-ade8-063b7f83c732": "What factors should be considered when identifying the intended purposes of an AI system according to MAP 1.1?", "cfef03c2-6d65-44e6-80dd-541bd25d5a95": "What are some potential impacts of AI system uses that need to be documented in the context of MAP 1.1?", "76527e98-f7dc-42c9-b453-0ba53d6dcde7": "What are some examples of how data privacy issues can arise in the context of insurance and personal data collection?", "0ae0b91d-4a1e-4931-a05f-9df21078b2ab": "How did the installation of a facial recognition system by a local public housing authority impact the privacy of individuals in the community?", "b869e4df-483b-45da-8b9f-f196ef8a8d92": "What methods are suggested for identifying the classes of individuals or groups impacted by GAI systems?", "8b6724c0-3fb9-45d2-915b-cb583267292f": "What factors should be reviewed and documented to measure sources of bias in GAI training and TEVV data?", "ac2bd177-fab6-4526-9ca2-888b015b9a96": "Why should validation testing not be assumed to transfer from one location or use case to another?", "bb6c1dec-4c07-4d2f-879b-b7ba98d40b02": "What role should human consideration play in high-risk decisions involving automated systems?", "9c2a53f0-f6f6-4127-87e2-280e804d5a6b": "What are the key expectations for automated systems to ensure they are safe and effective?", "fd2eac99-4b83-43e8-ac70-27b7d85f62d7": "Why is public consultation emphasized in the development phases of automated systems?", "491d53d0-760a-488f-a60b-906692d5f38d": "What are some of the specific contexts in which existing laws guide the collection and use of personal data?", "75edd594-e0c6-4888-a950-8dd547e3a3e5": "Why is there a need for additional protections regarding the use of automated systems and personal data in the United States?", "cdbf96b6-1326-437d-9088-a3c6ba667dc6": "What is automation bias and how does it relate to the perception of GAI content quality?", "ab1de749-7071-47a8-9ad0-b3212b6682f0": "What are the characteristics of trustworthy AI as mentioned in the context?", "ce20842a-aeee-40a4-9d50-ddcb7fdfbac6": "What are the expectations for automated systems intended to achieve in terms of technical standards and practices?", "e3f602e8-3fcb-4118-b6fc-8feb8db3b995": "How should automated systems be tested to ensure they are free from algorithmic discrimination?", "cb6c903e-8212-4dbc-83d3-ffa8a2b6739f": "What types of automated systems are mentioned as having the potential to impact civil rights, civil liberties, or privacy?", "74b84680-fc51-41d6-90e6-6e2d5bddcabd": "How does the Blueprint for an AI Bill of Rights define the scope of automated systems it should cover?", "146c98c4-6602-4fa8-afc5-23add5a87f0d": "What measures are suggested to assess the proportion of synthetic to non-synthetic training data in AI model development?", "861f50dc-637b-4ae1-a1f1-c4c3973c4518": "How should the environmental impacts of AI model training and management activities be documented according to the provided context?", "88557568-a41a-4e41-b469-bbffa4fe5934": "What measures are organizations taking to demonstrate compliance with applicable laws or regulations regarding privacy risks?", "0a9fd3bb-0c52-4952-a971-b4e645264e3c": "What prompted the state-wide biometrics moratorium in response to the school board's surveillance of public school students?", "f2677613-f012-4ac0-84dd-333e9ddc3b96": "What are the potential risks associated with the integration of upstream third-party components in the context of GAI and accountability?", "08f325f5-9e28-491b-9d48-7373910f5f37": "How might GAI impact the accessibility of CBRN weapons and related knowledge for malicious actors?", "d15f3e9e-f883-4359-a015-08d84a6a773e": "What are the potential privacy risks associated with AI systems as identified in the MAP function?", "1c734cd0-0122-4d00-8cbd-4720ebf30120": "How can AI red-teaming be utilized to assess issues related to harmful bias and information integrity in AI deployment?", "395c0c0c-9376-4571-a79c-fc962925da35": "How do government agencies enhance their surveillance capabilities through technology?", "60b9a068-dc49-4463-9b2e-95cd7cc7b9c8": "What challenges do members of the American public face regarding access to their personal data?", "dbdde0ad-14dc-4bc3-856e-54d857a3f162": "What concerns did the panelists raise about the impact of technology on individuals interacting with social welfare systems?", "3e6fc893-ad23-4a6d-b244-db5de7d794c4": "How might the use of technology in social welfare contribute to reinforcing inequality, according to the panelists?", "8bfad59a-0498-4be2-9379-68597b323036": "What steps should be taken to enhance transparency and accountability in the GAI system according to the context?", "2d2b2281-85e8-46a6-a161-9e6c89b1353e": "How can dataset modifications be tracked to ensure the provenance and verifiability of content origins?", "9b907102-3c1f-4a13-be25-070f61c28623": "What measures should entities take to ensure independent evaluation of their data policies?", "2ef2c2b6-1f34-46da-962c-c8da8a81ce45": "How should entities respond to public requests for information about the data collected or stored about individuals?", "3e20e422-f6c7-46b2-9850-75358283ce67": "What are some practical ways mentioned to reduce bias in evaluating chronic health conditions and future costs?", "b14c1eb9-4012-47eb-bf69-bf38b622ee2b": "What is the purpose of the Algorithmic Bias Safeguards for the Workforce initiative?", "b2beca45-c8ea-4d22-bbef-c9f4df425d57": "What are the primary goals of field testing in relation to AI-generated information?", "ae787237-1a2c-41a9-b414-5f9a5f6a8d81": "How does AI red-teaming contribute to identifying flaws in an AI system?", "2466912f-43b0-4cb5-88c2-e98299d71f0d": "What are the key metrics identified to reflect the effectiveness of security measures in content provenance?", "4549a5fd-9f33-41b8-b56b-95ba0d198bb8": "How do user perceptions of content authenticity influence their satisfaction with AI-generated content?", "8e905a67-89a2-42e7-866f-fad885936f40": "What is the purpose of regularly assessing and verifying security measures as stated in the context?", "22241859-2745-452a-adff-c048d64c0644": "How are risks associated with transparency and accountability examined and documented according to the provided measures?", "ae7e9c8f-c493-4244-8415-cdf5325d57a9": "What are the key barriers for malicious actors in the misuse of chemical or biological agents as discussed in the context?", "c415b33a-2ae1-40cc-a4c8-b362dba0cce7": "How might chemical and biological design tools (BDTs) enhance design capabilities in chemistry and biology compared to text-based LLMs?", "767f044c-12d7-4475-9893-bda3ea36063a": "What is the address of the NIST AI Innovation Lab?", "cfc1eed2-9dbe-4143-91a5-4bc80ebf170b": "Where can additional information about NIST AI publications be found?", "d904dc21-9a76-481a-8d4f-d3635fda72d4": "What contributions did the NIST Generative AI Public Working Group make to the report on trustworthy development and use of AI?", "21b9b95b-0b01-436f-83c4-ad58be7f0e18": "How can one contact the NIST AI Innovation Lab for inquiries related to AI?", "14246adb-f011-4bf4-a251-e0c3fed1495f": "What are some potential harmful outputs generated by text-to-image models in the context of unethical behavior?", "cd0d0a8d-6739-432c-81a5-5d754d2606a8": "How does the practice of \"jailbreaking\" relate to the limitations of GAI systems in producing harmful content?", "7b0e30a4-cc1e-4028-a24c-e75fe0abaf28": "What is the main theme of Shoshana Zuboff's book \"The Age of Surveillance Capitalism\"?", "71269d6c-2732-437a-8d73-bffc1da514ea": "How might an individual's online presence impact their life insurance options according to Angela Chen's article?", "3693d18e-c5f3-4974-878e-2cb9f00aac5a": "What concerns were raised by panelists regarding the delivery of healthcare through technology?", "f8abd077-a7a4-40c0-a1d1-c57574a9b763": "How do racial biases in medicine impact the use of technology in healthcare, according to the panelists?", "4bfef4c4-3cb9-49b1-9387-a7288e4f872d": "What are the potential consequences of using unobservable targets in automated systems?", "5e05291f-62d0-498c-a514-775e4ddaf733": "Why is ongoing monitoring and mitigation important for automated systems after deployment?", "d69f8a61-23f8-4c8c-ba63-7afecca60a33": "What measures should organizations take to ensure that a proxy feature is not given undue weight in their systems?", "b3fcec27-4737-4546-966a-bf6a6468552d": "How can organizations monitor their systems to prevent algorithmic discrimination related to proxy features?", "7730126b-dce7-4494-a8b0-a506864e96d4": "What was the purpose of the meetings conducted by OSTP with stakeholders in the private sector and civil society?", "1ba114be-8de0-4a1d-8553-c73024e14846": "Which organizations participated in the discussions regarding the Blueprint for an AI Bill of Rights?", "11ed4743-1d71-484e-8ca7-21f8e2d845df": "What are some of the problems that the predictive policing system aims to address regarding gun violence?", "e6bb11f1-4c37-4165-8a9d-d7d3e7d335ec": "How did the lack of transparency in the benefits awarding system impact individuals seeking assistance?", "bc0328a0-4d98-4afb-acc8-834c67ee0cb0": "What factors may organizations consider when tailoring their measurement of GAI risks?", "6fc2afea-e3e3-4e4e-a0ef-84ec0bfd5575": "Why is it challenging to estimate certain GAI risks?", "61426d26-118f-40d6-8061-f042005b7c1f": "What is the purpose of the document titled \"A Technical Companion to the Blueprint for an AI Bill of Rights\"?", "47b97e20-407a-4de0-9383-63014affaf56": "How does the document transition from principles to practice regarding AI rights?", "b8476e01-8814-420b-bf20-97b81161f224": "What are some domains considered sensitive in the context of the provided framework?", "e6bc7cd7-18df-4504-b84f-62023be0aa56": "How does the definition of \"surveillance technology\" relate to the monitoring of individuals or groups?", "e1b98422-109e-4282-9b66-ef7aa422ad48": "What role does provenance data tracking play in distinguishing between human-generated and AI-generated content?", "d87843f9-df40-4159-80ac-7c81f34e647a": "How can digital transparency mechanisms improve public trust in AI systems?", "9c5fd71e-253a-4cc0-842d-de8d1593a36e": "What is the purpose of ISO/IEC Guide 71:2014 as mentioned in the context?", "a7133dfb-1495-41d6-8c1a-c7af69b1469a": "When was the Web Content Accessibility Guidelines (WCAG) 2.0 published?", "d0f002aa-8482-4046-8ab9-a62ea7bbc170": "What are the potential risks associated with the misuse of automated systems as identified in the consultation process?", "fcda0206-7bfa-4b9c-8e9a-c37acab951e8": "How should the impact of high-risk scenarios be assessed and mitigated in the development of automated systems?", "e62cc9d7-c4e5-4137-8465-417609eb14fd": "What is the purpose of the reporting expectations mentioned in the context?", "14bbbd89-3c39-482d-bc68-b324f2822f96": "How can the guiding principles of the AI Bill of Rights be practically implemented according to the context?", "a1d62dde-5af6-42aa-ad8f-d4e9f9c53484": "What is the title of the NIST publication related to Artificial Intelligence Risk Management?", "a6af5723-dbf7-4ed6-8bad-1f15c24ffb13": "Where can the NIST AI 600-1 publication be accessed for free?"}, "relevant_contexts": {"9359ec2a-755e-4f38-ab9b-a959faade87c": ["5b8af9b5-6401-47b2-a0b0-cab411946bc0"], "7951764e-0adf-4275-96ec-66506ab80ada": ["5b8af9b5-6401-47b2-a0b0-cab411946bc0"], "e05e9fc0-5672-4ed8-b2a3-eb5820dede84": ["eee77049-8a28-4d67-9e7b-9a0645afd06f"], "53085e80-ee55-412f-ad31-87b71b19b291": ["eee77049-8a28-4d67-9e7b-9a0645afd06f"], "4bb1fe70-04c4-4914-80d2-bbc3262c2ea9": ["67abcb37-f776-4732-8a16-931c57acf4c9"], "30af0e74-6e5b-4ba3-b475-c15a6c0ce965": ["67abcb37-f776-4732-8a16-931c57acf4c9"], "cf83ff2d-a13d-4dd4-acef-354940f0c5d5": ["452b57dc-fb7c-4a90-bf60-179fd88a45e7"], "7f7cbad6-8db9-4604-b47e-9610d018534c": ["452b57dc-fb7c-4a90-bf60-179fd88a45e7"], "6cc44cb7-c062-4eb3-9909-7f9b7bd2ff5b": ["b1fd4721-6069-47b9-b282-954dc06a4d12"], "77aee64b-31c2-4f93-9d1c-2a0028a803ba": ["b1fd4721-6069-47b9-b282-954dc06a4d12"], "a6bfcc82-f6b5-4353-ace0-db9f6639ad13": ["1e0b6329-5019-4458-b61d-e70808fee6a7"], "3a279837-3633-45f1-bfe4-4543ff5fc090": ["1e0b6329-5019-4458-b61d-e70808fee6a7"], "67c59510-aa48-42b7-9a40-a17d83383699": ["e6699665-8b3c-4923-9ff5-b2bda5d2fa93"], "f4663d20-a072-45a3-ab51-0a9ed245c4d4": ["e6699665-8b3c-4923-9ff5-b2bda5d2fa93"], "5f8685ee-2106-4af5-ab7a-82031c9f457e": ["cb8782b0-a673-48a5-9c4e-71fb779cdee0"], "6be2b5d4-6566-4e26-afa3-bb5f15e800d6": ["cb8782b0-a673-48a5-9c4e-71fb779cdee0"], "e83ea684-71b2-44e1-be69-b11d2956d159": ["13591cf1-6123-49bc-b358-d1fccdce870d"], "21eec31c-f0e1-47cf-865b-0a2eb2f04d57": ["13591cf1-6123-49bc-b358-d1fccdce870d"], "de1912dc-75a2-4145-9340-7de4d68eb0f0": ["ef90770a-0a05-4e41-a4e1-8e7ce75a2d5d"], "4c0e6f0b-1312-41e4-ba12-883825b96e33": ["ef90770a-0a05-4e41-a4e1-8e7ce75a2d5d"], "8c73a115-dd73-47b5-9c01-68b0b99cd25a": ["c558c0ca-d2f5-4315-b6cb-4af590181d1a"], "7e4b1def-af67-4ecc-be8e-1d895e9226ee": ["c558c0ca-d2f5-4315-b6cb-4af590181d1a"], "aa0d05d8-639d-4438-b7b2-95dd14b860b5": ["3e8a526b-75ee-49f4-b078-280239ba1a8c"], "05051823-b0c9-404b-a96c-613a8826857a": ["3e8a526b-75ee-49f4-b078-280239ba1a8c"], "37356948-60d9-4493-b459-590b701db73f": ["bab4e82a-ed21-461b-881a-88c3be7ab6a4"], "48cfbe42-de1d-4557-82f8-838f40e26612": ["bab4e82a-ed21-461b-881a-88c3be7ab6a4"], "d0b46f77-ef1f-46fd-be09-9f2aecd247db": ["d3e800c3-7689-4e70-be5f-b31ae9e74668"], "9834ff3c-dbab-4399-96c4-592d6dc34021": ["d3e800c3-7689-4e70-be5f-b31ae9e74668"], "8bbbb635-9ae8-4396-83c5-a84e1b09c39f": ["64710ba6-2577-435e-831f-dc8c1124943e"], "14d358f8-d913-4b04-9a50-f40d9f739ad8": ["64710ba6-2577-435e-831f-dc8c1124943e"], "d36bca4b-2e3b-4d1f-a70e-37f6eda7349e": ["d1f538c7-f835-43b4-aeef-5d3e578f7ad2"], "20cec3ab-1c2b-4ecc-826f-420577c0100e": ["d1f538c7-f835-43b4-aeef-5d3e578f7ad2"], "aa660712-ab3d-4265-8080-b6598fff34ae": ["ae2c2380-14b2-415a-8635-7a7e7a8b1a95"], "bab92b97-da80-4aa1-af12-8eff44756bd7": ["ae2c2380-14b2-415a-8635-7a7e7a8b1a95"], "3b619b6f-78d7-4911-995e-09f9f01dc0b1": ["e21b0e3e-df6d-43bf-ae63-76120d3d8154"], "a4167b3e-2e87-4138-9ee3-aae00129f33f": ["e21b0e3e-df6d-43bf-ae63-76120d3d8154"], "c6eacea8-70df-43a7-9634-d45da6e24d8d": ["d16f1fbf-2d7f-4bc5-a574-381e441ee615"], "ad32eb95-d42d-402b-8d5d-bddbd49eaf96": ["d16f1fbf-2d7f-4bc5-a574-381e441ee615"], "ecdc997f-fcaf-4754-90f9-e7218f607a82": ["18893892-bf57-422b-84ef-785183d35218"], "bcb0b164-73c1-456c-a4c9-3ebb08b1d0ab": ["18893892-bf57-422b-84ef-785183d35218"], "b008ea2e-0fa6-4704-bcfe-63926e370b62": ["a9e8e9ec-ffee-4d4c-bf18-60e69fc3f6ef"], "b82511ce-0113-4ad5-b176-702cbd10c7e9": ["a9e8e9ec-ffee-4d4c-bf18-60e69fc3f6ef"], "931c42a0-82d3-40db-940f-d0fb3548d0e8": ["1578fb61-ab39-407a-9a7f-775e21a729e4"], "005c0ac3-f057-41f1-ab95-3309b1c37cf1": ["1578fb61-ab39-407a-9a7f-775e21a729e4"], "018689b1-aca8-4d06-ba46-5d5f5f9f266c": ["d5a52529-2620-451d-8910-3ec4463f63a0"], "fe0ae106-1da5-4a0a-8895-8c43da752054": ["d5a52529-2620-451d-8910-3ec4463f63a0"], "57a1ea6b-52e3-48fb-83d0-3b3dbbdd80ee": ["89d4b864-3fae-4dd1-b64b-42dea70146e1"], "2719ce52-1169-4489-ad53-008763fe4588": ["89d4b864-3fae-4dd1-b64b-42dea70146e1"], "24f5399a-0435-40e6-a88f-f3ff09352629": ["cbda89d4-e094-472c-9fb0-2e2ccd29052d"], "529587e1-9907-4f83-a6aa-7ab7b86ef74d": ["cbda89d4-e094-472c-9fb0-2e2ccd29052d"], "04aaf309-dcdd-462a-9566-025e5cd73079": ["8b40e1a8-eceb-4ea8-bad3-8f1eb8937a5c"], "e20a3f65-c076-42b7-a2ae-0689d2f336bf": ["8b40e1a8-eceb-4ea8-bad3-8f1eb8937a5c"], "939f291a-ab95-40b4-a0ce-19e5a29d9d98": ["46087c7d-6427-4c9b-967b-665e401ccb03"], "055bf158-d20d-4ff1-81bf-84c82b0cf748": ["46087c7d-6427-4c9b-967b-665e401ccb03"], "fb92684b-aea6-4dd5-9ab4-7b3fae408439": ["b108da9a-d2dc-46be-9672-9c71f39f1c68"], "26844282-e776-44b5-a21e-e3e71a5f599c": ["b108da9a-d2dc-46be-9672-9c71f39f1c68"], "c0d7b5d4-3668-4077-97c0-26a9be864434": ["b7cd6e58-9b28-479c-ae84-85fbf220a0f4"], "4d44a2ef-705b-431c-bba6-6eff589eb19b": ["b7cd6e58-9b28-479c-ae84-85fbf220a0f4"], "59651941-5e49-41d8-bc3d-9c5636f97522": ["6d5a2af7-141f-481f-9bd6-be9ba1c32357"], "bb7a66b6-4d1d-417b-b7ca-a931d048c453": ["6d5a2af7-141f-481f-9bd6-be9ba1c32357"], "b4a84cb9-3cfe-4b1c-9f6b-2a75c372584e": ["779898f7-f5b2-47cd-9a8e-7ab1617f17e6"], "8495c971-2834-4aa4-b622-e2416649d841": ["779898f7-f5b2-47cd-9a8e-7ab1617f17e6"], "68f00089-09f3-426d-8645-396bf9a16bf3": ["8b6e2467-5eb6-472c-a5c3-fccfc0a11841"], "56cd8376-c476-43e6-ad2e-20ba1b8426f5": ["8b6e2467-5eb6-472c-a5c3-fccfc0a11841"], "03b543bb-1ef8-4064-b5cc-4293df0ba9e8": ["471f0922-9410-4ce7-a715-27c8569b4102"], "be72edea-ff94-4257-8723-12f6a992e902": ["471f0922-9410-4ce7-a715-27c8569b4102"], "8f9fcbd0-7981-4444-8641-710f5d8a582e": ["9f8e960c-28d4-467b-af64-2d2c12ba2650"], "3af76725-6591-4dc5-8e40-a2e7707d4767": ["9f8e960c-28d4-467b-af64-2d2c12ba2650"], "d556e7c5-a187-4f35-b570-1e64a065f204": ["90fad26b-8ade-46d5-a1b6-8b82befa2d31"], "884e3c8b-c0e7-4e16-ba22-abfe38f1a3e9": ["90fad26b-8ade-46d5-a1b6-8b82befa2d31"], "0cf8120a-66d8-4673-9b9b-5eff48a1ab54": ["e2f125a7-80b6-4c8e-8e8c-c7c947a9f26d"], "fd891bc7-9f6b-481b-8bc2-c823d3b7d811": ["e2f125a7-80b6-4c8e-8e8c-c7c947a9f26d"], "583ed22f-9d24-4cf3-89d4-d8e63bcae3da": ["dba4ff20-578d-471c-b434-2cb8a41fe69c"], "982dbab6-0bad-4036-8fd8-a876896630a0": ["dba4ff20-578d-471c-b434-2cb8a41fe69c"], "11b668f1-c709-4076-ac60-01cd89e83e56": ["d08159f3-e8d6-43c1-8608-cb7b2938d0ae"], "b2b6189f-a512-439d-896b-d714c85405d1": ["d08159f3-e8d6-43c1-8608-cb7b2938d0ae"], "a28131e5-5ca5-4c33-b431-8a5e293605a9": ["09223ac2-b2c7-484e-8f92-028079d7dc0c"], "5bf61bad-d4c8-4f1b-b050-10b9eb8067f1": ["09223ac2-b2c7-484e-8f92-028079d7dc0c"], "a53eaae0-5966-4fb5-973e-f52aed4504e0": ["2979d054-7a34-4262-9fdd-c4ab4f3542ec"], "d14205f7-d215-4670-8056-5cc6fda7d594": ["2979d054-7a34-4262-9fdd-c4ab4f3542ec"], "fc133689-db4f-45ba-9583-137d2c106f4e": ["2bd97c55-3050-47fd-b13e-a6211d21dfeb"], "2f81df52-3a00-43d2-903c-71a2a59e158b": ["2bd97c55-3050-47fd-b13e-a6211d21dfeb"], "13f01366-a968-4eee-8550-1d1823c737ea": ["d4d4a8c1-5fa2-458e-bd39-21f04c221cab"], "d149ebac-bb46-48a4-aa2b-f9cb607f7e1f": ["d4d4a8c1-5fa2-458e-bd39-21f04c221cab"], "4a2da819-5658-4163-8119-3376a4630aa3": ["8bc1eeee-7f57-4ef5-a85f-d48dfa4d9817"], "51b62ad3-7982-4bf7-b987-d82a4f642e6f": ["8bc1eeee-7f57-4ef5-a85f-d48dfa4d9817"], "5e49492b-34e0-47fa-8b06-f867e531ba1e": ["eb059f63-f688-4142-b0f5-d1e962d6ce03"], "7a63a1f9-9f76-49fc-9aed-b57b23daa7bb": ["eb059f63-f688-4142-b0f5-d1e962d6ce03"], "641f7cd1-e718-4a9d-9c76-14f2396e2cec": ["b1d30e39-7d73-45dd-bc8d-fbd7392539cf"], "b5aad58a-6580-4ee1-a776-e9b77316afdd": ["b1d30e39-7d73-45dd-bc8d-fbd7392539cf"], "0ef9b874-ecb2-4ef0-8185-a799a89df06b": ["774f3535-e907-42f1-87fa-bb14b355780d"], "2829aa3a-2e25-45f1-93f7-a6f2e4eab423": ["774f3535-e907-42f1-87fa-bb14b355780d"], "47bae431-a432-4721-805d-d99841bc074b": ["c575e182-a0e8-455e-b0f9-33b348927d2a"], "e465f445-bfa3-4e73-af48-6db571aa7455": ["c575e182-a0e8-455e-b0f9-33b348927d2a"], "7ae3fa17-3050-4aaa-91dd-5976bba1d7d2": ["1c7305a9-3c6d-4165-a965-cc499d1b0ce8"], "a379ce8c-b367-43ef-868c-ca8b373932b3": ["1c7305a9-3c6d-4165-a965-cc499d1b0ce8"], "e4fcd729-4f79-4d1c-b2e6-d2ee57b90127": ["726a595b-bb8d-4e05-9019-19a146ca2e20"], "93fa3ce9-5961-4363-98d7-a73f82ab9c5f": ["726a595b-bb8d-4e05-9019-19a146ca2e20"], "8733fab2-2cb6-4c79-b549-9f87f590f6f3": ["d8541ea5-d276-4f04-b5da-60c79f4c4808"], "e63d3e63-4040-4324-8b5f-9db902f554b1": ["d8541ea5-d276-4f04-b5da-60c79f4c4808"], "649b6fba-6665-465f-b616-e10b0aa87a9a": ["53662a07-d6f4-4e76-a2f0-1aef0e349d1f"], "77b1b087-d26e-42a6-86e2-e73ee64d2264": ["53662a07-d6f4-4e76-a2f0-1aef0e349d1f"], "99a25e0f-f613-4906-bfa3-1f59f7578b41": ["c736820c-1ace-4809-b9e1-e7409aedbabb"], "5af0b0d7-19a9-477a-8c71-210158174b72": ["c736820c-1ace-4809-b9e1-e7409aedbabb"], "18280e9c-b6b9-43e1-a72c-2c8c777e2ecc": ["ca71303d-50e5-4932-ad22-cc28fd1b5973"], "a0417d68-27e1-4f69-b8e6-5c4d4d3bce0b": ["ca71303d-50e5-4932-ad22-cc28fd1b5973"], "f847bd16-3186-4fe0-b0da-8fcb4328190d": ["5cda24b9-1fe8-4f6f-9063-bce72af9de38"], "3590d8be-1272-435c-aa15-82ef17fbc630": ["5cda24b9-1fe8-4f6f-9063-bce72af9de38"], "7bcf1232-6a4b-4e5d-a59f-1dd83a8f27f1": ["a5855cb3-4c78-4d83-9d0b-ebc776ccc509"], "cfaed751-ceb1-4391-bf53-f69d7c995acc": ["a5855cb3-4c78-4d83-9d0b-ebc776ccc509"], "da0ce0db-5de0-46ea-9590-960fc8f3255f": ["0b2ceca5-9195-4710-8263-5243535f0ded"], "e428c5fb-d0f5-451f-8853-6196d4f0efd4": ["0b2ceca5-9195-4710-8263-5243535f0ded"], "1882074f-f8fa-4b42-ae0d-623f85b10328": ["d0dc8661-9290-4fe8-a6ee-50705c7ef84e"], "35a2027e-8c4e-420b-a6cd-b5c8981aa1ec": ["d0dc8661-9290-4fe8-a6ee-50705c7ef84e"], "5f68ea59-6563-4780-8657-6cac24c7258c": ["394f4c55-99c6-4780-adc2-a21afac9eb3c"], "7c6e11b5-7a96-43da-8dd0-413ed3195ee6": ["394f4c55-99c6-4780-adc2-a21afac9eb3c"], "6a4861ae-cc49-4bb2-93e4-a26056835633": ["df4b1933-e4e5-4d75-8122-4847e9d96181"], "008b3082-a083-4c80-87a0-e2d326ea1b90": ["df4b1933-e4e5-4d75-8122-4847e9d96181"], "7b336f26-dda0-40ef-9d8f-6add6391f80c": ["d6be1dfa-ecf2-4df2-ae32-e2fe00fcf280"], "91c6a460-c1f5-46da-8f90-b96979abc2b3": ["d6be1dfa-ecf2-4df2-ae32-e2fe00fcf280"], "ec1dd89b-0430-4db3-b8ea-5773a62a467b": ["952a7c87-ad77-48d4-9558-483e11461872"], "eb143311-e998-4f66-8f05-3f71b40d30fd": ["952a7c87-ad77-48d4-9558-483e11461872"], "234bfb1e-dec6-4b4e-9591-aaaa6adf8960": ["79791bf5-36a1-4b53-808f-06e613bde016"], "32cc9210-3b28-4ff0-a863-a2064a2030d2": ["79791bf5-36a1-4b53-808f-06e613bde016"], "e53f71d6-2f8e-49d3-bf6d-6f5752b82330": ["44ed0314-aa7d-41b7-b53b-f3e89b6c0241"], "8ac67b6a-ad42-43f0-89c7-2bb5374b1314": ["44ed0314-aa7d-41b7-b53b-f3e89b6c0241"], "5b2a4346-1ea7-4042-b11c-79e76d515db5": ["a3b2244d-4b36-48b9-b8a0-ca009eb2368a"], "30cc89d4-6920-44bd-8bd3-1e7d56108a3f": ["a3b2244d-4b36-48b9-b8a0-ca009eb2368a"], "a39f37c1-2f9c-4f7f-b589-5dcaeeae9406": ["0d84d5ec-ddd4-49c7-ad85-abedec52004f"], "799f814c-2bb7-4504-bf31-81da7898dff5": ["0d84d5ec-ddd4-49c7-ad85-abedec52004f"], "fe1c6cf7-70ba-49f5-ba55-3395951dccab": ["4c44df41-d3fa-4ebb-bd71-56049499a58f"], "77298fee-c5dd-4c64-b7db-5ad2641d983a": ["4c44df41-d3fa-4ebb-bd71-56049499a58f"], "7e8df5fa-97a9-417a-a530-4ff97c938f2e": ["a6211724-c72f-45a8-ac49-c48909e4b03f"], "26ed11e7-65d2-4671-8875-cbbd905966d5": ["a6211724-c72f-45a8-ac49-c48909e4b03f"], "ced547f5-5dd0-4dff-9055-8a7c953aa687": ["dba825c7-91df-4307-b440-8883ba97780e"], "238caa23-8d02-4cb5-8eb0-cc21d074dda5": ["dba825c7-91df-4307-b440-8883ba97780e"], "707e05bd-211f-495b-a35b-d0aa5b59bbb3": ["1f74cab4-d7bb-4a0a-a4b6-fda048160fbf"], "fb0f2683-c2aa-43a4-97fb-1b92bf4f9ae4": ["1f74cab4-d7bb-4a0a-a4b6-fda048160fbf"], "c6bf4149-3386-4219-8a8a-8c86e7d4b567": ["3d8ffd3f-8a4b-4519-be1a-1fb41c2516ef"], "117b8b57-3c7b-4948-9e7c-7f9ef12cb676": ["3d8ffd3f-8a4b-4519-be1a-1fb41c2516ef"], "46096ec7-2d54-45a3-8f22-4e978d0c557c": ["874d4eea-f631-4029-88c9-3bb7ec5b6799"], "8c617963-336f-46d9-bc51-90663a4e118c": ["874d4eea-f631-4029-88c9-3bb7ec5b6799"], "a286d90b-8777-4add-b96c-bd9459e86ac2": ["e403e66e-4463-4feb-b0f8-549065d3d0cd"], "2bbe3b74-88c0-4890-ae60-839a9ec6c50b": ["e403e66e-4463-4feb-b0f8-549065d3d0cd"], "0918e42e-634f-4863-9b7c-baa048a809f2": ["dee516cd-069d-42a7-a99a-48eaf99ecea1"], "475b8773-8cce-4d3e-abf6-91b38d5aa9f4": ["dee516cd-069d-42a7-a99a-48eaf99ecea1"], "ecc74d6c-998a-4034-8a19-612e477bdb8d": ["9532c8de-647b-4a7c-9bd1-ba0706027cc3"], "3bbf1ab7-380c-4c5a-b37f-e4f9d04f9e1f": ["9532c8de-647b-4a7c-9bd1-ba0706027cc3"], "8f2966b5-3c77-4464-98c2-cac2e663fefe": ["91c195f7-76df-4a41-821a-706980e7303d"], "bff58f34-4182-4a36-a611-2850128479fe": ["91c195f7-76df-4a41-821a-706980e7303d"], "d9284a2f-ec74-4904-9e0e-f9372fe19e54": ["2bac1e80-9d70-448a-91ad-5f5db978bfb0"], "01828659-89dd-4d87-af34-9f6ef54c8d69": ["2bac1e80-9d70-448a-91ad-5f5db978bfb0"], "dcedfb96-97f5-414f-8966-a192a5e8cf20": ["bf8c62ba-0a99-4b4e-ba76-134ed064e07c"], "84ef7097-cd99-4fa9-9d5d-b94f706fbec5": ["bf8c62ba-0a99-4b4e-ba76-134ed064e07c"], "dfdb907a-1d3f-483b-a3fb-40d375f9ae67": ["dcaa8692-bdab-46da-a3d0-6c80458d59dc"], "f9b97959-12b9-4f34-a80a-a4b15dbd8b4f": ["dcaa8692-bdab-46da-a3d0-6c80458d59dc"], "88972501-7b50-4d8f-bb1c-5f6465dd3588": ["27b0d1c5-ab5c-4da1-975d-8c653dc7ea35"], "2c30006f-83fd-42ba-be5d-2b158ea469d2": ["27b0d1c5-ab5c-4da1-975d-8c653dc7ea35"], "99065c49-5993-4a5e-859e-38fe3549d822": ["f45ef5ad-ea8b-4d8f-95bc-0124e9ac4bea"], "4b913cd1-f8c2-4d2b-bc43-4670d8ffc69d": ["f45ef5ad-ea8b-4d8f-95bc-0124e9ac4bea"], "bb2ab5d2-5d33-46bf-bf8d-adc62375d2e2": ["7d753662-c05f-4b88-be8d-15be78e51a5b"], "75af3581-8e4b-4a5e-a0a5-b487c8be009b": ["7d753662-c05f-4b88-be8d-15be78e51a5b"], "5509b57b-6088-48e4-85ec-d948b231ad5c": ["71111a8b-c09c-46c9-ba56-2dd62da7e064"], "34029bf8-f63d-4308-a8bc-2e0c68139750": ["71111a8b-c09c-46c9-ba56-2dd62da7e064"], "9ca51517-5f29-4fa0-aad5-1ddc9d958b1e": ["541362a2-abca-4663-a0d6-d0e94615eb8b"], "a8429f34-ca7e-493f-aa4e-f422232da1ce": ["541362a2-abca-4663-a0d6-d0e94615eb8b"], "f7dcacbd-9ed9-4887-b50f-c6d0cd992107": ["1cd98259-2c46-4bd1-a687-3e5a3960ea6e"], "f3e38776-3894-446e-b04e-44d0351bcd2d": ["1cd98259-2c46-4bd1-a687-3e5a3960ea6e"], "c5e8a4f8-ed05-473f-ac39-d9063b675bf8": ["1e4afe32-1616-471f-b01c-9243522ed764"], "c09fba32-4fd4-4db0-b28b-cd6d4668ba86": ["1e4afe32-1616-471f-b01c-9243522ed764"], "95ce6f6a-7c3a-41f4-94a3-ba8cdb5c1637": ["8505543c-6b05-4924-a1a0-64bdad635872"], "0c5e9a81-12af-4d80-9d33-5ba7f3af0f49": ["8505543c-6b05-4924-a1a0-64bdad635872"], "784931e2-64cf-43b8-8686-c4d0290268ad": ["7936f820-c3d6-4b50-b2da-c84f9a3dcccb"], "0b95f225-39ed-4da4-9334-f94e3e84f43e": ["7936f820-c3d6-4b50-b2da-c84f9a3dcccb"], "24124845-94a1-4719-9c89-65afbe4b5606": ["184ca7cb-86a8-446c-a33e-6102100fea68"], "c000979d-175a-44f8-963c-73766bd0b5c0": ["184ca7cb-86a8-446c-a33e-6102100fea68"], "c9d8b913-470e-43a1-8492-3e0be609ef01": ["e0ffe85d-16ed-4878-b4e3-295aa31e0b27"], "d08e03b2-19ea-4bbc-b0c5-499d79447d07": ["e0ffe85d-16ed-4878-b4e3-295aa31e0b27"], "64a2d688-a503-478b-9f9e-c29e4804ce31": ["13eeef46-3519-4029-93e4-0a6d9c879c1e"], "5ca7dbe2-e28e-458d-a95c-1deb0595e2cd": ["13eeef46-3519-4029-93e4-0a6d9c879c1e"], "0e41f2c3-6d31-41ef-ab65-f9119582cfdb": ["4deb37b2-62d4-466b-8d54-8db1960841b6"], "3a47e1f6-c619-41a3-b589-24cc61613716": ["4deb37b2-62d4-466b-8d54-8db1960841b6"], "0e4753cb-8424-477b-9ce5-babba7652abc": ["05d801bc-8707-4f49-9b8b-bbd98c183db6"], "85f5d0a6-14f4-4472-8fa6-bbe7b8185f76": ["05d801bc-8707-4f49-9b8b-bbd98c183db6"], "71cdaad0-bf94-4c3a-b1e0-fcba011f6c44": ["1a5a5c9f-bf1c-4b49-a652-a626ecd1fc72"], "5afb1559-6342-4bc2-9407-c6294526fbcd": ["1a5a5c9f-bf1c-4b49-a652-a626ecd1fc72"], "37dfda02-c719-4590-924a-edc49d84f5fa": ["3bf01afa-de74-4da2-b9e8-e0fc2c60f433"], "437dfbc6-f3f7-4e74-bfa0-d33e98e78d90": ["3bf01afa-de74-4da2-b9e8-e0fc2c60f433"], "3970a859-0021-420d-bc71-20ce93ace873": ["f508a1a9-5937-4fd1-9f4e-fd76e7060e7f"], "b2bf361a-9861-4116-b32b-48a52c3c39cc": ["f508a1a9-5937-4fd1-9f4e-fd76e7060e7f"], "f98c5248-24d4-4997-9b8f-fdc917f811d2": ["a25825a2-9545-4754-8b47-d2c835ae0277"], "5cabd937-7d69-4608-a461-772664ca8684": ["a25825a2-9545-4754-8b47-d2c835ae0277"], "4e1953d7-6982-4c53-8c2a-7b03daf7d7b5": ["159da431-8282-4348-b6ab-0ffbaab8d0e3"], "fa705d1a-2487-4214-b296-0d7e42b0eed1": ["159da431-8282-4348-b6ab-0ffbaab8d0e3"], "9b67dc74-75c1-4ea0-af82-3b4475fd02ef": ["9eb7bae6-ba9b-41c0-bdab-67ddf414b5e7"], "a9fd266e-fd5c-4450-9d82-217382310904": ["9eb7bae6-ba9b-41c0-bdab-67ddf414b5e7"], "c8f0f9d9-4dd1-4194-aca3-52b6fab15963": ["96b3520c-e51a-4bca-880f-c6ee2e4549fc"], "fa96eadb-1fab-4b9e-ba49-cd43bf8bf7ba": ["96b3520c-e51a-4bca-880f-c6ee2e4549fc"], "8c64893a-146b-439a-89b5-4e4750acd04e": ["6ac87d10-79a8-477b-af36-2b91fca9d740"], "862afdc5-1ca1-4a87-a45c-10a511b3965c": ["6ac87d10-79a8-477b-af36-2b91fca9d740"], "45e7f413-1e12-4cbd-9e75-feec63a84920": ["8f14f4f7-2949-4890-aa60-776832710cf9"], "313bf6c8-c5ab-4fe3-8d77-26dda05b27ea": ["8f14f4f7-2949-4890-aa60-776832710cf9"], "121f9c82-7903-4809-898b-8c426fc28bdc": ["32b574ab-df81-4836-825b-ebd15d6f6933"], "e07d6280-119e-411b-b6af-e0451b343fbe": ["32b574ab-df81-4836-825b-ebd15d6f6933"], "4e92b2f3-df8c-4936-9d5b-e3bceebbf7e8": ["02403941-419a-4437-8539-3619d13dadd0"], "62120fd7-38e6-433e-b947-a099ec35b5cd": ["02403941-419a-4437-8539-3619d13dadd0"], "022119c8-675b-4f23-949a-f653e8082a2d": ["70607810-fc33-4977-b1c4-2aafa131fe73"], "d3d8dfc7-cc00-4a04-9c62-9a88959616aa": ["70607810-fc33-4977-b1c4-2aafa131fe73"], "8759afea-2213-407d-8530-165dab52c701": ["1d9e0333-6e5d-440b-9f8f-d66d847da116"], "4dacfc71-3361-4d75-80ff-5040c7b24d02": ["1d9e0333-6e5d-440b-9f8f-d66d847da116"], "440d320b-5312-4654-b068-bcc9414a8c64": ["388a8e0a-68af-4704-b41d-6aa3f1cf93d3"], "3ff43819-bbb3-4c9d-880b-005110a5d27e": ["388a8e0a-68af-4704-b41d-6aa3f1cf93d3"], "6ef4accc-35cb-4fdd-bbc4-e5b7ec44b8f7": ["284bf0f0-c913-4bd8-abfb-13eb20037df5"], "1ec1885c-3c1b-4e4e-893c-199c7f143158": ["284bf0f0-c913-4bd8-abfb-13eb20037df5"], "ccd2d100-7136-410b-a3e9-75d2e7df65c4": ["d6e30012-fa19-4dcd-a2cf-f69d6fb26661"], "d380a44b-f522-4256-8a6c-bbcbe3bf9fd5": ["d6e30012-fa19-4dcd-a2cf-f69d6fb26661"], "d7d70876-8523-4c11-b423-b58bde1682e0": ["9fbf8f80-e6dd-400f-a303-a41949e33f88"], "f4ca6361-9dfd-4774-8b7b-648d8658a665": ["9fbf8f80-e6dd-400f-a303-a41949e33f88"], "c64e0ab3-e23e-4c53-9817-d44f69b94258": ["c773eac8-5b55-4176-9caf-d8b787d44bbf"], "bf45fdf0-76d0-4f8b-a480-58147905bf39": ["c773eac8-5b55-4176-9caf-d8b787d44bbf"], "dc9e137d-d095-41a2-93e4-c4e8afaf5f81": ["7bb30ce5-0600-4b33-9bfd-7ac4ca73e5ac"], "7d9b39c3-7cc3-4470-8877-f18c1a1d2ae2": ["7bb30ce5-0600-4b33-9bfd-7ac4ca73e5ac"], "c8373bf3-02d7-42a2-8702-c5c5688a1481": ["7210036c-acb0-4819-9550-ffa78232f5ba"], "037654f0-c70b-47b5-809f-1c4d15e85c3a": ["7210036c-acb0-4819-9550-ffa78232f5ba"], "af90f57b-c0de-4a66-af9a-8037ec52d099": ["3c0df90f-a14c-4f10-aa0c-c97aff119f68"], "e6ebb88c-b398-4b91-bb03-cc70c6d85b80": ["3c0df90f-a14c-4f10-aa0c-c97aff119f68"], "eb57c278-22b2-4f86-9a60-f01f11c861dd": ["bf95a1a2-c283-48fe-a55f-e803793424c7"], "c95c78ab-a17a-42ec-a0ce-c8ef2c7106c8": ["bf95a1a2-c283-48fe-a55f-e803793424c7"], "a08c38e9-d179-4027-ab4b-298bf9297e56": ["320a9f8a-cf3b-477b-bff3-6d7763f40da5"], "29d179a7-4799-4b6c-8bcd-7ccbca8f2e0d": ["320a9f8a-cf3b-477b-bff3-6d7763f40da5"], "21827305-0823-497f-b15b-599ccb2a839a": ["84960ccd-69b5-46d1-8d44-e29d88913944"], "c82f1f3a-fb42-449d-b898-cb7b068f91c7": ["84960ccd-69b5-46d1-8d44-e29d88913944"], "879eea4c-1d4b-45dd-834a-cfe6195dfe0b": ["5222fdb5-dc94-46b4-a787-d0b88634fadf"], "63c0c80f-78df-4c6f-978c-35fd98b2f548": ["5222fdb5-dc94-46b4-a787-d0b88634fadf"], "c2da4324-ea8c-4489-9f85-4f656f87a83b": ["3d97b11c-2ed2-4889-b752-391f0291a629"], "5b540966-663f-4d68-90d9-12953e58080b": ["3d97b11c-2ed2-4889-b752-391f0291a629"], "56c9f95e-7504-4971-ade0-7e8bb9af0a70": ["07c4f5ec-74a2-4d13-91b7-24e7f03335a0"], "aa41d8b7-3f96-47e1-9e49-b70c8c028303": ["07c4f5ec-74a2-4d13-91b7-24e7f03335a0"], "6caf288d-335e-47a2-ae83-ce828c8726ac": ["1afec939-fedf-4622-b091-e4465e0875b5"], "5f0826be-9e91-4723-8c49-238a63f1ee75": ["1afec939-fedf-4622-b091-e4465e0875b5"], "a0e8e0fa-b5ba-4212-8ca4-e11f772ec3cd": ["f1486495-75ff-443e-b15a-df643197c8c5"], "bd0df280-49a6-46ff-a08e-e2fd030dedae": ["f1486495-75ff-443e-b15a-df643197c8c5"], "ffbbebbf-8490-4b4f-82e0-fa7bc3326dc6": ["cf51300a-fd0f-4a8b-aba6-71b8db414ee5"], "427a26a8-9d70-44ba-98c7-900a134e9647": ["cf51300a-fd0f-4a8b-aba6-71b8db414ee5"], "05e99477-e456-498c-b40e-1438e37d559b": ["d7320b82-d133-49cd-babc-2edfa833533a"], "482ad7b6-78a3-4465-822c-7619b62a6239": ["d7320b82-d133-49cd-babc-2edfa833533a"], "17e92a1e-b7d8-471f-8761-c48a2585ce48": ["15f67fe3-092e-437b-8fba-86194cda745d"], "9758304b-6b26-4c4c-a495-cf0db71fff15": ["15f67fe3-092e-437b-8fba-86194cda745d"], "6685a18c-d7cf-4533-a22f-5403166a2fe8": ["64d3b194-0d10-43b7-ac48-6b7fd3f5a728"], "0a2c3b6d-6a54-4ac4-b9b0-f13a48fd4de2": ["64d3b194-0d10-43b7-ac48-6b7fd3f5a728"], "d812ff42-89e2-4ec2-88db-65fb862e260e": ["4a3166f2-fc7d-4645-914c-f71a8fd0fd88"], "08502bf6-3cc8-43cf-a6d4-fbb9dd4bc94e": ["4a3166f2-fc7d-4645-914c-f71a8fd0fd88"], "c1ba6a8a-f2c2-4e31-9c7e-87cb44353015": ["ef6d2ad8-ebcb-4fa2-bb91-b1ad2693f34a"], "5e6b5891-f31d-40dd-9de9-a689975eb1d6": ["ef6d2ad8-ebcb-4fa2-bb91-b1ad2693f34a"], "eef23962-ae8c-4048-aa9b-a42e6934d7f0": ["15f1b827-8da3-4e39-9696-1b9a845c86a5"], "2e1b4f00-30d4-4781-883d-07335ae17f31": ["15f1b827-8da3-4e39-9696-1b9a845c86a5"], "ace5291b-d405-4032-bce1-2192a8287c53": ["addaf767-7c86-46b7-80aa-28655ccafbe4"], "92b6ba26-57a0-4144-99e4-46847b9db0ad": ["addaf767-7c86-46b7-80aa-28655ccafbe4"], "54662c3c-3b4d-4b68-822e-302640e48167": ["f4065dcf-bf73-4ae2-a6bb-16db538d597b"], "4f17f355-e531-4dfd-a69e-d17f5127eb47": ["f4065dcf-bf73-4ae2-a6bb-16db538d597b"], "d3015184-067a-4f85-828d-9419f38ebe7d": ["82c714ee-ee32-4528-b3b5-1544183465af"], "b9db7225-3aab-4890-b433-9163917b2cbd": ["82c714ee-ee32-4528-b3b5-1544183465af"], "00eb2463-330c-4b4c-a333-af8ea394402a": ["7c72cb92-5744-44ef-963b-8b0601fd1959"], "562255f8-2171-4089-9ac6-e94959d127d7": ["7c72cb92-5744-44ef-963b-8b0601fd1959"], "b0c27726-d570-4f03-8c13-8d09fe49138d": ["acf6667f-34ed-46ee-84b0-260cfe802491"], "217093c3-c905-4a5b-8785-667c6d76c50a": ["acf6667f-34ed-46ee-84b0-260cfe802491"], "91ebb5f1-6ac6-4f16-ac97-d9758ae20b12": ["64997920-e3c3-43af-b3d2-8d09925627cc"], "dd0b92f6-85e4-45b7-bfd4-1f462556d878": ["64997920-e3c3-43af-b3d2-8d09925627cc"], "6f806534-7869-4fb6-8e06-0354094e316f": ["0024ba76-72ab-4845-a492-caa62b5e1747"], "f6b3d5a2-7890-4977-81d9-eaf14525e139": ["0024ba76-72ab-4845-a492-caa62b5e1747"], "0a1c8be2-2819-4e53-9e67-484f320af15b": ["1e88bf8f-3a6f-4116-bf99-f76291748b33"], "12fb84d3-58a4-4149-910e-c65c4c60bf1c": ["1e88bf8f-3a6f-4116-bf99-f76291748b33"], "aa81ac5e-a134-46e2-9095-8d17698fc227": ["66619ab4-51be-4cf3-9585-49a8251fa615"], "cd1fd036-4c79-45e2-abd0-6b59198164f3": ["66619ab4-51be-4cf3-9585-49a8251fa615"], "e66ea9b6-691f-4df9-b796-b98a9e6d23bd": ["885c6001-eb5d-4b31-9314-4cca630d58eb"], "f7ba5f62-d047-4b12-a8b9-8204de59843c": ["885c6001-eb5d-4b31-9314-4cca630d58eb"], "ab7d779b-083d-40e9-b1bf-3d921bec998f": ["a9e910c3-8eaf-4524-a17f-0d9fa0bba947"], "19251687-f898-4279-9132-c2a5940cdfce": ["a9e910c3-8eaf-4524-a17f-0d9fa0bba947"], "388c31f3-d681-485f-9cc6-ca65bc53c856": ["26e5d365-ee16-4e37-9bfd-22be24c5e1a9"], "1307a1ac-da37-457b-a59d-20498d2a4bd2": ["26e5d365-ee16-4e37-9bfd-22be24c5e1a9"], "cc9da629-4b96-4240-82f0-af0c190e0ceb": ["dcdf0c46-6835-45f4-8e33-e93b3790db3d"], "026ccc65-6bb2-4bbc-a725-c2416f258492": ["dcdf0c46-6835-45f4-8e33-e93b3790db3d"], "1fbcb8ba-e9ce-4904-ad0b-587ac0ca27ce": ["f8563a71-e404-4d29-86ec-63b4d95a1b9a"], "ac69bdaa-4d02-4dec-91e0-f39355715fec": ["f8563a71-e404-4d29-86ec-63b4d95a1b9a"], "68a41bf6-0d87-46bd-91bd-7f2fb4edfc55": ["d656cb70-6a01-42da-9ff8-447ad3efc799"], "2a6f16db-12a8-4e6c-9f9f-2edcf928370b": ["d656cb70-6a01-42da-9ff8-447ad3efc799"], "6b3c9c23-eb38-461a-94f9-07a7f6965229": ["a1449222-e53d-41c4-9a7d-64c24fb004e5"], "16cf78bb-c4d0-4144-8d10-e6dbf976f4e0": ["a1449222-e53d-41c4-9a7d-64c24fb004e5"], "7a090519-47bd-4909-a436-068b8936a9fc": ["15c61fc1-8c4f-4452-b6f2-127bdb367650"], "63fbc7f8-f4b7-4c3a-a80b-e5dfcc25d2a3": ["15c61fc1-8c4f-4452-b6f2-127bdb367650"], "37be8050-9dc9-43fc-bcb1-8fe0b7f32687": ["dd023346-71f3-4f53-8861-8768d815c002"], "22b42e66-fad3-4c65-acf9-b1f9dcc1cd01": ["dd023346-71f3-4f53-8861-8768d815c002"], "f25bf8d4-4535-4d77-8166-083f5a935d4c": ["cf569b91-d97e-4871-b54f-690ba02f4707"], "179ad3a6-1cf4-4f3b-8fe1-9a6e2ceb1bf7": ["cf569b91-d97e-4871-b54f-690ba02f4707"], "bab93157-ef62-43c6-8018-20ff0edb8689": ["e2ffcc20-0808-4994-8d78-18f3ab52d874"], "445b3544-ceff-4332-a602-54b32165a543": ["e2ffcc20-0808-4994-8d78-18f3ab52d874"], "3e608b4f-e830-4765-b9de-59b7efa36964": ["72f04af9-1c06-4d2f-ad95-9072a7fa2c3c"], "4153e7e9-83d0-45a2-8149-60453ae3473e": ["72f04af9-1c06-4d2f-ad95-9072a7fa2c3c"], "4990a25e-ff36-4112-9653-0361ff5119e8": ["bab17963-2f97-4633-ad8e-0405e7bf0213"], "01051c46-d951-4b59-a96f-42e4a1d12fc1": ["bab17963-2f97-4633-ad8e-0405e7bf0213"], "9546a589-c675-4224-91fa-60446c99f3a9": ["5d3f7e37-0ab4-47fa-b0b8-330a17b35163"], "00334ec7-eb04-4c2e-93eb-88f350be5685": ["5d3f7e37-0ab4-47fa-b0b8-330a17b35163"], "b67c46ce-3c43-4127-acc6-e31e06a44421": ["04de3548-c169-4f3b-915b-78dad74ae71c"], "7956197e-9ca3-4a9b-9571-fd584a27af8d": ["04de3548-c169-4f3b-915b-78dad74ae71c"], "2c042492-9062-44ca-80f2-3cb7781ad055": ["9c980d10-927d-46ae-a49c-657484e24a3d"], "e5513860-660f-423f-90a6-af0fd082a932": ["9c980d10-927d-46ae-a49c-657484e24a3d"], "e54fa07e-3e07-4f1a-af95-238db8f73285": ["c58df9b0-8568-4fb9-b7f7-9a54396f10ce"], "c57d39eb-607d-4c41-a5fe-f4a3a36fb313": ["c58df9b0-8568-4fb9-b7f7-9a54396f10ce"], "617f86d8-c8da-425d-a175-22bd7a077c22": ["1cc15760-d963-4710-81b1-eab3d7f22ebc"], "92949a62-8971-45ec-84b2-7b29baa30396": ["1cc15760-d963-4710-81b1-eab3d7f22ebc"], "209e3cbb-a414-4733-a855-3ad2990a57aa": ["29ccc988-39a2-495c-9b9b-db6d0f698e52"], "b1caf441-4206-4bcb-8539-e94ae8e1d427": ["29ccc988-39a2-495c-9b9b-db6d0f698e52"], "c138f871-ee8c-4f1a-a56f-9ac1d5b6bab6": ["e2f8e10d-59e9-4db2-a953-5ebfe27bc88d"], "5db37c4b-566f-42fb-8980-de127941c908": ["e2f8e10d-59e9-4db2-a953-5ebfe27bc88d"], "871398e8-debb-42ea-be42-fa7343670b8e": ["77cbc32d-fd24-4d74-9328-fb8117b79b5a"], "19ae23dd-fca5-4dd2-a904-c7b3022a8b36": ["77cbc32d-fd24-4d74-9328-fb8117b79b5a"], "7b1e72ef-6fac-4dfd-baa9-116b76741bbf": ["d2f0639d-87a7-4f85-a092-1986c7b71a08"], "ca802ba8-80cc-4d3c-916f-4bb4da90857b": ["d2f0639d-87a7-4f85-a092-1986c7b71a08"], "c0812497-006d-433b-9793-bed7ef0ed900": ["cdc2d327-aa60-4db1-8925-eb5a85c318bd"], "c0118b13-51c9-4edd-a1dd-f6c71f7f7919": ["cdc2d327-aa60-4db1-8925-eb5a85c318bd"], "fe67901b-170c-4b55-82e5-e42f6dbb4141": ["2454b84e-7842-4241-a90f-8a8db64f9d43"], "ff3ac23f-c5b2-4075-9465-e290331fa4a4": ["2454b84e-7842-4241-a90f-8a8db64f9d43"], "c8c5653b-1645-45d5-b580-8c2472a5cf52": ["f1c6beca-4fdd-4b12-9c6d-7e24a26bcad2"], "5357ba14-cdfd-428e-988c-83f2294fbdc5": ["f1c6beca-4fdd-4b12-9c6d-7e24a26bcad2"], "f86f5106-d2b9-4129-beb1-5c6dfbd75f4e": ["33da7cae-ae21-496b-8026-a45ac0a5d140"], "b789df15-89b6-4057-a27c-ed90ba21939e": ["33da7cae-ae21-496b-8026-a45ac0a5d140"], "f0c44885-a26d-4f90-a581-2d38fb40074c": ["178bdbaf-625d-4777-aa5a-7f0c51ffcbcd"], "15446d6f-74b6-4174-842e-72b4e89501be": ["178bdbaf-625d-4777-aa5a-7f0c51ffcbcd"], "dd2eac10-e764-4b4a-9032-8ff57e3c6b73": ["d578bdf8-490c-4a99-81f1-9baf1338d684"], "d3d6b54e-fb46-4e28-90e1-b953afa93dc2": ["d578bdf8-490c-4a99-81f1-9baf1338d684"], "2ae0d18e-e434-4ce6-9686-cf59fb6b3092": ["9079ebe8-b9c4-422b-bd5b-473bc9792a4d"], "8d5f03ff-8c34-499b-a4e1-d8157029ee0f": ["9079ebe8-b9c4-422b-bd5b-473bc9792a4d"], "65bb1d8a-fd18-4afc-8b62-6ad095a61e8a": ["5e2dbc3b-32b2-4158-8255-bfab23a5a84b"], "afa72550-84ad-45e4-a029-7c5e2d17b4e7": ["5e2dbc3b-32b2-4158-8255-bfab23a5a84b"], "0573cf74-c634-44d5-ba70-af3350e0bbca": ["fa6fde95-3cf3-4931-9167-e045cc46311d"], "3f032e60-78c1-49e2-a6da-dc2cce29bfb0": ["fa6fde95-3cf3-4931-9167-e045cc46311d"], "a34c6721-e7fe-426b-8d12-dcf1c65faee3": ["9482da44-56b0-47e3-867f-e35af0504811"], "2f71f30b-36ab-4786-9205-0bd781b5a408": ["9482da44-56b0-47e3-867f-e35af0504811"], "28d50e27-0577-48c4-b4c2-18d92edbe514": ["c3576568-c7a7-474b-be7d-784a7b6d5a99"], "c3b0bbc5-7fd2-4d2c-b36c-51e3b6ccd88a": ["c3576568-c7a7-474b-be7d-784a7b6d5a99"], "cac32498-70ce-4097-8315-7caf5147c141": ["474752d0-4f05-429a-8eb1-b2e9c2a145fd"], "7a88bfa7-50b0-401d-8ca3-47601dc1cbdb": ["474752d0-4f05-429a-8eb1-b2e9c2a145fd"], "4d584453-9d84-4560-9948-05662b0fcbb5": ["28c283ca-bb93-4ee6-9549-610078737859"], "284f5fbd-7f7b-4580-8d6a-4047300640db": ["28c283ca-bb93-4ee6-9549-610078737859"], "6b3b55b8-dfd0-47a8-9fec-c1b57ad6267f": ["191380bc-73da-4332-ae19-e34a3531f0a6"], "9e2647bc-80f5-449c-87bc-d56c9646f370": ["191380bc-73da-4332-ae19-e34a3531f0a6"], "f06bc859-c6e9-4df7-8d4e-634dd2074454": ["91f77135-1c3e-48d6-8582-59cbd51aa3b7"], "a1351a2a-f2a3-4a86-9f05-8fd9b9ff4a42": ["91f77135-1c3e-48d6-8582-59cbd51aa3b7"], "6b6e0162-a818-4f70-8674-d54933989ad9": ["54777858-c093-4e16-b64c-d83f8c91384f"], "43379da9-6bad-49b5-9c91-b4f0f88e2310": ["54777858-c093-4e16-b64c-d83f8c91384f"], "df684eeb-3334-4b3c-950b-22367a3ade89": ["6a960312-dd8d-4c68-9c02-e82e1faea9f5"], "61856785-5c4e-49e7-89e6-4050a55fa96c": ["6a960312-dd8d-4c68-9c02-e82e1faea9f5"], "90d869b2-cee9-4c56-915c-654fda9f227a": ["ebd3b826-524a-47fc-ad2a-e1da632b69c5"], "c1f75d6c-27ed-40b9-befc-ff1e65902b17": ["ebd3b826-524a-47fc-ad2a-e1da632b69c5"], "7b44d3b4-c8dd-4a73-86f4-c0d5ba039c75": ["9c43c322-dfc9-4f06-a164-d6b37a9eca0f"], "20bbee03-1540-4db2-afcc-b591cdd559be": ["9c43c322-dfc9-4f06-a164-d6b37a9eca0f"], "86761d95-7806-4069-b9d1-bb0ae160b5ed": ["1d950592-0348-48b5-89a3-8ce33a2ca43c"], "a43a5b18-9f0f-4c42-ad67-43380c0d49a2": ["1d950592-0348-48b5-89a3-8ce33a2ca43c"], "7cc29955-e340-4430-b146-a7611e709c88": ["d846bb0a-5558-4aca-91ca-962e6d1b5023"], "f4305e3d-9a16-4b92-85bc-27cf25adb3c9": ["d846bb0a-5558-4aca-91ca-962e6d1b5023"], "9c482bbc-b948-48c5-b071-cc7f6fafcbee": ["042e40bc-137f-4663-9552-54961f9f757e"], "04f91e5c-4ff4-40c4-b2af-bc0a75bbf9a3": ["042e40bc-137f-4663-9552-54961f9f757e"], "45a8e235-2618-469c-9f66-d93a6a38d3fa": ["ce6be881-7637-4e38-ba95-44c2402d4eb5"], "a867b1e7-46bb-46fb-b6f7-03e6d5e53658": ["ce6be881-7637-4e38-ba95-44c2402d4eb5"], "19227b4b-3748-401c-82e1-f65eec7e1987": ["95ba3a07-5283-4935-9b69-446659d8cab3"], "9201bc25-550e-4250-999b-748722a2eb6e": ["95ba3a07-5283-4935-9b69-446659d8cab3"], "35100d31-258b-4927-ae84-c2e1acd29f05": ["b547593a-a9df-4f8e-9881-2c5478a8e89f"], "5688741c-26ca-4bc5-975e-44e6aea8cc04": ["b547593a-a9df-4f8e-9881-2c5478a8e89f"], "8e8033af-1acc-4cd6-890a-87f249ea26fd": ["d6a6f1b3-f544-43d6-b1be-89747c0706c2"], "91993f90-2129-4899-9e6e-5629d9218594": ["d6a6f1b3-f544-43d6-b1be-89747c0706c2"], "b0e16fa8-d18a-4960-b739-124315bc8b80": ["e13f8677-dc1f-427b-9c01-85ea077d0a7c"], "80c51600-c396-4fbb-a9af-397b1bf4df10": ["e13f8677-dc1f-427b-9c01-85ea077d0a7c"], "00e38210-0e59-468c-bed2-e1861fc8fa72": ["0e74ae24-e850-4843-8bd0-46212443df76"], "f2499075-4456-4b99-843f-be61fa42fd87": ["0e74ae24-e850-4843-8bd0-46212443df76"], "568d5e9b-2a53-4d65-a2a4-c25447a264d4": ["c33626cb-a5bf-4fd8-b0f9-243a9d1e6c24"], "d649feeb-3824-45fc-8760-22830fe7bd1e": ["c33626cb-a5bf-4fd8-b0f9-243a9d1e6c24"], "d1c718e8-56d1-46c2-858d-117077b73507": ["ca68c07f-0dc2-43aa-af85-b842e1c8a93e"], "da8c32e1-3387-4723-84ae-7496f0798bfd": ["ca68c07f-0dc2-43aa-af85-b842e1c8a93e"], "8302b95b-a3d7-4d82-a1f7-bfdc4a562ae4": ["aa66b36f-05f3-4224-abb7-ae217524332b"], "91947ba0-a268-40c7-84d0-3d12a7ad3861": ["aa66b36f-05f3-4224-abb7-ae217524332b"], "7ba61c64-6d80-4f16-986a-6d98866e17ec": ["393cbcfc-3eb0-423e-ad1f-4dafabe76b34"], "0ea09853-e524-4fbf-90c7-efc92f19cf2a": ["393cbcfc-3eb0-423e-ad1f-4dafabe76b34"], "54dcd851-b53a-4dec-9d4e-25959431298c": ["90a1f5d6-9a78-4491-8ebc-2e7bc1756dc5"], "0932885d-9262-4c08-8138-90adae0cacf3": ["90a1f5d6-9a78-4491-8ebc-2e7bc1756dc5"], "7dced792-124c-4fc4-acb1-14acf6e0aba7": ["0f26971c-5936-4a44-8cbb-787131e527c2"], "4601d51c-2533-4e35-95c7-0e7ac23df4b0": ["0f26971c-5936-4a44-8cbb-787131e527c2"], "98574c22-9001-4181-b94f-0174572e69a0": ["ecbc1513-3de2-42fc-b21f-e591a7ca1359"], "73eb8344-5811-4b80-9bd3-2908daa20b14": ["ecbc1513-3de2-42fc-b21f-e591a7ca1359"], "d367fca5-e6f5-4172-997c-22933e02d5d6": ["eb240bfb-5213-4b97-9193-c24ccd04989a"], "b018df58-62ee-4950-aedf-6f4face1a456": ["eb240bfb-5213-4b97-9193-c24ccd04989a"], "eed35e80-0c64-4fe2-96d6-e62fbde504a7": ["59d661c7-7ed8-4a0b-b32b-6b55c13461a6"], "ca2fbff7-bd7f-4eb6-89e2-509315f4f026": ["59d661c7-7ed8-4a0b-b32b-6b55c13461a6"], "670d4ad9-791e-43d2-b50d-ce315ec93bff": ["8acec6c1-fa7a-40d7-900f-1fe306e07aa6"], "c6986ee6-197d-4d0d-b5c3-2273014ca4a8": ["8acec6c1-fa7a-40d7-900f-1fe306e07aa6"], "8fdb5621-c22f-4ba9-8fe6-542f5140f8fc": ["1498c5ba-3791-49a5-867b-73b92217b4ed"], "f4475605-1469-4f7a-86e5-5c6c4dfff062": ["1498c5ba-3791-49a5-867b-73b92217b4ed"], "58ef895e-9d3a-48b4-bc65-2d7ce3e624ad": ["42b9801b-79f6-4a4b-896b-a05f7e6f938a"], "9dc5d2ac-6501-4c9b-8fc7-8781ac526be4": ["42b9801b-79f6-4a4b-896b-a05f7e6f938a"], "3f342f9a-9dfa-4cfb-a805-f03ea30936d0": ["3f06930f-34ae-465f-9127-8a0510821b2c"], "8272595b-9c68-488d-9dc8-23317f95e0f0": ["3f06930f-34ae-465f-9127-8a0510821b2c"], "433a16fd-c540-4978-a0f2-7d52f44d9a34": ["cb8f8727-af89-4193-9272-526395679ece"], "4040256f-1654-4824-a930-9d34a3f7d9e8": ["cb8f8727-af89-4193-9272-526395679ece"], "8bb8bbab-3dc8-41a3-833c-85d16690c27f": ["49b8a8d9-6be1-41bb-95fd-9599977b40e3"], "9e26333d-0470-42fe-b3d9-7748eb9e8d6b": ["49b8a8d9-6be1-41bb-95fd-9599977b40e3"], "1a4406b0-3ea8-47ff-a6b9-cb3cb90f44fa": ["127dabc6-591a-46f3-b983-4dc993bb6074"], "a9e5b9fe-1741-41d2-b973-8ecbdb34b7fe": ["127dabc6-591a-46f3-b983-4dc993bb6074"], "2bc3f62b-40bd-4601-8a92-a5f61c7fd080": ["40c08338-a614-4372-b193-ffc65bb8fa63"], "262b7d02-b974-4b62-905f-1f49f44388e0": ["40c08338-a614-4372-b193-ffc65bb8fa63"], "d5bfa5ef-3ed2-4ffc-ad48-5d322b8e5744": ["bcbe36ec-6b0a-4955-8d3e-34608c088928"], "5686b77d-627a-4a7a-a842-b53a5d23f801": ["bcbe36ec-6b0a-4955-8d3e-34608c088928"], "3a62d74d-a065-43f1-a498-9282e52023b7": ["10f9e473-83b3-48ff-be16-32db6b105992"], "588df61b-8d7b-47a0-abc2-20c19b242ec1": ["10f9e473-83b3-48ff-be16-32db6b105992"], "3ed2350b-1301-4669-9615-7cd29418e762": ["b1143d64-22b0-413a-ac03-68bbd6392dee"], "d9c7d599-b536-4465-9bf9-f57012d08166": ["b1143d64-22b0-413a-ac03-68bbd6392dee"], "522f20a1-260e-46fa-9bea-6d9279815bef": ["6fdd931f-8ab5-4ac4-99ba-95a2fa9988ca"], "f9b0f71b-2368-4e94-8fde-02023651655f": ["6fdd931f-8ab5-4ac4-99ba-95a2fa9988ca"], "75d5081d-922c-4e2b-9eb9-bbbb8fe09786": ["db772381-adfb-41e0-bfa8-e6d177d4fc4f"], "d05613fc-32a9-4f0c-b8d8-505ae888caa9": ["db772381-adfb-41e0-bfa8-e6d177d4fc4f"], "c9ac2658-baaa-4935-80ec-1ed96d504d4e": ["39624cba-2c85-429f-a578-66871e590a49"], "ab5dc45a-355c-4963-99c5-7e8a2c9032ee": ["39624cba-2c85-429f-a578-66871e590a49"], "4da428b4-93fd-4e59-9f96-0b5383fd29d2": ["6fc9575a-0b2d-4162-9bf6-ddbb9ff3dcc2"], "d3a5fd8a-9875-488d-a700-c0379f7d0840": ["6fc9575a-0b2d-4162-9bf6-ddbb9ff3dcc2"], "d50f8538-352a-4ffc-a882-dcb8959384d1": ["a07f79fb-249b-4a1e-9bc1-fae65d99b374"], "53fbb287-5e44-48b7-a706-283c47c73bfb": ["a07f79fb-249b-4a1e-9bc1-fae65d99b374"], "e50b831a-3a29-4d74-9492-ccf5331cb3cc": ["1c5821e8-25cd-455b-aa81-e84260072bab"], "77cdcb7e-7ed1-4618-93ab-632568a6bd0c": ["1c5821e8-25cd-455b-aa81-e84260072bab"], "7abbd449-5cde-4df7-adca-dac356f7cdf7": ["3ffcefc2-5dc7-4b37-92fb-eb8a5d78b8b3"], "3668dbf3-6749-4772-969d-a108c1f04903": ["3ffcefc2-5dc7-4b37-92fb-eb8a5d78b8b3"], "f3b608e7-0779-4388-ad78-bacbecd4a672": ["580f2db8-8415-4aa6-9fe4-fcb63d5bcc3f"], "5f7a632c-6bdc-4240-8f17-300f93998569": ["580f2db8-8415-4aa6-9fe4-fcb63d5bcc3f"], "1e006d60-237a-42f0-8076-f38eb25bf6d4": ["37db406b-c270-45c6-b644-9a85a90bd617"], "69405fb0-0810-4ea4-9e75-29ef608f91b2": ["37db406b-c270-45c6-b644-9a85a90bd617"], "d3431fc6-1a26-431c-9df6-045368fcc00e": ["aaec4923-df0a-417c-ab69-41de41fa921f"], "421fd769-39b2-45c3-9457-5e38ffd58634": ["aaec4923-df0a-417c-ab69-41de41fa921f"], "efe55b57-94c8-492b-a75a-a10d7d31b1c4": ["3c7a90aa-0ae3-4b47-aa92-5eabbddb08b6"], "2a046917-5086-4bdb-a5d7-b559ccc6fdea": ["3c7a90aa-0ae3-4b47-aa92-5eabbddb08b6"], "07a64697-1323-4a24-a6d1-3eb4bb5075e1": ["a9e56adb-2851-467f-8c88-9ea345f2b538"], "2447398e-12b0-48d1-93dd-80c1093f2175": ["a9e56adb-2851-467f-8c88-9ea345f2b538"], "7e1ef5db-ea16-4d72-8652-83e2d2c2796b": ["972d5cc1-bd99-472a-a4c8-c715096b6e61"], "e9c32f82-0ab4-437c-84bf-46179845b8f6": ["972d5cc1-bd99-472a-a4c8-c715096b6e61"], "eb6f2339-5835-448a-af94-f8236ba90e11": ["74950177-49a9-46e8-a162-4a60ca08f965"], "0570cbed-3520-4585-82eb-30a22a104d5d": ["74950177-49a9-46e8-a162-4a60ca08f965"], "bcf24575-8d84-4edf-9cce-48ddc5ee0faf": ["0a7da557-a485-453d-8e1c-0c45b60597bb"], "d27f6e98-19f3-4073-9f10-9e1312f69406": ["0a7da557-a485-453d-8e1c-0c45b60597bb"], "818a4dd9-5054-4f4a-bebd-a7cfe737486f": ["787bf7c1-d709-4dba-ab77-0b45d99cfcde"], "ee5e7804-d5dc-4f30-9d9c-943c4b2624b0": ["787bf7c1-d709-4dba-ab77-0b45d99cfcde"], "aa301ad2-a144-4e28-aba5-32a66523436b": ["d855dc71-56b0-4c05-9109-5ff9268d5351"], "ec76ec19-a322-49b9-99a6-7d7e8957842a": ["d855dc71-56b0-4c05-9109-5ff9268d5351"], "ee38f986-4f8f-4997-94f4-e7121ff0936d": ["39d2f1be-0856-4873-a601-b43b3364c09c"], "46edbdac-ca2c-45f3-8983-2bb2703cdaec": ["39d2f1be-0856-4873-a601-b43b3364c09c"], "d9fba9bf-ff32-40f9-be90-533fd00dee09": ["975a8b11-da95-46a4-972d-a52efcf70eeb"], "23403185-4e9b-4048-a7bd-42a5162f7eff": ["975a8b11-da95-46a4-972d-a52efcf70eeb"], "afdad59a-8d9b-47ee-9f15-e4aa7ad24e8e": ["1947e184-b336-459e-89e6-b2e4d1ca7d0d"], "6f0b6103-d318-4d3e-aac7-99121dbf385d": ["1947e184-b336-459e-89e6-b2e4d1ca7d0d"], "ced5cadc-1eff-4975-97a0-10782e9f4729": ["6bdfd516-e0bd-4ad3-bda4-5c6cf908a3af"], "ed295a73-f263-433e-957f-de3169eba5ae": ["6bdfd516-e0bd-4ad3-bda4-5c6cf908a3af"], "de8a52e4-3e82-4b72-a933-5c3bf00e665b": ["385954bc-4c86-4c7d-a4b6-1a9d195907be"], "6c2f0573-89cd-41f6-8d3f-372db5e94d1e": ["385954bc-4c86-4c7d-a4b6-1a9d195907be"], "19914be9-45bd-42a7-a1e3-14d827646633": ["79e08302-c037-4b11-a38c-dcf039f5345e"], "1c60f5c8-42e6-49e0-9bf4-a00e9e540fb8": ["79e08302-c037-4b11-a38c-dcf039f5345e"], "7dca4d20-4ef0-49fe-9266-0ffc5ee147ea": ["830c7c3d-78f8-4e74-b6e6-1ffbae8bd8be"], "e01c37e8-616f-410c-8f84-f47c80b79c3c": ["830c7c3d-78f8-4e74-b6e6-1ffbae8bd8be"], "3f7bac5d-800c-42e2-8d5a-ba7971e8ab41": ["146f38dc-15a3-4385-8408-ac43c18c0639"], "97fb7019-e43d-4a5e-be8c-5338a22f1e77": ["146f38dc-15a3-4385-8408-ac43c18c0639"], "f523c7db-d0dd-4254-becd-e0d446266102": ["4bfcc364-839f-4848-a922-2e0c1e79f9e5"], "9f7e08cf-1101-4a54-9126-c48e4a29dbc4": ["4bfcc364-839f-4848-a922-2e0c1e79f9e5"], "f07213a8-088b-4e64-bb5c-13087794bc0c": ["94a65f59-f102-4309-a9bd-b5c443b1dd1b"], "4d9cb278-709d-4d86-a9d3-0166e56bba4f": ["94a65f59-f102-4309-a9bd-b5c443b1dd1b"], "bdfa454e-8811-40e2-a1b8-8612efacf83e": ["da601a31-dfab-49e0-86cf-eef8e55a4f98"], "1d0ea6bd-7eb9-490d-9d71-e216b294a2df": ["da601a31-dfab-49e0-86cf-eef8e55a4f98"], "0946b374-67f1-4ad2-82b1-a1a600e2cfe3": ["d7881d3c-24e7-4585-93af-8695a597ba95"], "bcf0d8ab-d72a-41c9-89f7-c2c9f5201643": ["d7881d3c-24e7-4585-93af-8695a597ba95"], "d9157e6f-9928-4a37-9a79-fd09d73844c8": ["fde0a57d-613d-4e91-8b21-36327dcf8774"], "2aa7c8f2-95cc-4328-9096-654352b7dd2c": ["fde0a57d-613d-4e91-8b21-36327dcf8774"], "437b6fa5-ead7-4392-9a1a-3967e3f6c814": ["554181be-cf89-49c9-b571-9e39219b45ed"], "6bb5e6e1-2c40-4ca5-84ce-0bb7aa44c640": ["554181be-cf89-49c9-b571-9e39219b45ed"], "49cf1b46-1b5f-4b12-b624-f6395f1bb465": ["0955182b-b64b-4db9-a8dc-d4ddf81fe014"], "5d1bee94-fe36-4cfe-b172-f56d60f2d973": ["0955182b-b64b-4db9-a8dc-d4ddf81fe014"], "54d3ebcc-992c-47ac-b26c-73b41f4b1066": ["242e0ec6-efbd-48cf-bcb1-54a0191361d0"], "00f8d243-5dcc-4fb0-8274-69adb80f424e": ["242e0ec6-efbd-48cf-bcb1-54a0191361d0"], "a24ae91e-1a03-4814-bc53-5c1d945dd7fe": ["6f23a99d-bce0-4da9-bd89-c5350c1c9644"], "60bf9e79-1458-44f5-9f8b-7aefbcbfd556": ["6f23a99d-bce0-4da9-bd89-c5350c1c9644"], "713a991b-8a8f-4972-9275-6b7b4ed7983b": ["c079913d-5668-44ef-b6c7-4e2660f94099"], "5a21399e-3e68-4267-ab90-984b0dd68440": ["c079913d-5668-44ef-b6c7-4e2660f94099"], "10df46e0-aa53-4cc8-ab1c-38bdeee23ed8": ["f87b9cf8-7404-429d-94f2-28b22ad8e1bc"], "cc72cd6a-a029-4422-932b-ad3892f12949": ["f87b9cf8-7404-429d-94f2-28b22ad8e1bc"], "a46f1765-4104-4060-bf4b-cc3a9a87c169": ["61d43bdc-f423-4809-ad08-b21856b62465"], "d2f22508-8074-4588-a9b2-30e3b9c9bb1b": ["61d43bdc-f423-4809-ad08-b21856b62465"], "7a4943ad-0b2a-47ea-9345-d7117b715e89": ["3c7f560e-924d-4931-a830-1f1162ff97d1"], "efa24a20-746c-4452-a068-793a94874939": ["3c7f560e-924d-4931-a830-1f1162ff97d1"], "76acfb70-0b8f-4d02-9d0c-ed71a08126f0": ["104fca8a-92dc-4c38-94d7-0b08437f3cd7"], "854e290d-18b6-436a-9d00-1da4abb72bf2": ["104fca8a-92dc-4c38-94d7-0b08437f3cd7"], "3a30161e-eb65-4c76-8b82-9c96d692ac8c": ["b3a70cda-c86a-4c60-b737-ba600bc9b1d6"], "63edbe35-9e61-4194-b0ed-3ece00033578": ["b3a70cda-c86a-4c60-b737-ba600bc9b1d6"], "a2165b66-b277-403a-ac4a-df5f42045bfc": ["9ea42846-3aba-4bbe-b340-aa710755a097"], "979b9120-8c5b-47e0-a244-81f9e1822750": ["9ea42846-3aba-4bbe-b340-aa710755a097"], "fa6a6cbd-f7a5-4049-949a-444c36c9922f": ["3f777ea5-23d5-444e-abfc-8b7659d25ca5"], "c1c2b6bf-ff54-431d-8c7d-abbfccb8c1e5": ["3f777ea5-23d5-444e-abfc-8b7659d25ca5"], "f715d51d-7856-4b22-bfcb-696c03bb19b6": ["0e25fc5a-9c40-4d0e-8460-4eb68b9b8c12"], "340a53a9-fc52-453c-b055-b820f2b2c01f": ["0e25fc5a-9c40-4d0e-8460-4eb68b9b8c12"], "acc3f8fd-6a30-429a-be35-22ce9826f73a": ["9d856b4f-a199-48d0-9ebb-0a3e149854e4"], "0d1515be-0297-47d3-be57-23e493924ffe": ["9d856b4f-a199-48d0-9ebb-0a3e149854e4"], "852a8805-c66e-40c4-86ee-4ce374f0c2a0": ["3f60f140-2a94-4e5a-aae4-f54530704075"], "f9f20f2d-faaa-4a0f-989a-1bcd6bcf0bf1": ["3f60f140-2a94-4e5a-aae4-f54530704075"], "98f87900-cbae-49e5-8f1f-0639ce47f976": ["1c651545-95e0-4ab4-953c-1ef65ddb2be3"], "5d4c5e99-bcef-4ba3-8bda-4caffa863a53": ["1c651545-95e0-4ab4-953c-1ef65ddb2be3"], "0c7fa675-b8ec-478e-95a7-500958f22157": ["4e35fafd-81a3-43e1-b271-9d5c4da66e60"], "a239a6da-dc7d-427d-9ba5-5d4934386201": ["4e35fafd-81a3-43e1-b271-9d5c4da66e60"], "42f8e9e1-f6ca-4695-a0a3-0e692840f3f4": ["3b72284f-1c72-4114-9c41-64be27da5b6a"], "59b9097c-5525-4b82-8443-eccefd9b1cf3": ["3b72284f-1c72-4114-9c41-64be27da5b6a"], "e85147b2-414a-4c8d-ac16-18762e3c46dd": ["62cae94c-7d87-4966-8848-cc0feafa2d37"], "f9ecb98e-a971-4c38-b8aa-e240e7db75e3": ["62cae94c-7d87-4966-8848-cc0feafa2d37"], "4e7e96e3-ce71-4afd-a7d5-58e5e44dc7ce": ["8a6654b7-221b-453a-ac99-17e5dd712b6f"], "58a0a2ee-a693-4494-97a6-c6e7874c8c75": ["8a6654b7-221b-453a-ac99-17e5dd712b6f"], "a2baf73b-8409-4f49-b089-0bc567906f60": ["4204f35f-98dd-4508-b0b4-ebab0c7c8f33"], "c3415e0f-3913-4413-8ec5-86caf09b9b58": ["4204f35f-98dd-4508-b0b4-ebab0c7c8f33"], "4f3f76b3-1772-43ec-8f80-4c4300870e3a": ["3d0f790b-1797-4aa3-beac-f6cd44ba4898"], "63ac3a5e-ae8f-4b27-a77b-d7ed080454ff": ["3d0f790b-1797-4aa3-beac-f6cd44ba4898"], "3fd117d1-a7d5-41c6-bca9-7ca566527140": ["449961a6-bf39-4c7c-baff-149d99cb9549"], "39f074ed-9605-4b37-a9e0-ecb02c51dbb8": ["449961a6-bf39-4c7c-baff-149d99cb9549"], "b6dfb9e9-a49f-4dc7-8f9e-2fd1e456d733": ["c379510c-9dbc-42d0-9263-4b16bc2a374d"], "8f636506-005c-48bc-b882-09d722d92484": ["c379510c-9dbc-42d0-9263-4b16bc2a374d"], "6f99fd41-dbdd-4202-ade8-063b7f83c732": ["7775d86c-c343-4e74-808f-28244456986e"], "cfef03c2-6d65-44e6-80dd-541bd25d5a95": ["7775d86c-c343-4e74-808f-28244456986e"], "76527e98-f7dc-42c9-b453-0ba53d6dcde7": ["0cefe1b6-a0f3-4eb6-abcb-439999b00962"], "0ae0b91d-4a1e-4931-a05f-9df21078b2ab": ["0cefe1b6-a0f3-4eb6-abcb-439999b00962"], "b869e4df-483b-45da-8b9f-f196ef8a8d92": ["a5edf3db-6076-4475-818a-2d1f77d528a0"], "8b6724c0-3fb9-45d2-915b-cb583267292f": ["a5edf3db-6076-4475-818a-2d1f77d528a0"], "ac2bd177-fab6-4526-9ca2-888b015b9a96": ["b760ccb4-e8ea-456b-b851-4ba450881099"], "bb6c1dec-4c07-4d2f-879b-b7ba98d40b02": ["b760ccb4-e8ea-456b-b851-4ba450881099"], "9c2a53f0-f6f6-4127-87e2-280e804d5a6b": ["5ed69716-2ef9-4c3e-ad2e-6944a6b2de5a"], "fd2eac99-4b83-43e8-ac70-27b7d85f62d7": ["5ed69716-2ef9-4c3e-ad2e-6944a6b2de5a"], "491d53d0-760a-488f-a60b-906692d5f38d": ["c99a31b8-88fc-4c6a-b194-8dcfd89d0a44"], "75edd594-e0c6-4888-a950-8dd547e3a3e5": ["c99a31b8-88fc-4c6a-b194-8dcfd89d0a44"], "cdbf96b6-1326-437d-9088-a3c6ba667dc6": ["52b58f7e-ada8-4cb4-8703-7deda473a155"], "ab1de749-7071-47a8-9ad0-b3212b6682f0": ["52b58f7e-ada8-4cb4-8703-7deda473a155"], "ce20842a-aeee-40a4-9d50-ddcb7fdfbac6": ["2c1216c9-c47f-43e8-956a-520644a74ef3"], "e3f602e8-3fcb-4118-b6fc-8feb8db3b995": ["2c1216c9-c47f-43e8-956a-520644a74ef3"], "cb6c903e-8212-4dbc-83d3-ffa8a2b6739f": ["717d3c41-4562-40e9-83f7-99c53a4f65ed"], "74b84680-fc51-41d6-90e6-6e2d5bddcabd": ["717d3c41-4562-40e9-83f7-99c53a4f65ed"], "146c98c4-6602-4fa8-afc5-23add5a87f0d": ["f3b812ba-146c-4d00-ae12-e183b088588b"], "861f50dc-637b-4ae1-a1f1-c4c3973c4518": ["f3b812ba-146c-4d00-ae12-e183b088588b"], "88557568-a41a-4e41-b469-bbffa4fe5934": ["7cffdf09-eed0-40e8-b85b-d15725e4c866"], "0a9fd3bb-0c52-4952-a971-b4e645264e3c": ["7cffdf09-eed0-40e8-b85b-d15725e4c866"], "f2677613-f012-4ac0-84dd-333e9ddc3b96": ["626702a6-9a32-4e5a-98dd-9aaa21410ccc"], "08f325f5-9e28-491b-9d48-7373910f5f37": ["626702a6-9a32-4e5a-98dd-9aaa21410ccc"], "d15f3e9e-f883-4359-a015-08d84a6a773e": ["a0110c5f-bbbf-47be-bff9-d339431192d8"], "1c734cd0-0122-4d00-8cbd-4720ebf30120": ["a0110c5f-bbbf-47be-bff9-d339431192d8"], "395c0c0c-9376-4571-a79c-fc962925da35": ["7b4f17aa-fdb5-4748-82db-c09fa44acf51"], "60b9a068-dc49-4463-9b2e-95cd7cc7b9c8": ["7b4f17aa-fdb5-4748-82db-c09fa44acf51"], "dbdde0ad-14dc-4bc3-856e-54d857a3f162": ["b0719b6b-2236-4fc9-a0e9-1f7cccdf5792"], "3e6fc893-ad23-4a6d-b244-db5de7d794c4": ["b0719b6b-2236-4fc9-a0e9-1f7cccdf5792"], "8bfad59a-0498-4be2-9379-68597b323036": ["a2650578-6e24-4412-b436-9de809c63351"], "2d2b2281-85e8-46a6-a161-9e6c89b1353e": ["a2650578-6e24-4412-b436-9de809c63351"], "9b907102-3c1f-4a13-be25-070f61c28623": ["e0057a35-9370-43bd-a0b9-22f30e45dc9b"], "2ef2c2b6-1f34-46da-962c-c8da8a81ce45": ["e0057a35-9370-43bd-a0b9-22f30e45dc9b"], "3e20e422-f6c7-46b2-9850-75358283ce67": ["d629d92a-d422-46de-9249-555ddf9683da"], "b14c1eb9-4012-47eb-bf69-bf38b622ee2b": ["d629d92a-d422-46de-9249-555ddf9683da"], "b2beca45-c8ea-4d22-bbef-c9f4df425d57": ["6087332b-1112-4e02-8446-f01fd5bf86ec"], "ae787237-1a2c-41a9-b414-5f9a5f6a8d81": ["6087332b-1112-4e02-8446-f01fd5bf86ec"], "2466912f-43b0-4cb5-88c2-e98299d71f0d": ["2489697d-9053-482f-84a2-a1d89bfa6f81"], "4549a5fd-9f33-41b8-b56b-95ba0d198bb8": ["2489697d-9053-482f-84a2-a1d89bfa6f81"], "8e905a67-89a2-42e7-866f-fad885936f40": ["283378e4-cef6-4bfb-a632-be8e5ae7189e"], "22241859-2745-452a-adff-c048d64c0644": ["283378e4-cef6-4bfb-a632-be8e5ae7189e"], "ae7e9c8f-c493-4244-8415-cdf5325d57a9": ["d6c7d674-1330-451a-87a9-9a09888c83bd"], "c415b33a-2ae1-40cc-a4c8-b362dba0cce7": ["d6c7d674-1330-451a-87a9-9a09888c83bd"], "767f044c-12d7-4475-9893-bda3ea36063a": ["a13d387e-9ee8-45b3-a8a3-7d6ee3696b0f"], "cfc1eed2-9dbe-4143-91a5-4bc80ebf170b": ["a13d387e-9ee8-45b3-a8a3-7d6ee3696b0f"], "d904dc21-9a76-481a-8d4f-d3635fda72d4": ["b7e4c49d-2cba-48a0-9f0f-f7ca461723e2"], "21b9b95b-0b01-436f-83c4-ad58be7f0e18": ["b7e4c49d-2cba-48a0-9f0f-f7ca461723e2"], "14246adb-f011-4bf4-a251-e0c3fed1495f": ["86a6b7d2-3d05-42ee-b044-111013d2585f"], "cd0d0a8d-6739-432c-81a5-5d754d2606a8": ["86a6b7d2-3d05-42ee-b044-111013d2585f"], "7b0e30a4-cc1e-4028-a24c-e75fe0abaf28": ["c3d4b574-885a-4c0b-9256-9ec82a72a7f9"], "71269d6c-2732-437a-8d73-bffc1da514ea": ["c3d4b574-885a-4c0b-9256-9ec82a72a7f9"], "3693d18e-c5f3-4974-878e-2cb9f00aac5a": ["25166c1a-9f42-441b-8a55-62f5a027ff9a"], "f8abd077-a7a4-40c0-a1d1-c57574a9b763": ["25166c1a-9f42-441b-8a55-62f5a027ff9a"], "4bfef4c4-3cb9-49b1-9387-a7288e4f872d": ["d6cff776-43c6-4cd1-8ac1-e9767d4c92f6"], "5e05291f-62d0-498c-a514-775e4ddaf733": ["d6cff776-43c6-4cd1-8ac1-e9767d4c92f6"], "d69f8a61-23f8-4c8c-ba63-7afecca60a33": ["61833493-95e3-444c-8824-33af09d23751"], "b3fcec27-4737-4546-966a-bf6a6468552d": ["61833493-95e3-444c-8824-33af09d23751"], "7730126b-dce7-4494-a8b0-a506864e96d4": ["fe339b6e-df99-458e-b3dc-9e0831f88d2e"], "1ba114be-8de0-4a1d-8553-c73024e14846": ["fe339b6e-df99-458e-b3dc-9e0831f88d2e"], "11ed4743-1d71-484e-8ca7-21f8e2d845df": ["8d28e5eb-06a0-4f34-8900-87da71236434"], "e6bb11f1-4c37-4165-8a9d-d7d3e7d335ec": ["8d28e5eb-06a0-4f34-8900-87da71236434"], "bc0328a0-4d98-4afb-acc8-834c67ee0cb0": ["31e88228-f2c5-4fb8-9e69-4fe4693c4915"], "6fc2afea-e3e3-4e4e-a0ef-84ec0bfd5575": ["31e88228-f2c5-4fb8-9e69-4fe4693c4915"], "61426d26-118f-40d6-8061-f042005b7c1f": ["5189c194-6890-4fe8-8d79-05849fb8f99d"], "47b97e20-407a-4de0-9383-63014affaf56": ["5189c194-6890-4fe8-8d79-05849fb8f99d"], "b8476e01-8814-420b-bf20-97b81161f224": ["50ad7d3d-22a8-404d-8505-90f73aee2073"], "e6bc7cd7-18df-4504-b84f-62023be0aa56": ["50ad7d3d-22a8-404d-8505-90f73aee2073"], "e1b98422-109e-4282-9b66-ef7aa422ad48": ["0584125c-261e-4f68-bc6a-8f7dbdb48f30"], "d87843f9-df40-4159-80ac-7c81f34e647a": ["0584125c-261e-4f68-bc6a-8f7dbdb48f30"], "9c5fd71e-253a-4cc0-842d-de8d1593a36e": ["269fa491-ab38-4a83-8630-e92d6754008d"], "a7133dfb-1495-41d6-8c1a-c7af69b1469a": ["269fa491-ab38-4a83-8630-e92d6754008d"], "d0f002aa-8482-4046-8ab9-a62ea7bbc170": ["24bc7db7-96e9-4bb4-9858-b34090f6bd77"], "fcda0206-7bfa-4b9c-8e9a-c37acab951e8": ["24bc7db7-96e9-4bb4-9858-b34090f6bd77"], "e62cc9d7-c4e5-4137-8465-417609eb14fd": ["97a49933-d6c4-4031-99c7-23828939f326"], "14bbbd89-3c39-482d-bc68-b324f2822f96": ["97a49933-d6c4-4031-99c7-23828939f326"], "a1d62dde-5af6-42aa-ad8f-d4e9f9c53484": ["bfe0b27a-754a-4543-a665-86c6860254f0"], "a6af5723-dbf7-4ed6-8bad-1f15c24ffb13": ["bfe0b27a-754a-4543-a665-86c6860254f0"]}, "corpus": {"5b8af9b5-6401-47b2-a0b0-cab411946bc0": "45 \nMG-4.1-007 \nVerify that AI Actors responsible for monitoring reported issues can e\ufb00ectively \nevaluate GAI system performance including the application of content \nprovenance data tracking techniques, and promptly escalate issues for response. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring \n \nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \nengagement with interested parties, including relevant AI Actors. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the \nperformance, feedback received, and improvements made. \nHarmful Bias and Homogenization \nMG-4.2-002 \nPractice and follow incident response plans for addressing the generation of", "eee77049-8a28-4d67-9e7b-9a0645afd06f": "them\n10. Samantha Cole. This Horrifying App Undresses a Photo of Any Woman With a Single Click. Motherboard.\nJune 26, 2019. https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman\n11. Lauren Kaori Gurley. Amazon\u2019s AI Cameras Are Punishing Drivers for Mistakes They Didn\u2019t Make.\nMotherboard. Sep. 20, 2021. https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing\u00ad\ndrivers-for-mistakes-they-didnt-make\n63", "67abcb37-f776-4732-8a16-931c57acf4c9": "manage risks associated with activities or business processes common across sectors, such as the use of \nlarge language models (LLMs), cloud-based services, or acquisition. \nThis document de\ufb01nes risks that are novel to or exacerbated by the use of GAI. After introducing and \ndescribing these risks, the document provides a set of suggested actions to help organizations govern, \nmap, measure, and manage these risks. \n \n \n1 EO 14110 de\ufb01nes Generative AI as \u201cthe class of AI models that emulate the structure and characteristics of input \ndata in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital \ncontent.\u201d While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers \nto generative foundation models. The foundation model subcategory of \u201cdual-use foundation models\u201d is de\ufb01ned by", "452b57dc-fb7c-4a90-bf60-179fd88a45e7": "DATA PRIVACY \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \u00ad\u00ad\u00ad\u00ad\u00ad\u00ad\nIn addition to the privacy expectations above for general non-sensitive data, any system collecting, using, shar-\ning, or storing sensitive data should meet the expectations below. Depending on the technological use case and \nbased on an ethical assessment, consent for sensitive data may need to be acquired from a guardian and/or child. \nProvide enhanced protections for data related to sensitive domains \nNecessary functions only. Sensitive data should only be used for functions strictly necessary for that \ndomain or for functions that are required for administrative reasons (e.g., school attendance records), unless", "b1fd4721-6069-47b9-b282-954dc06a4d12": "and type of requests for consideration, fallback employed, and any repeated requests; the timeliness of the \nhandling of these requests, including mean wait times for different types of requests as well as maximum wait \ntimes; and information about the procedures used to address requests for consideration along with the results \nof the evaluation of their accessibility. For systems used in sensitive domains, reporting should include infor\u00ad\nmation about training and governance procedures for these technologies. Reporting should also include docu\u00ad\nmentation of goals and assessment of meeting those goals, consideration of data included, and documentation \nof the governance of reasonable access to the technology. Reporting should be provided in a clear and \nmachine-readable manner. \n51", "1e0b6329-5019-4458-b61d-e70808fee6a7": "house, revealing to her father that she was pregnant.71\n\u2022\nSchool audio surveillance systems monitor student conversations to detect potential \"stress indicators\" as\na warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an\nexam using biometric markers.73 These systems have the potential to limit student freedom to express a range\nof emotions at school and may inappropriately flag students with disabilities who need accommodations or\nuse screen readers or dictation software as cheating.74\n\u2022\nLocation data, acquired from a data broker, can be used to identify people who visit abortion clinics.75\n\u2022\nCompanies collect student data such as demographic information, free or reduced lunch status, whether\nthey've used drugs, or whether they've expressed interest in LGBTQI+ groups, and then use that data to \nforecast student success.76 Parents and education experts have expressed concern about collection of such", "e6699665-8b3c-4923-9ff5-b2bda5d2fa93": "information in their credit report.\" The CFPB has also asserted that \"[t]he law gives every applicant the right to \na specific explanation if their application for credit was denied, and that right is not diminished simply because \na company uses a complex algorithm that it doesn't understand.\"92 Such explanations illustrate a shared value \nthat certain decisions need to be explained. \nA California law requires that warehouse employees are provided with notice and explana-\ntion about quotas, potentially facilitated by automated systems, that apply to them. Warehous-\ning employers in California that use quota systems (often facilitated by algorithmic monitoring systems) are \nrequired to provide employees with a written description of each quota that applies to the employee, including \n\u201cquantified number of tasks to be performed or materials to be produced or handled, within the defined", "cb8782b0-a673-48a5-9c4e-71fb779cdee0": "during adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \nincluded in their training data. This problem has been referred to as data memorization, and may pose \nexacerbated privacy risks even for data present only in a small number of training samples.  \nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly \ninfer PII or sensitive data that was not in their training data nor disclosed by the user by stitching \ntogether information from disparate sources. These inferences can have negative impact on an individual \neven if the inferences are not accurate (e.g., confabulations), and especially if they reveal information \nthat the individual considers sensitive or that is used to disadvantage or harm them. \nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate", "13591cf1-6123-49bc-b358-d1fccdce870d": "Action ID \nSuggested Action \nGAI Risks \nMS-3.3-001 \nConduct impact assessments on how AI-generated content might a\ufb00ect \ndi\ufb00erent social, economic, and cultural groups. \nHarmful Bias and Homogenization \nMS-3.3-002 \nConduct studies to understand how end users perceive and interact with GAI \ncontent and accompanying content provenance within context of use. Assess \nwhether the content aligns with their expectations and how they may act upon \nthe information presented. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMS-3.3-003 \nEvaluate potential biases and stereotypes that could emerge from the AI-\ngenerated content using appropriate methodologies including computational \ntesting methods as well as evaluating structured feedback input. \nHarmful Bias and Homogenization", "ef90770a-0a05-4e41-a4e1-8e7ce75a2d5d": "indirect-disclosure/ \nQu, Y. et al. (2023) Unsafe Di\ufb00usion: On the Generation of Unsafe Images and Hateful Memes From Text-\nTo-Image Models. arXiv. https://arxiv.org/pdf/2305.13873 \nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model \ncompression. PLOS One. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668 \nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes \nin Victimization and Perpetration. Sage. \nhttps://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834 \nSandbrink, J. (2023) Arti\ufb01cial intelligence and biological misuse: Di\ufb00erentiating risks of language models \nand biological design tools. arXiv. https://arxiv.org/pdf/2306.13952", "c558c0ca-d2f5-4315-b6cb-4af590181d1a": "https://arxiv.org/abs/2304.02819 \nLuccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \nhttps://arxiv.org/pdf/2311.16863 \nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND. \nhttps://www.rand.org/pubs/research_reports/RRA2977-2.html. \nNicoletti, L. et al. (2023) Humans Are Biased. Generative Ai Is Even Worse. Bloomberg. \nhttps://www.bloomberg.com/graphics/2023-generative-ai-bias/. \nNational Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and \nTerminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/\ufb01nal \nNational Institute of Standards and Technology (2023) AI Risk Management Framework. \nhttps://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \nRisks and Trustworthiness.", "3e8a526b-75ee-49f4-b078-280239ba1a8c": "systems should be evaluated, protected against, and redressed at both the individual and community levels. \nEQUITY: \u201cEquity\u201d means the consistent and systematic fair, just, and impartial treatment of all individuals. \nSystemic, fair, and just treatment must take into account the status of individuals who belong to underserved \ncommunities that have been denied such treatment, such as Black, Latino, and Indigenous and Native American \npersons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; \nwomen, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) \npersons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely \naffected by persistent poverty or inequality. \nRIGHTS, OPPORTUNITIES, OR ACCESS: \u201cRights, opportunities, or access\u201d is used to indicate the scoping", "bab4e82a-ed21-461b-881a-88c3be7ab6a4": "before it occurs and be informed about how the data gathered through surveillance will be used. \nScope limits on surveillance to protect rights and democratic values. Civil liberties and civil \nrights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated \nsystem. Surveillance systems should not be used to monitor the exercise of democratic rights, such as voting, \nprivacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liber\u00ad\nties. Information about or algorithmically-determined assumptions related to identity should be carefully \nlimited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such iden\u00ad\ntity-related information includes group characteristics or affiliations, geographic designations, location-based \nand association-based inferences, social networks, and biometrics. Continuous surveillance and monitoring", "d3e800c3-7689-4e70-be5f-b31ae9e74668": "Technology \nCenter for New Democratic \nProcesses \nCenter for Research and Education \non Accessible Technology and \nExperiences at University of \nWashington, Devva Kasnitz, L Jean \nCamp, Jonathan Lazar, Harry \nHochheiser \nCenter on Privacy & Technology at \nGeorgetown Law \nCisco Systems \nCity of Portland Smart City PDX \nProgram \nCLEAR \nClearview AI \nCognoa \nColor of Change \nCommon Sense Media \nComputing Community Consortium \nat Computing Research Association \nConnected Health Initiative \nConsumer Technology Association \nCourtney Radsch \nCoworker \nCyber Farm Labs \nData & Society Research Institute \nData for Black Lives \nData to Actionable Knowledge Lab \nat Harvard University \nDeloitte \nDev Technology Group \nDigital Therapeutics Alliance \nDigital Welfare State & Human \nRights Project and Center for \nHuman Rights and Global Justice at \nNew York University School of \nLaw, and Temple University \nInstitute for Law, Innovation & \nTechnology \nDignari \nDouglas Goddard \nEdgar Dworsky", "64710ba6-2577-435e-831f-dc8c1124943e": "HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \nThere are many reasons people may prefer not to use an automated system: the system can be flawed and can lead to \nunintended outcomes; it may reinforce bias or be inaccessible; it may simply be inconvenient or unavailable; or it may \nreplace a paper or manual process to which people had grown accustomed. Yet members of the public are often \npresented with no alternative, or are forced to endure a cumbersome process to reach a human decision-maker once \nthey decide they no longer want to deal exclusively with the automated system or be impacted by its results. As a result \nof this lack of human reconsideration, many receive delayed access, or lose access, to rights, opportunities, benefits,", "d1f538c7-f835-43b4-aeef-5d3e578f7ad2": "system. \nA.1.8. Incident Disclosure \nOverview \nAI incidents can be de\ufb01ned as an \u201cevent, circumstance, or series of events where the development, use, \nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \nmental health); disruption of the management and operation of critical infrastructure; violations of \nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \nand intellectual property rights; or harm to property, communities, or the environment.\u201d AI incidents can \noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual). \nState of AI Incident Tracking and Disclosure \nFormal channels do not currently exist to report and document AI incidents. However, a number of", "ae2c2380-14b2-415a-8635-7a7e7a8b1a95": "This important progress must not come at the price of civil rights or democratic values, foundational American \nprinciples that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the \nPresident ordered the full Federal government to work to root out inequity, embed fairness in decision-\nmaking processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.1 The \nPresident has spoken forcefully about the urgent challenges posed to democracy today and has regularly called \non people of conscience to act to preserve civil rights\u2014including the right to privacy, which he has called \u201cthe \nbasis for so many more rights that we have come to take for granted that are ingrained in the fabric of this \ncountry.\u201d2\nTo advance President Biden\u2019s vision, the White House Office of Science and Technology Policy has identified", "e21b0e3e-df6d-43bf-ae63-76120d3d8154": "You should be able to opt out, where appropriate, and \nhave access to a person who can quickly consider and \nremedy problems you encounter. You should be able to opt \nout from automated systems in favor of a human alternative, where \nappropriate. Appropriateness should be determined based on rea\u00ad\nsonable expectations in a given context and with a focus on ensuring \nbroad accessibility and protecting the public from especially harm\u00ad\nful impacts. In some cases, a human or other alternative may be re\u00ad\nquired by law. You should have access to timely human consider\u00ad\nation and remedy by a fallback and escalation process if an automat\u00ad\ned system fails, it produces an error, or you would like to appeal or \ncontest its impacts on you. Human consideration and fallback \nshould be accessible, equitable, effective, maintained, accompanied \nby appropriate operator training, and should not impose an unrea\u00ad\nsonable burden on the public. Automated systems with an intended", "d16f1fbf-2d7f-4bc5-a574-381e441ee615": "consumers\u2019 permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to \nmake decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillance \ntechnologies has increased in schools and workplaces, and, when coupled with consequential management and \nevaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, and \na reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated by \ndata brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive, \nbreeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; and \nthreatening our democratic process.63 The American public should be protected from these growing risks.", "18893892-bf57-422b-84ef-785183d35218": "DATA PRIVACY \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nProtect the public from unchecked surveillance \nHeightened oversight of surveillance. Surveillance or monitoring systems should be subject to \nheightened oversight that includes at a minimum assessment of potential harms during design (before deploy\u00ad\nment) and in an ongoing manner, to ensure that the American public\u2019s rights, opportunities, and access are \nprotected. This assessment should be done before deployment and should give special attention to ensure \nthere is not algorithmic discrimination, especially based on community membership, when deployed in a \nspecific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as the \nsystem is in use.", "a9e8e9ec-ffee-4d4c-bf18-60e69fc3f6ef": "SECTION TITLE\n \n \n \n \n \n \nApplying The Blueprint for an AI Bill of Rights \nRELATIONSHIP TO EXISTING LAW AND POLICY\nThere are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-spe\u00ad\ncific privacy and security protections. Ensuring some of the additional protections proposed in this framework \nwould require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions to \nthe principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, \nconform to the practicalities of a specific use case, or balance competing public interests. In particular, law \nenforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, \nand privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in", "1578fb61-ab39-407a-9a7f-775e21a729e4": "26 \nMAP 4.1: Approaches for mapping AI technology and legal risks of its components \u2013 including the use of third-party data or \nsoftware \u2013 are in place, followed, and documented, as are risks of infringement of a third-party\u2019s intellectual property or other \nrights. \nAction ID \nSuggested Action \nGAI Risks \nMP-4.1-001 Conduct periodic monitoring of AI-generated content for privacy risks; address any \npossible instances of PII or sensitive data exposure. \nData Privacy \nMP-4.1-002 Implement processes for responding to potential intellectual property infringement \nclaims or other rights. \nIntellectual Property \nMP-4.1-003 \nConnect new GAI policies, procedures, and processes to existing model, data, \nsoftware development, and IT governance and to legal, compliance, and risk \nmanagement activities. \nInformation Security; Data Privacy \nMP-4.1-004 Document training data curation policies, to the extent possible and according to \napplicable laws and policies.", "d5a52529-2620-451d-8910-3ec4463f63a0": "WHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \u00ad\u00ad\u00ad\n\u2022\nAn automated sentiment analyzer, a tool often used by technology platforms to determine whether a state-\nment posted online expresses a positive or negative sentiment, was found to be biased against Jews and gay\npeople. For example, the analyzer marked the statement \u201cI\u2019m a Jew\u201d as representing a negative sentiment,\nwhile \u201cI\u2019m a Christian\u201d was identified as expressing a positive sentiment.36 This could lead to the\npreemptive blocking of social media comments such as: \u201cI\u2019m gay.\u201d A related company with this bias concern\nhas made their data public to encourage researchers to help address the issue37 and has released reports\nidentifying and measuring this problem as well as detailing attempts to address it.38\n\u2022", "89d4b864-3fae-4dd1-b64b-42dea70146e1": "SECTION TITLE\nDATA PRIVACY\nYou should be protected from abusive data practices via built-in protections and you \nshould have agency over how data about you is used. You should be protected from violations of \nprivacy through design choices that ensure such protections are included by default, including ensuring that \ndata collection conforms to reasonable expectations and that only data strictly necessary for the specific \ncontext is collected. Designers, developers, and deployers of automated systems should seek your permission \nand respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate \nways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be \nused. Systems should not employ user experience and design decisions that obfuscate user choice or burden \nusers with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases", "cbda89d4-e094-472c-9fb0-2e2ccd29052d": "relevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \ncould facilitate its analysis or synthesis, particularly by individuals without formal scienti\ufb01c training or \nexpertise.  \nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \nbiological agents will continue to require both applicable expertise and supporting materials and \ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \ncan help actors address those barriers.", "8b40e1a8-eceb-4ea8-bad3-8f1eb8937a5c": "Wei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \nhttps://arxiv.org/pdf/2403.18802 \nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \nhttps://arxiv.org/pdf/2112.04359 \nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \nhttps://arxiv.org/pdf/2310.11986 \nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT \u201922. \nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \nanalyses. arXiv. https://arxiv.org/pdf/2402.02008 \nYin, L. et al. (2024) OpenAI\u2019s GPT Is A Recruiter\u2019s Dream Tool. Tests Show There\u2019s Racial Bias. Bloomberg. \nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/", "46087c7d-6427-4c9b-967b-665e401ccb03": "impacts. \nHuman-AI Con\ufb01guration; Value \nChain and Component Integration \nAI Actor Tasks: AI Deployment, AI Design, AI Impact Assessment, A\ufb00ected Individuals and Communities, Domain Experts, End-\nUsers, Human Factors, Operation and Monitoring  \n \nMEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for \nimplementation starting with the most signi\ufb01cant AI risks. The risks or trustworthiness characteristics that will not \u2013 or cannot \u2013 be \nmeasured are properly documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-1.1-001 Employ methods to trace the origin and modi\ufb01cations of digital content. \nInformation Integrity \nMS-1.1-002 \nIntegrate tools designed to analyze content provenance and detect data \nanomalies, verify the authenticity of digital signatures, and identify patterns \nassociated with misinformation or manipulation. \nInformation Integrity \nMS-1.1-003", "b108da9a-d2dc-46be-9672-9c71f39f1c68": "SECTION TITLE\u00ad\nFOREWORD\nAmong the great challenges posed to democracy today is the use of technology, data, and automated systems in \nways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities and \nprevent our access to critical resources or services. These problems are well documented. In America and around \nthe world, systems supposed to help with patient care have proven unsafe, ineffective, or biased. Algorithms used \nin hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed \nnew harmful bias and discrimination. Unchecked social media data collection has been used to threaten people\u2019s \nopportunities, undermine their privacy, or pervasively track their activity\u2014often without their knowledge or \nconsent. \nThese outcomes are deeply harmful\u2014but they are not inevitable. Automated systems have brought about extraor-", "b7cd6e58-9b28-479c-ae84-85fbf220a0f4": "the community, both those living in the housing complex and not, to have videos of them sent to the local\npolice department and made available for scanning by its facial recognition software.66\n\u2022\nCompanies use surveillance software to track employee discussions about union activity and use the\nresulting data to surveil individual employees and surreptitiously intervene in discussions.67\n32", "6d5a2af7-141f-481f-9bd6-be9ba1c32357": "data, internet of things (IOT) or AI systems; Use of open-source data or models; \nUsers\u2019 emotional entanglement with GAI functions. \nHuman-AI Con\ufb01guration; \nInformation Security; Value Chain \nand Component Integration \nAI Actor Tasks: AI Deployment, Operation and Monitoring \n \nGOVERN 2.1: Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are \ndocumented and are clear to individuals and teams throughout the organization. \nAction ID \nSuggested Action \nGAI Risks \nGV-2.1-001 \nEstablish organizational roles, policies, and procedures for communicating GAI \nincidents and performance to AI Actors and downstream stakeholders (including \nthose potentially impacted), via community or o\ufb03cial resources (e.g., AI incident \ndatabase, AVID, CVE, NVD, or OECD AI incident monitor). \nHuman-AI Con\ufb01guration; Value \nChain and Component Integration \nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with", "779898f7-f5b2-47cd-9a8e-7ab1617f17e6": "ABOUT THIS FRAMEWORK\u00ad\u00ad\u00ad\u00ad\u00ad\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the \ndesign, use, and deployment of automated systems to protect the rights of the American public in the age of \nartificial intel-ligence. Developed through extensive consultation with the American public, these principles are \na blueprint for building and deploying automated systems that are aligned with democratic values and protect \ncivil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five \nprinciples, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives \nconcrete steps that can be taken by many kinds of organizations\u2014from governments at all levels to companies of \nall sizes\u2014to uphold these values. Experts from across the private sector, governments, and international", "8b6e2467-5eb6-472c-a5c3-fccfc0a11841": "the Privacy Act, federal agencies may only retain data about an individual that is \u201crelevant and necessary\u201d to \naccomplish an agency\u2019s statutory purpose or to comply with an Executive Order of the President. The law allows \nfor individuals to be able to access any of their individual information stored in a federal system of records, if not \nincluded under one of the systems of records exempted pursuant to the Privacy Act. In these cases, federal agen\u00ad\ncies must provide a method for an individual to determine if their personal information is stored in a particular \nsystem of records, and must provide procedures for an individual to contest the contents of a record about them. \nFurther, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does not \ncomply with the Privacy Act\u2019s requirements. Among other things, a court may order a federal agency to amend or", "471f0922-9410-4ce7-a715-27c8569b4102": "experiences and perspectives to the task of AI red-teaming. These individuals may have been \nprovided instructions and material to complete tasks which may elicit harmful model behaviors. \nThis type of exercise can be more e\ufb00ective with large groups of AI red-teamers. \n\u2022 \nExpert: Performed by specialists with expertise in the domain or speci\ufb01c AI red-teaming context \nof use (e.g., medicine, biotech, cybersecurity).  \n\u2022 \nCombination: In scenarios when it is di\ufb03cult to identify and recruit specialists with su\ufb03cient \ndomain and contextual expertise, AI red-teaming exercises may leverage both expert and", "9f8e960c-28d4-467b-af64-2d2c12ba2650": "preemptive blocking of social media comments such as: \u201cI\u2019m gay.\u201d A related company with this bias concern\nhas made their data public to encourage researchers to help address the issue37 and has released reports\nidentifying and measuring this problem as well as detailing attempts to address it.38\n\u2022\nSearches for \u201cBlack girls,\u201d \u201cAsian girls,\u201d or \u201cLatina girls\u201d return predominantly39 sexualized content, rather\nthan role models, toys, or activities.40 Some search engines have been working to reduce the prevalence of\nthese results, but the problem remains.41\n\u2022\nAdvertisement delivery systems that predict who is most likely to click on a job advertisement end up deliv-\nering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermar-\nket cashier ads to women and jobs with taxi companies to primarily Black people.42\u00ad\n\u2022\nBody scanners, used by TSA at airport checkpoints, require the operator to select a \u201cmale\u201d or \u201cfemale\u201d", "90fad26b-8ade-46d5-a1b6-8b82befa2d31": "opportunities, undermine their privacy, or pervasively track their activity\u2014often without their knowledge or \nconsent. \nThese outcomes are deeply harmful\u2014but they are not inevitable. Automated systems have brought about extraor-\ndinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm \npaths, to algorithms that can identify diseases in patients. These tools now drive important decisions across \nsectors, while data is helping to revolutionize global industries. Fueled by the power of American innovation, \nthese tools hold the potential to redefine every part of our society and make life better for everyone. \nThis important progress must not come at the price of civil rights or democratic values, foundational American \nprinciples that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the", "e2f125a7-80b6-4c8e-8e8c-c7c947a9f26d": "sources; demographic group and subgroup coverage in GAI system training \ndata; Forms of latent systemic bias in images, text, audio, embeddings, or other \ncomplex or unstructured data; Input data features that may serve as proxies for \ndemographic group membership (i.e., image metadata, language dialect) or \notherwise give rise to emergent bias within GAI systems; The extent to which \nthe digital divide may negatively impact representativeness in GAI system \ntraining and TEVV data; Filtering of hate speech or content in GAI system \ntraining data; Prevalence of GAI-generated data in GAI system training data. \nHarmful Bias and Homogenization \n \n \n15 Winogender Schemas is a sample set of paired sentences which di\ufb00er only by gender of the pronouns used, \nwhich can be used to evaluate gender bias in natural language processing coreference resolution systems.", "dba4ff20-578d-471c-b434-2cb8a41fe69c": "5 \noperations, or other cyberattacks; increased attack surface for targeted cyberattacks, which may \ncompromise a system\u2019s availability or the con\ufb01dentiality or integrity of training data, code, or \nmodel weights.  \n10. Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or \nlicensed content without authorization (possibly in situations which do not fall under fair use); \neased exposure of trade secrets; or plagiarism or illegal replication.  \n11. Obscene, Degrading, and/or Abusive Content: Eased production of and access to obscene, \ndegrading, and/or abusive imagery which can cause harm, including synthetic child sexual abuse \nmaterial (CSAM), and nonconsensual intimate images (NCII) of adults. \n12. Value Chain and Component Integration: Non-transparent or untraceable integration of \nupstream third-party components, including data that has been improperly obtained or not", "d08159f3-e8d6-43c1-8608-cb7b2938d0ae": "Integrity \nAI Actor Tasks: Governance and Oversight \n \nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \ncontrols based on organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.4-001 \nEstablish policies and mechanisms to prevent GAI systems from generating \nCSAM, NCII or content that violates the law.  \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization; \nDangerous, Violent, or Hateful \nContent \nGV-1.4-002 \nEstablish transparent acceptable use policies for GAI that address illegal use or \napplications of GAI. \nCBRN Information or \nCapabilities; Obscene, \nDegrading, and/or Abusive \nContent; Data Privacy; Civil \nRights violations \nAI Actor Tasks: AI Development, AI Deployment, Governance and Oversight", "09223ac2-b2c7-484e-8f92-028079d7dc0c": "modeling libraries, tools and APIs, \ufb01ne-tuned models, and embedded tools; \nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \nproviders against incident or vulnerability databases. \nData Privacy; Human-AI \nCon\ufb01guration; Information \nSecurity; Intellectual Property; \nValue Chain and Component \nIntegration; Harmful Bias and \nHomogenization \nGV-6.1-010 \nUpdate GAI acceptable use policies to address proprietary and open-source GAI \ntechnologies and data, and contractors, consultants, and other third-party \npersonnel. \nIntellectual Property; Value Chain \nand Component Integration \nAI Actor Tasks: Operation and Monitoring, Procurement, Third-party entities \n \nGOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be \nhigh-risk. \nAction ID \nSuggested Action \nGAI Risks \nGV-6.2-001 \nDocument GAI risks associated with system value chain to identify over-reliance", "2979d054-7a34-4262-9fdd-c4ab4f3542ec": "Enhanced  \n2.11. \nObscene, Degrading, and/or Abusive Content \nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, \nand/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can \ncreate privacy, psychological and emotional, and even physical harms, and in some cases may be illegal.  \nGenerated explicit or obscene AI content may include highly realistic \u201cdeepfakes\u201d of real individuals, \nincluding children. The spread of this kind of material can have downstream negative consequences: in \nthe context of CSAM, even if the generated images do not resemble speci\ufb01c individuals, the prevalence \nof such images can divert time and resources from e\ufb00orts to \ufb01nd real-world victims. Outside of CSAM, \nthe creation and spread of NCII disproportionately impacts women and sexual minorities, and can have \nsubsequent negative consequences including decline in overall mental health, substance abuse, and", "2bd97c55-3050-47fd-b13e-a6211d21dfeb": "its potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \nsensitivity and broad heterogeneity of contexts of use. \nA.1.5. Structured Public Feedback \nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \nbut are not limited to: \n\u2022 \nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \na\ufb00ected communities, and users, including focus groups, small user studies, and surveys. \n\u2022 \nField Testing: Methods used to determine how people interact with, consume, use, and make \nsense of AI-generated information, and subsequent actions and e\ufb00ects, including UX, usability, \nand other structured, randomized experiments.  \n\u2022", "d4d4a8c1-5fa2-458e-bd39-21f04c221cab": "12 \nCSAM. Even when trained on \u201cclean\u201d data, increasingly capable GAI models can synthesize or produce \nsynthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII \nhave moved from niche internet forums to mainstream, automated, and scaled online businesses.  \nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Privacy Enhanced \n2.12. \nValue Chain and Component Integration \nGAI value chains involve many third-party components such as procured datasets, pre-trained models, \nand software libraries. These components might be improperly obtained or not properly vetted, leading \nto diminished transparency or accountability for downstream users. While this is a risk for traditional AI \nsystems and some other digital technologies, the risk is exacerbated for GAI due to the scale of the \ntraining data, which may be too large for humans to vet; the di\ufb03culty of training foundation models,", "8bc1eeee-7f57-4ef5-a85f-d48dfa4d9817": "22 \nGV-6.2-003 \nEstablish incident response plans for third-party GAI technologies: Align incident \nresponse plans with impacts enumerated in MAP 5.1; Communicate third-party \nGAI incident response plans to all relevant AI Actors; De\ufb01ne ownership of GAI \nincident response functions; Rehearse third-party GAI incident response plans at \na regular cadence; Improve incident response plans based on retrospective \nlearning; Review incident response plans for alignment with relevant breach \nreporting, data protection, data privacy, or other laws. \nData Privacy; Human-AI \nCon\ufb01guration; Information \nSecurity; Value Chain and \nComponent Integration; Harmful \nBias and Homogenization \nGV-6.2-004 \nEstablish policies and procedures for continuous monitoring of third-party GAI \nsystems in deployment. \nValue Chain and Component \nIntegration \nGV-6.2-005 \nEstablish policies and procedures that address GAI data redundancy, including \nmodel weights and other system artifacts.", "eb059f63-f688-4142-b0f5-d1e962d6ce03": "technologies in measuring and managing identi\ufb01ed risks. \nInformation Integrity; Harmful Bias \nand Homogenization \nMG-4.1-002 \nEstablish, maintain, and evaluate e\ufb00ectiveness of organizational processes and \nprocedures for post-deployment monitoring of GAI systems, particularly for \npotential confabulation, CBRN, or cyber risks. \nCBRN Information or Capabilities; \nConfabulation; Information \nSecurity \nMG-4.1-003 \nEvaluate the use of sentiment analysis to gauge user sentiment regarding GAI \ncontent performance and impact, and work in collaboration with AI Actors \nexperienced in user research and experience. \nHuman-AI Con\ufb01guration \nMG-4.1-004 Implement active learning techniques to identify instances where the model fails \nor produces unexpected outputs. \nConfabulation \nMG-4.1-005 \nShare transparency reports with internal and external stakeholders that detail \nsteps taken to update the GAI system to enhance transparency and \naccountability. \nHuman-AI Con\ufb01guration; Harmful", "b1d30e39-7d73-45dd-bc8d-fbd7392539cf": "correcting data. Entities should conduct regular, independent audits and take prompt corrective measures to \nmaintain accurate, timely, and complete data. \nLimit access to sensitive data and derived data. Sensitive data and derived data should not be sold, \nshared, or made public as part of data brokerage or other agreements. Sensitive data includes data that can be \nused to infer sensitive information; even systems that are not directly marketed as sensitive domain technologies \nare expected to keep sensitive data private. Access to such data should be limited based on necessity and based \non a principle of local control, such that those individuals closest to the data subject have more access while \nthose who are less proximate do not (e.g., a teacher has access to their students\u2019 daily progress data while a \nsuperintendent does not). \nReporting. In addition to the reporting on data privacy (as listed above for non-sensitive data), entities devel-", "774f3535-e907-42f1-87fa-bb14b355780d": "Wang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \nhttps://aclanthology.org/2023.\ufb01ndings-emnlp.607.pdf \nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \nhttps://arxiv.org/pdf/2308.13387 \nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\nframework-for-researc/168076277c \nWeatherbed, J. (2024) Trolls have \ufb02ooded X with graphic Taylor Swift AI fakes. The Verge. \nhttps://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \nhttps://arxiv.org/pdf/2403.18802 \nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \nhttps://arxiv.org/pdf/2112.04359", "c575e182-a0e8-455e-b0f9-33b348927d2a": "Information Security \n \nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 3.1: AI risks and bene\ufb01ts from third-party resources are regularly monitored, and risk controls are applied and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMG-3.1-001 \nApply organizational risk tolerances and controls (e.g., acquisition and \nprocurement processes; assessing personnel credentials and quali\ufb01cations, \nperforming background checks; \ufb01ltering GAI input and outputs, grounding, \ufb01ne \ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \norganizational risk tolerance to the utilization of third-party datasets and other \nGAI resources; Apply organizational risk tolerances to \ufb01ne-tuned third-party \nmodels; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after \ufb01ne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property", "1c7305a9-3c6d-4165-a965-cc499d1b0ce8": "guard the American public against many of the potential and actual harms identified by researchers, technolo\u00ad\ngists, advocates, journalists, policymakers, and communities in the United States and around the world. This \ntechnical companion is intended to be used as a reference by people across many circumstances \u2013 anyone \nimpacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to \ngovern the use of an automated system. \nEach principle is accompanied by three supplemental sections: \n1\n2\nWHY THIS PRINCIPLE IS IMPORTANT: \nThis section provides a brief summary of the problems that the principle seeks to address and protect against, including \nillustrative examples. \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS: \n\u2022 The expectations for automated systems are meant to serve as a blueprint for the development of additional technical\nstandards and practices that should be tailored for particular sectors and contexts.", "726a595b-bb8d-4e05-9019-19a146ca2e20": "DATA PRIVACY \nEXTRA PROTECTIONS FOR DATA RELATED TO SENSITIVE\nDOMAINS\nSome domains, including health, employment, education, criminal justice, and personal finance, have long been \nsingled out as sensitive domains deserving of enhanced data protections. This is due to the intimate nature of these \ndomains as well as the inability of individuals to opt out of these domains in any meaningful way, and the \nhistorical discrimination that has often accompanied data knowledge.69 Domains understood by the public to be \nsensitive also change over time, including because of technological developments. Tracking and monitoring \ntechnologies, personal tracking devices, and our extensive data footprints are used and misused more than ever \nbefore; as such, the protections afforded by current legal guidelines may be inadequate. The American public \ndeserves assurances that data related to such sensitive domains is protected and used appropriately and only in", "d8541ea5-d276-4f04-b5da-60c79f4c4808": "Suggested Action \nGAI Risks \nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the \nperformance, feedback received, and improvements made. \nHarmful Bias and Homogenization \nMG-4.2-002 \nPractice and follow incident response plans for addressing the generation of \ninappropriate or harmful content and adapt processes based on \ufb01ndings to \nprevent future occurrences. Conduct post-mortem analyses of incidents with \nrelevant AI Actors, to understand the root causes and implement preventive \nmeasures. \nHuman-AI Con\ufb01guration; \nDangerous, Violent, or Hateful \nContent \nMG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease \nnon-technical stakeholders understanding of GAI system functionality. \nHuman-AI Con\ufb01guration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, A\ufb00ected Individuals and Communities, End-Users, Operation and \nMonitoring, TEVV", "53662a07-d6f4-4e76-a2f0-1aef0e349d1f": "62. See, e.g., Federal Trade Commission. Data Brokers: A Call for Transparency and Accountability. May\n2014.\nhttps://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability\u00ad\nreport-federal-trade-commission-may-2014/140527databrokerreport.pdf; Cathy O\u2019Neil.\nWeapons of Math Destruction. Penguin Books. 2017.\nhttps://en.wikipedia.org/wiki/Weapons_of_Math_Destruction\n63. See, e.g., Rachel Levinson-Waldman, Harsha Pandurnga, and Faiza Patel. Social Media Surveillance by\nthe U.S. Government. Brennan Center for Justice. Jan. 7, 2022.\nhttps://www.brennancenter.org/our-work/research-reports/social-media-surveillance-us-government;\nShoshana Zuboff. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of\nPower. Public Affairs. 2019.\n64. Angela Chen. Why the Future of Life Insurance May Depend on Your Online Presence. The Verge. Feb.\n7, 2019.", "c736820c-1ace-4809-b9e1-e7409aedbabb": "vulnerabilities and potential manipulation or misuse. \nInformation Security \nAI Actor Tasks: AI Development, Domain Experts, TEVV \n \nMAP 3.4: Processes for operator and practitioner pro\ufb01ciency with AI system performance and trustworthiness \u2013 and relevant \ntechnical standards and certi\ufb01cations \u2013 are de\ufb01ned, assessed, and documented. \nAction ID \nSuggested Action \nGAI Risks \nMP-3.4-001 \nEvaluate whether GAI operators and end-users can accurately understand \ncontent lineage and origin. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMP-3.4-002 Adapt existing training programs to include modules on digital content \ntransparency. \nInformation Integrity \nMP-3.4-003 Develop certi\ufb01cation programs that test pro\ufb01ciency in managing GAI risks and \ninterpreting content provenance, relevant to speci\ufb01c industry and context. \nInformation Integrity \nMP-3.4-004 Delineate human pro\ufb01ciency tests from tests of GAI capabilities. \nHuman-AI Con\ufb01guration", "ca71303d-50e5-4932-ad22-cc28fd1b5973": "60 \nZhang, Y. et al. (2023) Human favoritism, not AI aversion: People\u2019s perceptions (and bias) toward \ngenerative AI, human experts, and human\u2013GAI collaboration in persuasive content generation. Judgment \nand Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\nmaking/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\nhuman-experts-and-humangai-collaboration-in-persuasive-content-\ngeneration/419C4BD9CE82673EAF1D8F6C350C4FA8 \nZhang, Y. et al. (2023) Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \narXiv. https://arxiv.org/pdf/2309.01219 \nZhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \nhttps://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\nAnanth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc", "5cda24b9-1fe8-4f6f-9063-bce72af9de38": "and Homogenization \nMS-4.2-004 \nMonitor and document instances where human operators or other systems \noverride the GAI's decisions. Evaluate these cases to understand if the overrides \nare linked to issues related to content provenance. \nInformation Integrity \nMS-4.2-005 \nVerify and document the incorporation of results of structured public feedback \nexercises into design, implementation, deployment approval (\u201cgo\u201d/\u201cno-go\u201d \ndecisions), monitoring, and decommission decisions. \nHuman-AI Con\ufb01guration; \nInformation Security \nAI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV", "a5855cb3-4c78-4d83-9d0b-ebc776ccc509": "into technology design processes. The most prevalent in the United States is the Access Board\u2019s Section \n508 regulations,56 which are the technical standards for federal information communication technology (software, \nhardware, and web). Other standards include those issued by the International Organization for \nStandardization,57 and the World Wide Web Consortium Web Content Accessibility Guidelines,58 a globally \nrecognized voluntary consensus standard for web content and other information and communications \ntechnology. \nNIST has released Special Publication 1270, Towards a Standard for Identifying and Managing Bias \nin Artificial Intelligence.59 The special publication: describes the stakes and challenges of bias in artificial \nintelligence and provides examples of how and why it can chip away at public trust; identifies three categories \nof bias in AI \u2013 systemic, statistical, and human \u2013 and describes how and where they contribute to harms; and", "0b2ceca5-9195-4710-8263-5243535f0ded": "https://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology\u00ad\nschools-are-using-to-monitor-students/\n73. Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\nfighting back. Washington Post. Nov. 12, 2020.\nhttps://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/\n74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government\nTechnology. May 24, 2022.\nhttps://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;\nLydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability\nDiscrimination In New Surveillance Technologies: How new surveillance technologies in education,\npolicing, health care, and the workplace disproportionately harm disabled people. Center for Democracy\nand Technology Report. May 24, 2022.", "d0dc8661-9290-4fe8-a6ee-50705c7ef84e": "-    \nUSING THIS TECHNICAL COMPANION\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, \nuse, and deployment of automated systems to protect the rights of the American public in the age of artificial \nintelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights and \nprovides examples and concrete steps for communities, industry, governments, and others to take in order to \nbuild these protections into policy, practice, or the technological design process. \nTaken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help \nguard the American public against many of the potential and actual harms identified by researchers, technolo\u00ad\ngists, advocates, journalists, policymakers, and communities in the United States and around the world. This", "394f4c55-99c6-4780-adc2-a21afac9eb3c": "the creation and spread of NCII disproportionately impacts women and sexual minorities, and can have \nsubsequent negative consequences including decline in overall mental health, substance abuse, and \neven suicidal thoughts.  \nData used for training GAI models may unintentionally include CSAM and NCII. A recent report noted \nthat several commonly used GAI training datasets were found to contain hundreds of known images of", "df4b1933-e4e5-4d75-8122-4847e9d96181": "www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-rights-for-an\u00ad\nautomated-society/\n4. U.S. Dept. of Health, Educ. & Welfare, Report of the Sec\u2019y\u2019s Advisory Comm. on Automated Pers. Data Sys.,\nRecords, Computers, and the Rights of Citizens (July 1973). https://www.justice.gov/opcl/docs/rec-com\u00ad\nrights.pdf.\n5. See, e.g., Office of Mgmt. & Budget, Exec. Office of the President, Circular A-130, Managing Information as a\nStrategic Resource, app. II \u00a7\u00a03 (July 28, 2016); Org. of Econ. Co-Operation & Dev., Revision of the\nRecommendation of the Council Concerning Guidelines Governing the Protection of Privacy and Transborder\nFlows of Personal Data, Annex Part Two (June 20, 2013). https://one.oecd.org/document/C(2013)79/en/pdf.\n6. Andrew Wong et al. External validation of a widely implemented proprietary sepsis prediction model in\nhospitalized patients. JAMA Intern Med. 2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626", "d6be1dfa-ecf2-4df2-ae32-e2fe00fcf280": "a system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should be \navoided. In many cases, attributes that are highly correlated with demographic features, known as proxies, can \ncontribute to algorithmic discrimination. In cases where use of the demographic features themselves would \nlead to illegal algorithmic discrimination, reliance on such proxies in decision-making (such as that facilitated \nby an algorithm) may also be prohibited by law. Proactive testing should be performed to identify proxies by \ntesting for correlation between demographic information and attributes in any data used as part of system \ndesign, development, or use. If a proxy is identified, designers, developers, and deployers should remove the \nproxy; if needed, it may be possible to identify alternative attributes that can be used instead. At a minimum, \norganizations should ensure a proxy feature is not given undue weight and should monitor the system closely", "952a7c87-ad77-48d4-9558-483e11461872": "implement or improve social welfare systems, social development programs, and other systems that can impact \nlife chances. \nWelcome:\n\u2022\nSuresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\nand Technology Policy\n\u2022\nAnne-Marie Slaughter, CEO, New America\nModerator: Michele Evermore, Deputy Director for Policy, Office of Unemployment Insurance \nModernization, Office of the Secretary, Department of Labor \nPanelists:\n\u2022\nBlake Hall, CEO and Founder, ID.Me\n\u2022\nKarrie Karahalios, Professor of Computer Science, University of Illinois, Urbana-Champaign\n\u2022\nChristiaan van Veen, Director of Digital Welfare State and Human Rights Project, NYU School of Law's\nCenter for Human Rights and Global Justice\n58", "79791bf5-36a1-4b53-808f-06e613bde016": "Algorithmic \nDiscrimination \nProtections \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \nThere is extensive evidence showing that automated systems can produce inequitable outcomes and amplify \nexisting inequity.30 Data that fails to account for existing systemic biases in American society can result in a range of \nconsequences. For example, facial recognition technology that can contribute to wrongful and discriminatory \narrests,31 hiring algorithms that inform discriminatory decisions, and healthcare algorithms that discount \nthe severity of certain diseases in Black Americans. Instances of discriminatory practices built into and \nresulting from AI and other automated systems exist across many industries, areas, and contexts. While automated", "44ed0314-aa7d-41b7-b53b-f3e89b6c0241": "harm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about \nthose who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes, \nbut is not limited to, numerical, text, image, audio, or video data. \nSENSITIVE DOMAINS: \u201cSensitive domains\u201d are those in which activities being conducted can cause material \nharms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liber\u00ad\nties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections \nor where such enhanced protections are reasonably expected by the public include, but are not limited to, \nhealth, family planning and care, employment, education, criminal justice, and personal finance. In the context \nof this framework, such domains are considered sensitive whether or not the specifics of a system context", "a3b2244d-4b36-48b9-b8a0-ca009eb2368a": "membership inference risks; Revealing biometric, con\ufb01dential, copyrighted, \nlicensed, patented, personal, proprietary, sensitive, or trade-marked information; \nTracking or revealing location information of users or members of training \ndatasets. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Intellectual \nProperty \nMS-2.10-002 \nEngage directly with end-users and other stakeholders to understand their \nexpectations and concerns regarding content provenance. Use this feedback to \nguide the design of provenance data-tracking techniques. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMS-2.10-003 Verify deduplication of GAI training data samples, particularly regarding synthetic \ndata. \nHarmful Bias and Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV", "0d84d5ec-ddd4-49c7-ad85-abedec52004f": "42 \nMG-2.4-002 \nEstablish and maintain procedures for escalating GAI system incidents to the \norganizational risk management authority when speci\ufb01c criteria for deactivation \nor disengagement is met for a particular context of use or for the GAI system as a \nwhole. \nInformation Security \nMG-2.4-003 \nEstablish and maintain procedures for the remediation of issues which trigger \nincident response processes for the use of a GAI system, and provide stakeholders \ntimelines associated with the remediation plan. \nInformation Security \n \nMG-2.4-004 Establish and regularly review speci\ufb01c criteria that warrants the deactivation of \nGAI systems in accordance with set risk tolerances and appetites. \nInformation Security \n \nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 3.1: AI risks and bene\ufb01ts from third-party resources are regularly monitored, and risk controls are applied and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMG-3.1-001", "4c44df41-d3fa-4ebb-bd71-56049499a58f": "HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nImplement additional human oversight and safeguards for automated systems related to \nsensitive domains \nAutomated systems used within sensitive domains, including criminal justice, employment, education, and \nhealth, should meet the expectations laid out throughout this framework, especially avoiding capricious, \ninappropriate, and discriminatory impacts of these technologies. Additionally, automated systems used within \nsensitive domains should meet these expectations: \nNarrowly scoped data and inferences. Human oversight should ensure that automated systems in \nsensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attri\u00ad", "a6211724-c72f-45a8-ac49-c48909e4b03f": "Action ID \nSuggested Action \nGAI Risks \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities. \nInformation Integrity; Information \nSecurity \nMP-5.1-002 \nIdentify potential content provenance harms of GAI, such as misinformation or \ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \nrank risks based on their likelihood and potential impact, and determine how well \nprovenance solutions address speci\ufb01c risks and/or harms. \nInformation Integrity; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMP-5.1-003 \nConsider disclosing use of GAI to end users in relevant contexts, while considering \nthe objective of disclosure, the context of use, the likelihood and magnitude of the \nrisk posed, the audience of the disclosure, as well as the frequency of the \ndisclosures. \nHuman-AI Con\ufb01guration", "dba825c7-91df-4307-b440-8883ba97780e": "15 \nGV-1.3-004 Obtain input from stakeholder communities to identify unacceptable use, in \naccordance with activities in the AI RMF Map function. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content \nGV-1.3-005 \nMaintain an updated hierarchy of identi\ufb01ed and expected GAI risks connected to \ncontexts of GAI model advancement and use, potentially including specialized risk \nlevels for GAI systems that address issues such as model collapse and algorithmic \nmonoculture. \nHarmful Bias and Homogenization \nGV-1.3-006 \nReevaluate organizational risk tolerances to account for unacceptable negative risk \n(such as where signi\ufb01cant negative impacts are imminent, severe harms are \nactually occurring, or large-scale risks could occur); and broad GAI negative risks, \nincluding: Immature safety or risk cultures related to AI and GAI design,", "1f74cab4-d7bb-4a0a-a4b6-fda048160fbf": "estimate the value of homes used in mortgage underwriting or home insurance, and automated  \n    valuations from online aggregator websites; and \nEmployment-related systems such as workplace algorithms that inform all aspects of the terms  \n    and conditions of employment including, but not limited to, pay or promotion, hiring or termina- \n   tion algorithms, virtual or augmented reality workplace training programs, and electronic work \nplace surveillance and management systems. \n\u2022 Access to critical resources and services, including but not limited to:\nHealth  and health insurance technologies such as medical AI systems and devices, AI-assisted \n    diagnostic tools, algorithms or predictive models used to support clinical decision making, medical  \n    or insurance health risk assessments, drug addiction risk assessments and associated access alg \n-orithms, wearable technologies, wellness apps, insurance care allocation algorithms, and health", "3d8ffd3f-8a4b-4519-be1a-1fb41c2516ef": "impersonation, cyber-attacks, and weapons creation. \nCBRN Information or Capabilities; \nInformation Security \nMS-2.6-007 Regularly evaluate GAI system vulnerabilities to possible circumvention of safety \nmeasures.  \nCBRN Information or Capabilities; \nInformation Security \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV", "874d4eea-f631-4029-88c9-3bb7ec5b6799": "human-based components of a system are effective. \nTraining and assessment. Anyone administering, interacting with, or interpreting the outputs of an auto\u00ad\nmated system should receive training in that system, including how to properly interpret outputs of a system \nin light of its intended purpose and in how to mitigate the effects of automation bias. The training should reoc\u00ad\ncur regularly to ensure it is up to date with the system and to ensure the system is used appropriately. Assess\u00ad\nment should be ongoing to ensure that the use of the system with human involvement provides for appropri\u00ad\nate results, i.e., that the involvement of people does not invalidate the system's assessment as safe and effective \nor lead to algorithmic discrimination. \nOversight. Human-based systems have the potential for bias, including automation bias, as well as other \nconcerns that may limit their effectiveness. The results of assessments of the efficacy and potential bias of", "e403e66e-4463-4feb-b0f8-549065d3d0cd": "levels (i.e., for a speci\ufb01c use case), or at the ecosystem level \u2013 that is, beyond a single system or \norganizational context. Examples of the latter include the expansion of \u201calgorithmic \nmonocultures,3\u201d resulting from repeated use of the same model, or impacts on access to \nopportunity, labor markets, and the creative economies.4 \n\u2022 \nSource of risk: Risks may emerge from factors related to the design, training, or operation of the \nGAI model itself, stemming in some cases from GAI model or system inputs, and in other cases, \nfrom GAI system outputs. Many GAI risks, however, originate from human behavior, including \n \n \n3 \u201cAlgorithmic monocultures\u201d refers to the phenomenon in which repeated use of the same model or algorithm in \nconsequential decision-making settings like employment and lending can result in increased susceptibility by \nsystems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm.", "dee516cd-069d-42a7-a99a-48eaf99ecea1": "2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina\u00ad\nteenager-2022-03-30/\n40. Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.\nFeb. 2018. https://nyupress.org/9781479837243/algorithms-of-oppression/\n41. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\n2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina\u00ad\nteenager-2022-03-30/\n42. Miranda Bogen. All the Ways Hiring Algorithms Can Introduce Bias. Harvard Business Review. May\n6, 2019. https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias\n43. Arli Christian. Four Ways the TSA Is Making Flying Easier for Transgender People. American Civil\nLiberties Union. Apr. 5, 2022. https://www.aclu.org/news/lgbtq-rights/four-ways-the-tsa-is-making\u00ad\nflying-easier-for-transgender-people", "9532c8de-647b-4a7c-9bd1-ba0706027cc3": "those potentially impacted), via community or o\ufb03cial resources (e.g., AI incident \ndatabase, AVID, CVE, NVD, or OECD AI incident monitor). \nHuman-AI Con\ufb01guration; Value \nChain and Component Integration \nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with \ndiverse composition and responsibilities based on the particular incident type. \nHarmful Bias and Homogenization \nGV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks \ndemonstrate and maintain the appropriate skills and training. \nHuman-AI Con\ufb01guration \nGV-2.1-004 When systems may raise national security risks, involve national security \nprofessionals in mapping, measuring, and managing those risks. \nCBRN Information or Capabilities; \nDangerous, Violent, or Hateful \nContent; Information Security \nGV-2.1-005 \nCreate mechanisms to provide protections for whistleblowers who report, based", "91c195f7-76df-4a41-821a-706980e7303d": "MS-1.1-005 \nEvaluate novel methods and technologies for the measurement of GAI-related \nrisks including in content provenance, o\ufb00ensive cyber, and CBRN, while \nmaintaining the models\u2019 ability to produce valid, reliable, and factually accurate \noutputs. \nInformation Integrity; CBRN \nInformation or Capabilities; \nObscene, Degrading, and/or \nAbusive Content", "2bac1e80-9d70-448a-91ad-5f5db978bfb0": "APPENDIX\nLisa Feldman Barrett \nMadeline Owens \nMarsha Tudor \nMicrosoft Corporation \nMITRE Corporation \nNational Association for the \nAdvancement of Colored People \nLegal Defense and Educational \nFund \nNational Association of Criminal \nDefense Lawyers \nNational Center for Missing & \nExploited Children \nNational Fair Housing Alliance \nNational Immigration Law Center \nNEC Corporation of America \nNew America\u2019s Open Technology \nInstitute \nNew York Civil Liberties Union \nNo Name Provided \nNotre Dame Technology Ethics \nCenter \nOffice of the Ohio Public Defender \nOnfido \nOosto \nOrissa Rose \nPalantir \nPangiam \nParity Technologies \nPatrick A. Stewart, Jeffrey K. \nMullins, and Thomas J. Greitens \nPel Abbott \nPhiladelphia Unemployment \nProject \nProject On Government Oversight \nRecording Industry Association of \nAmerica \nRobert Wilkens \nRon Hedges \nScience, Technology, and Public \nPolicy Program at University of \nMichigan Ann Arbor \nSecurity Industry Association \nSheila Dean", "bf8c62ba-0a99-4b4e-ba76-134ed064e07c": "Protection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation \nEquity includes a commitment from the agencies that oversee mortgage lending to include a \nnondiscrimination standard in the proposed rules for Automated Valuation Models.52\nThe Equal Employment Opportunity Commission and the Department of Justice have clearly \nlaid out how employers\u2019 use of AI and other automated systems can result in \ndiscrimination against job applicants and employees with disabilities.53 The documents explain \nhow employers\u2019 use of software that relies on algorithmic decision-making may violate existing requirements \nunder Title I of the Americans with Disabilities Act (\u201cADA\u201d). This technical assistance also provides practical \ntips to employers on how to comply with the ADA, and to job applicants and employees who think that their \nrights may have been violated. \nDisparity assessments identified harms to Black patients' healthcare access. A widely", "dcaa8692-bdab-46da-a3d0-6c80458d59dc": "seizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for \nhuman review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws \nprotect the American people against discrimination. \n8", "27b0d1c5-ab5c-4da1-975d-8c653dc7ea35": "vulnerabilities by stealing proprietary data or running malicious code remotely on a machine. Merely \nquerying a closed production model can elicit previously undisclosed information about that model. \nAnother cybersecurity risk to GAI is data poisoning, in which an adversary compromises a training \ndataset used by a model to manipulate its outputs or operation. Malicious tampering with data or parts \nof the model could exacerbate risks associated with GAI system outputs. \nTrustworthy AI Characteristics: Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \n2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system\u2019s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright.", "f45ef5ad-ea8b-4d8f-95bc-0124e9ac4bea": "they've used drugs, or whether they've expressed interest in LGBTQI+ groups, and then use that data to \nforecast student success.76 Parents and education experts have expressed concern about collection of such\nsensitive data without express parental consent, the lack of transparency in how such data is being used, and\nthe potential for resulting discriminatory impacts.\n\u2022 Many employers transfer employee data to third party job verification services. This information is then used\nby potential future employers, banks, or landlords. In one case, a former employee alleged that a\ncompany supplied false data about her job title which resulted in a job offer being revoked.77\n37", "7d753662-c05f-4b88-be8d-15be78e51a5b": "17 \nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that \ndoes not increase risks or decrease the organization\u2019s trustworthiness. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when \nnecessary.  \nInformation Security; Value Chain \nand Component Integration \nGV-1.7-002 \nConsider the following factors when decommissioning GAI systems: Data \nretention requirements; Data security, e.g., containment, protocols, Data leakage \nafter decommissioning; Dependencies between upstream, downstream, or other \ndata, internet of things (IOT) or AI systems; Use of open-source data or models; \nUsers\u2019 emotional entanglement with GAI functions. \nHuman-AI Con\ufb01guration; \nInformation Security; Value Chain \nand Component Integration \nAI Actor Tasks: AI Deployment, Operation and Monitoring", "71111a8b-c09c-46c9-ba56-2dd62da7e064": "occur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual). \nState of AI Incident Tracking and Disclosure \nFormal channels do not currently exist to report and document AI incidents. However, a number of \npublicly available databases have been created to document their occurrence. These reporting channels \nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \namount of media coverage.", "541362a2-abca-4663-a0d6-d0e94615eb8b": "adversaries and are often subject to special requirements, such as those governing classified information and \nother protected data. Such activities require alternative, compatible safeguards through existing policies that \ngovern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and \nResponsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and \nFramework. The implementation of these policies to national security and defense activities can be informed by \nthe Blueprint for an AI Bill of Rights where feasible. \nThe Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or \ndefense, substantive or procedural, enforceable at law or in equity by any party against the United States, its \ndepartments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a \nwaiver of sovereign immunity. \nCopyright Information", "1cd98259-2c46-4bd1-a687-3e5a3960ea6e": "collected or stored about them. Such a report should be machine-readable, understandable by most users, and \ninclude, to the greatest extent allowable under law, any data and metadata about them or collected from them, \nwhen and how their data and metadata were collected, the specific ways that data or metadata are being used, \nwho has access to their data and metadata, and what time limitations apply to these data. In cases where a user \nlogin is not available, identity verification may need to be performed before providing such a report to ensure \nuser privacy. Additionally, summary reporting should be proactively made public with general information \nabout how peoples\u2019 data and metadata is used, accessed, and stored. Summary reporting should include the \nresults of any surveillance pre-deployment assessment, including disparity assessment in the real-world \ndeployment context, the specific identified goals of any data collection, and the assessment done to ensure", "1e4afe32-1616-471f-b01c-9243522ed764": "software development, and IT governance and to legal, compliance, and risk \nmanagement activities. \nInformation Security; Data Privacy \nMP-4.1-004 Document training data curation policies, to the extent possible and according to \napplicable laws and policies. \nIntellectual Property; Data Privacy; \nObscene, Degrading, and/or \nAbusive Content \nMP-4.1-005 \nEstablish policies for collection, retention, and minimum quality of data, in \nconsideration of the following risks: Disclosure of inappropriate CBRN information; \nUse of Illegal or dangerous content; O\ufb00ensive cyber capabilities; Training data \nimbalances that could give rise to harmful biases; Leak of personally identi\ufb01able \ninformation, including facial likenesses of individuals. \nCBRN Information or Capabilities; \nIntellectual Property; Information \nSecurity; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content; Data \nPrivacy", "8505543c-6b05-4924-a1a0-64bdad635872": "content/uploads/2016/10/Dietvorst-Simmons-Massey-2014.pdf \nDuhigg, C. (2012) How Companies Learn Your Secrets. New York Times. \nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html \nElsayed, G. et al. (2024) Images altered to trick machine vision can in\ufb02uence humans too. Google \nDeepMind. https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-\nin\ufb02uence-humans-too/ \nEpstein, Z. et al. (2023). Art and the science of generative AI. Science. \nhttps://www.science.org/doi/10.1126/science.adh4451 \nFe\ufb00er, M. et al. (2024) Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv. \nhttps://arxiv.org/pdf/2401.15897 \nGlazunov, S. et al. (2024) Project Naptime: Evaluating O\ufb00ensive Security Capabilities of Large Language \nModels. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated", "7936f820-c3d6-4b50-b2da-c84f9a3dcccb": "NOTICE & \nEXPLANATION \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \nAutomated systems now determine opportunities, from employment to credit, and directly shape the American \npublic\u2019s experiences, from the courtroom to online classrooms, in ways that profoundly impact people\u2019s lives. But this \nexpansive impact is not always visible. An applicant might not know whether a person rejected their resume or a \nhiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny\u00ad\ning their bail is informed by an automated system that labeled them \u201chigh risk.\u201d From correcting errors to contesting \ndecisions, people are often denied the knowledge they need to address the impact of automated systems on their lives.", "184ca7cb-86a8-446c-a33e-6102100fea68": "50. Various organizations have offered proposals for how such assessments might be designed. See, e.g.,\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf.\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. Data & Society\nResearch Institute Report. June 29, 2021. https://datasociety.net/library/assembling-accountability\u00ad\nalgorithmic-impact-assessment-for-the-public-interest/; Nicol Turner Lee, Paul Resnick, and Genie\nBarton. Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms.\nBrookings Report. May 22, 2019.\nhttps://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and\u00ad\npolicies-to-reduce-consumer-harms/; Andrew D. Selbst. An Institutional View Of Algorithmic Impact\nAssessments. Harvard Journal of Law & Technology. June 15, 2021. https://ssrn.com/abstract=3867634;", "e0ffe85d-16ed-4878-b4e3-295aa31e0b27": "23 \nMP-1.1-002 \nDetermine and document the expected and acceptable GAI system context of \nuse in collaboration with socio-cultural and other domain experts, by assessing: \nAssumptions and limitations; Direct value to the organization; Intended \noperational environment and observed usage patterns; Potential positive and \nnegative impacts to individuals, public safety, groups, communities, \norganizations, democratic institutions, and the physical environment; Social \nnorms and expectations. \nHarmful Bias and Homogenization \nMP-1.1-003 \nDocument risk measurement plans to address identi\ufb01ed risks. Plans may \ninclude, as applicable: Individual and group cognitive biases (e.g., con\ufb01rmation \nbias, funding bias, groupthink) for AI Actors involved in the design, \nimplementation, and use of GAI systems; Known past GAI system incidents and \nfailure modes; In-context use and foreseeable misuse, abuse, and o\ufb00-label use; \nOver reliance on quantitative metrics and methodologies without su\ufb03cient", "13eeef46-3519-4029-93e4-0a6d9c879c1e": "documented. Documentation provides su\ufb03cient information to assist relevant AI Actors when making decisions and taking \nsubsequent actions. \nAction ID \nSuggested Action \nGAI Risks \nMP-2.2-001 \nIdentify and document how the system relies on upstream data sources, \nincluding for content provenance, and if it serves as an upstream dependency for \nother systems. \nInformation Integrity; Value Chain \nand Component Integration \nMP-2.2-002 \nObserve and analyze how the GAI system interacts with external networks, and \nidentify any potential for negative externalities, particularly where content \nprovenance might be compromised. \nInformation Integrity \nAI Actor Tasks: End Users \n \nMAP 2.3: Scienti\ufb01c integrity and TEVV considerations are identi\ufb01ed and documented, including those related to experimental \ndesign, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct \nvalidation \nAction ID \nSuggested Action \nGAI Risks \nMP-2.3-001", "4deb37b2-62d4-466b-8d54-8db1960841b6": "Table of Contents \n1. \nIntroduction ..............................................................................................................................................1 \n2. \nOverview of Risks Unique to or Exacerbated by GAI .....................................................................2 \n3. \nSuggested Actions to Manage GAI Risks ......................................................................................... 12 \nAppendix A. Primary GAI Considerations ............................................................................................... 47 \nAppendix B. References ................................................................................................................................ 54", "05d801bc-8707-4f49-9b8b-bbd98c183db6": "9 \nand reduced content diversity). Overly homogenized outputs can themselves be incorrect, or they may \nlead to unreliable decision-making or amplify harmful biases. These phenomena can \ufb02ow from \nfoundation models to downstream models and systems, with the foundation models acting as \n\u201cbottlenecks,\u201d or single points of failure.  \nOverly homogenized content can contribute to \u201cmodel collapse.\u201d Model collapse can occur when model \ntraining over-relies on synthetic data, resulting in data points disappearing from the distribution of the \nnew model\u2019s outputs. In addition to threatening the robustness of the model overall, model collapse \ncould lead to homogenized outputs, including by amplifying any homogenization from the model used to \ngenerate the synthetic training data. \nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Valid and Reliable \n2.7. Human-AI Con\ufb01guration \nGAI system use can involve varying risks of miscon\ufb01gurations and poor interactions between a system", "1a5a5c9f-bf1c-4b49-a652-a626ecd1fc72": "to being entitled to clear mechanisms to control access to and use of their data\u2014including their metadata\u2014in a \nproactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal data \nshould meet these expectations. \nProtect privacy by design and by default \nPrivacy by design and by default. Automated systems should be designed and built with privacy protect\u00ad\ned by default. Privacy risks should be assessed throughout the development life cycle, including privacy risks \nfrom reidentification, and appropriate technical and policy mitigation measures should be implemented. This \nincludes potential harms to those who are not users of the automated system, but who may be harmed by \ninferred data, purposeful privacy violations, or community surveillance or other community harms. Data \ncollection should be minimized and clearly communicated to the people whose data is collected. Data should", "3bf01afa-de74-4da2-b9e8-e0fc2c60f433": "the explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment \nof how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of \nrisk. Individualized profile information should be made readily available to the greatest extent possible that \nincludes explanations for any system impacts or inferences. Reporting should be provided in a clear plain \nlanguage and machine-readable manner. \n44", "f508a1a9-5937-4fd1-9f4e-fd76e7060e7f": "51 \ngeneral public participants. For example, expert AI red-teamers could modify or verify the \nprompts written by general public AI red-teamers. These approaches may also expand coverage \nof the AI risk attack surface.  \n\u2022 \nHuman / AI: Performed by GAI in combination with specialist or non-specialist human teams. \nGAI-led red-teaming can be more cost e\ufb00ective than human red-teamers alone. Human or GAI-\nled AI red-teaming may be better suited for eliciting di\ufb00erent types of harms. \n \nA.1.6. Content Provenance \nOverview \nGAI technologies can be leveraged for many applications such as content generation and synthetic data. \nSome aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to \ndistinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and", "a25825a2-9545-4754-8b47-d2c835ae0277": "listed organizations and individuals:\nAccenture \nAccess Now \nACT | The App Association \nAHIP \nAIethicist.org \nAirlines for America \nAlliance for Automotive Innovation \nAmelia Winger-Bearskin \nAmerican Civil Liberties Union \nAmerican Civil Liberties Union of \nMassachusetts \nAmerican Medical Association \nARTICLE19 \nAttorneys General of the District of \nColumbia, Illinois, Maryland, \nMichigan, Minnesota, New York, \nNorth Carolina, Oregon, Vermont, \nand Washington \nAvanade \nAware \nBarbara Evans \nBetter Identity Coalition \nBipartisan Policy Center \nBrandon L. Garrett and Cynthia \nRudin \nBrian Krupp \nBrooklyn Defender Services \nBSA | The Software Alliance \nCarnegie Mellon University \nCenter for Democracy & \nTechnology \nCenter for New Democratic \nProcesses \nCenter for Research and Education \non Accessible Technology and \nExperiences at University of \nWashington, Devva Kasnitz, L Jean \nCamp, Jonathan Lazar, Harry \nHochheiser \nCenter on Privacy & Technology at \nGeorgetown Law \nCisco Systems", "159da431-8282-4348-b6ab-0ffbaab8d0e3": "SAFE AND EFFECTIVE \nSYSTEMS \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nDerived data sources tracked and reviewed carefully. Data that is derived from other data through \nthe use of algorithms, such as data derived or inferred from prior model outputs, should be identified and \ntracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk \ninputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be care\u00ad\nfully validated against the risk of collateral consequences. \nData reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result \nin the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi\u00ad", "9eb7bae6-ba9b-41c0-bdab-67ddf414b5e7": "Value Chain and Component \nIntegration \nGV-6.1-008 Maintain records of changes to content made by third parties to promote content \nprovenance, including sources, timestamps, metadata. \nInformation Integrity; Value Chain \nand Component Integration; \nIntellectual Property \nGV-6.1-009 \nUpdate and integrate due diligence processes for GAI acquisition and \nprocurement vendor assessments to include intellectual property, data privacy, \nsecurity, and other risks. For example, update processes to: Address solutions that \nmay rely on embedded GAI technologies; Address ongoing monitoring, \nassessments, and alerting, dynamic risk assessments, and real-time reporting \ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \nmodeling libraries, tools and APIs, \ufb01ne-tuned models, and embedded tools; \nAssess GAI vendors, open-source or proprietary GAI tools, or GAI service \nproviders against incident or vulnerability databases. \nData Privacy; Human-AI", "96b3520c-e51a-4bca-880f-c6ee2e4549fc": "HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nEquitable. Consideration should be given to ensuring outcomes of the fallback and escalation system are \nequitable when compared to those of the automated system and such that the fallback and escalation \nsystem provides equitable access to underserved communities.105 \nTimely. Human consideration and fallback are only useful if they are conducted and concluded in a \ntimely manner. The determination of what is timely should be made relative to the specific automated \nsystem, and the review system should be staffed and regularly assessed to ensure it is providing timely \nconsideration and fallback. In time-critical systems, this mechanism should be immediately available or,", "6ac87d10-79a8-477b-af36-2b91fca9d740": "MP-3.4-003 Develop certi\ufb01cation programs that test pro\ufb01ciency in managing GAI risks and \ninterpreting content provenance, relevant to speci\ufb01c industry and context. \nInformation Integrity \nMP-3.4-004 Delineate human pro\ufb01ciency tests from tests of GAI capabilities. \nHuman-AI Con\ufb01guration \nMP-3.4-005 Implement systems to continually monitor and track the outcomes of human-GAI \ncon\ufb01gurations for future re\ufb01nement and improvements. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMP-3.4-006 \nInvolve the end-users, practitioners, and operators in GAI system in prototyping \nand testing activities. Make sure these tests cover various scenarios, such as crisis \nsituations or ethically sensitive contexts. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content \nAI Actor Tasks: AI Design, AI Development, Domain Experts, End-Users, Human Factors, Operation and Monitoring", "8f14f4f7-2949-4890-aa60-776832710cf9": "You should know that an automated system is being used, \nand understand how and why it contributes to outcomes \nthat impact you. Designers, developers, and deployers of automat\u00ad\ned systems should provide generally accessible plain language docu\u00ad\nmentation including clear descriptions of the overall system func\u00ad\ntioning and the role automation plays, notice that such systems are in \nuse, the individual or organization responsible for the system, and ex\u00ad\nplanations of outcomes that are clear, timely, and accessible. Such \nnotice should be kept up-to-date and people impacted by the system \nshould be notified of significant use case or key functionality chang\u00ad\nes. You should know how and why an outcome impacting you was de\u00ad\ntermined by an automated system, including when the automated \nsystem is not the sole input determining the outcome. Automated \nsystems should provide explanations that are technically valid, \nmeaningful and useful to you and to any operators or others who", "32b574ab-df81-4836-825b-ebd15d6f6933": "tion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason\u00ad\nable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to \nidentify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for \nreplacing individual-level sensitive data. \nDemonstrate the safety and effectiveness of the system \nIndependent evaluation. Automated systems should be designed to allow for independent evaluation (e.g., \nvia application programming interfaces). Independent evaluators, such as researchers, journalists, ethics \nreview boards, inspectors general, and third-party auditors, should be given access to the system and samples \nof associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual \nproperty law), in order to perform such evaluations. Mechanisms should be included to ensure that system", "02403941-419a-4437-8539-3619d13dadd0": "including automated tenant background screening and facial recognition-based controls to enter or exit \nhousing complexes. Employment-related concerning uses included discrimination in automated hiring \nscreening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key \nconcern, pointing out that students should be able to reinvent themselves and require privacy of their student \nrecords and education-related data in order to do so. The overarching concerns of surveillance in these \ndomains included concerns about the chilling effects of surveillance on student expression, inappropriate \ncontrol of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work \nand life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists \npointed out ways that data from one situation was misapplied in another in a way that limited people's", "70607810-fc33-4977-b1c4-2aafa131fe73": "Children. The Markup. Jan. 11, 2022.\nhttps://themarkup.org/machine-learning/2022/01/11/this-private-equity-firm-is-amassing-companies\u00ad\nthat-collect-data-on-americas-children\n77. Reed Albergotti. Every employee who leaves Apple becomes an \u2018associate\u2019: In job databases used by\nemployers to verify resume information, every former Apple employee\u2019s title gets erased and replaced with\na generic title. The Washington Post. Feb. 10, 2022.\nhttps://www.washingtonpost.com/technology/2022/02/10/apple-associate/\n78. National Institute of Standards and Technology. Privacy Framework Perspectives and Success\nStories. Accessed May 2, 2022.\nhttps://www.nist.gov/privacy-framework/getting-started-0/perspectives-and-success-stories\n79. ACLU of New York. What You Need to Know About New York\u2019s Temporary Ban on Facial\nRecognition in Schools. Accessed May 2, 2022.\nhttps://www.nyclu.org/en/publications/what-you-need-know-about-new-yorks-temporary-ban-facial\u00ad\nrecognition-schools", "1d9e0333-6e5d-440b-9f8f-d66d847da116": "41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Con\ufb01guration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-\ngenerated content to detect subtle shifts in quality or alignment with \ncommunity and societal values. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMG-2.2-009 \nConsider opportunities to responsibly use synthetic data and other privacy \nenhancing techniques in GAI development, where appropriate and applicable, \nmatch the statistical properties of real-world data without disclosing personally \nidenti\ufb01able information or contributing to homogenization. \nData Privacy; Intellectual Property; \nInformation Integrity;", "388a8e0a-68af-4704-b41d-6aa3f1cf93d3": "Karen Levy, Assistant Professor, Department of Information Science, Cornell University\n\u2022\nNatasha Duarte, Project Director, Upturn\n\u2022\nElana Zeide, Assistant Professor, University of Nebraska College of Law\n\u2022\nFabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community\nAdvocate and Floor Captain, Atlantic Plaza Towers Tenants Association\nThe individual panelists described the ways in which AI systems and other technologies are increasingly being \nused to limit access to equal opportunities in education, housing, and employment. Education-related \nconcerning uses included the increased use of remote proctoring systems, student location and facial \nrecognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses \nincluding automated tenant background screening and facial recognition-based controls to enter or exit \nhousing complexes. Employment-related concerning uses included discrimination in automated hiring", "284bf0f0-c913-4bd8-abfb-13eb20037df5": "improvement processes to mitigate risks related to unexplainable GAI systems. \nHarmful Bias and Homogenization \nMG-3.2-002 \nDocument how pre-trained models have been adapted (e.g., \ufb01ne-tuned, or \nretrieval-augmented generation) for the speci\ufb01c generative task, including any \ndata augmentations, parameter adjustments, or other modi\ufb01cations. Access to \nun-tuned (baseline) models supports debugging the relative in\ufb02uence of the pre-\ntrained weights compared to the \ufb01ne-tuned model weights or other system \nupdates. \nInformation Integrity; Data Privacy \nMG-3.2-003 \nDocument sources and types of training data and their origins, potential biases \npresent in the data related to the GAI application and its content provenance, \narchitecture, training process of the pre-trained model including information on \nhyperparameters, training duration, and any \ufb01ne-tuning or retrieval-augmented \ngeneration processes applied. \nInformation Integrity; Harmful Bias \nand Homogenization; Intellectual \nProperty", "d6e30012-fa19-4dcd-a2cf-f69d6fb26661": "ENDNOTES\n1.The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the\nFederal\u00a0Government. https://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive\norder-advancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\n2. The White House. Remarks by President Biden on the Supreme Court Decision to Overturn Roe v. Wade. Jun.\n24, 2022. https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/06/24/remarks-by-president\u00ad\nbiden-on-the-supreme-court-decision-to-overturn-roe-v-wade/\n3. The White House. Join the Effort to Create A Bill of Rights for an Automated Society. Nov. 10, 2021. https://\nwww.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of-rights-for-an\u00ad\nautomated-society/\n4. U.S. Dept. of Health, Educ. & Welfare, Report of the Sec\u2019y\u2019s Advisory Comm. on Automated Pers. Data Sys.,", "9fbf8f80-e6dd-400f-a303-a41949e33f88": "sures, overall and subgroup parity assessment, and calibration. Demographic data collected for disparity \nassessment should be separated from data used for the automated system and privacy protections should be \ninstituted; in some cases it may make sense to perform such assessment using a data sample. For every \ninstance where the deployed automated system leads to different treatment or impacts disfavoring the identi\u00ad\nfied groups, the entity governing, implementing, or using the system should document the disparity and a \njustification for any continued use of the system. \nDisparity mitigation. When a disparity assessment identifies a disparity against an assessed group, it may \nbe appropriate to take steps to mitigate or eliminate the disparity. In some cases, mitigation or elimination of \nthe disparity may be required by law. \nDisparities that have the potential to lead to algorithmic", "c773eac8-5b55-4176-9caf-d8b787d44bbf": "attempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropri\u00ad\nately to identified risks. Appropriate responses include determining not to process data when the privacy risks \noutweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do not \ninclude sharing or transferring the privacy risks to users via notice or consent requests where users could not \nreasonably be expected to understand the risks without further support. \nPrivacy-preserving security. Entities creating, using, or governing automated systems should follow \nprivacy and security best practices designed to ensure data and metadata do not leak beyond the specific \nconsented use case. Best practices could include using privacy-enhancing cryptography or other types of \nprivacy-enhancing technologies or fine-grained permissions and access control mechanisms, along with \nconventional system security protocols. \n33", "7bb30ce5-0600-4b33-9bfd-7ac4ca73e5ac": "than non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \nAccordingly, GAI may call for di\ufb00erent levels of oversight from AI Actors or di\ufb00erent human-AI \ncon\ufb01gurations in order to manage their risks e\ufb00ectively. Organizations\u2019 use of GAI systems may also \nwarrant additional human review, tracking and documentation, and greater management oversight.  \nAI technology can produce varied outputs in multiple modalities and present many classes of user \ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely di\ufb00ering \napplications and contexts of use. These can include data labeling and preparation, development of GAI \nmodels, content moderation, code generation and review, text generation and editing, image and video \ngeneration, summarization, search, and chat. These activities can take place within organizational \nsettings or in the public domain.", "7210036c-acb0-4819-9550-ffa78232f5ba": "implementation, and use of GAI systems; Known past GAI system incidents and \nfailure modes; In-context use and foreseeable misuse, abuse, and o\ufb00-label use; \nOver reliance on quantitative metrics and methodologies without su\ufb03cient \nawareness of their limitations in the context(s) of use; Standard measurement \nand structured human feedback approaches; Anticipated human-AI \ncon\ufb01gurations. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMP-1.1-004 \nIdentify and document foreseeable illegal uses or applications of the GAI system \nthat surpass organizational risk tolerances. \nCBRN Information or Capabilities; \nDangerous, Violent, or Hateful \nContent; Obscene, Degrading, \nand/or Abusive Content \nAI Actor Tasks: AI Deployment \n \nMAP 1.2: Interdisciplinary AI Actors, competencies, skills, and capacities for establishing context re\ufb02ect demographic diversity and", "3c0df90f-a14c-4f10-aa0c-c97aff119f68": "aversion, emotional entanglement); Possibility for malicious use; Whether the \nsystem introduces signi\ufb01cant new security vulnerabilities; Anticipated system \nimpact on some groups compared to others; Unreliable decision making \ncapabilities, validity, adaptability, and variability of GAI system performance over \ntime. \nInformation Integrity; Obscene, \nDegrading, and/or Abusive \nContent; Value Chain and \nComponent Integration; Harmful \nBias and Homogenization; \nDangerous, Violent, or Hateful \nContent; CBRN Information or \nCapabilities \nGV-1.3-002 \nEstablish minimum thresholds for performance or assurance criteria and review as \npart of deployment approval (\u201cgo/\u201dno-go\u201d) policies, procedures, and processes, \nwith reviewed processes and approval thresholds re\ufb02ecting measurement of GAI \ncapabilities and risks. \nCBRN Information or Capabilities; \nConfabulation; Dangerous, \nViolent, or Hateful Content \nGV-1.3-003", "bf95a1a2-c283-48fe-a55f-e803793424c7": "make it possible to generate both text-based disinformation and highly realistic \u201cdeepfakes\u201d \u2013 that is, \nsynthetic audiovisual content and photorealistic images.12 Additional disinformation threats could be \nenabled by future GAI models trained on new data modalities. \nDisinformation and misinformation \u2013 both of which may be facilitated by GAI \u2013 may erode public trust in \ntrue or valid evidence and information, with downstream e\ufb00ects. For example, a synthetic image of a \nPentagon blast went viral and brie\ufb02y caused a drop in the stock market. Generative AI models can also \nassist malicious actors in creating compelling imagery and propaganda to support disinformation \ncampaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and \nengagement on social media platforms. Additionally, generative AI models can assist malicious actors in \ncreating fraudulent content intended to impersonate others.", "320a9f8a-cf3b-477b-bff3-6d7763f40da5": "ENDNOTES\n75. See., e.g., Sam Sabin. Digital surveillance in a post-Roe world. Politico. May 5, 2022. https://\nwww.politico.com/newsletters/digital-future-daily/2022/05/05/digital-surveillance-in-a-post-roe\u00ad\nworld-00030459; Federal Trade Commission. FTC Sues Kochava for Selling Data that Tracks People at\nReproductive Health Clinics, Places of Worship, and Other Sensitive Locations. Aug. 29, 2022. https://\nwww.ftc.gov/news-events/news/press-releases/2022/08/ftc-sues-kochava-selling-data-tracks-people\u00ad\nreproductive-health-clinics-places-worship-other\n76. Todd Feathers. This Private Equity Firm Is Amassing Companies That Collect Data on America\u2019s\nChildren. The Markup. Jan. 11, 2022.\nhttps://themarkup.org/machine-learning/2022/01/11/this-private-equity-firm-is-amassing-companies\u00ad\nthat-collect-data-on-americas-children\n77. Reed Albergotti. Every employee who leaves Apple becomes an \u2018associate\u2019: In job databases used by", "84960ccd-69b5-46d1-8d44-e29d88913944": "enforcement or national security restrictions prevent doing so. Care should be taken to balance individual \nprivacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and \ncontrols allow access to such data without compromising privacy. \nReporting. Entities responsible for the development or use of automated systems should provide \nreporting of an appropriately designed algorithmic impact assessment,50 with clear specification of who \nperforms the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in \nresponse to the assessment. This algorithmic impact assessment should include at least: the results of any \nconsultation, design stage equity assessments (potentially including qualitative analysis), accessibility \ndesigns and testing, disparity testing, document any remaining disparities, and detail any mitigation", "5222fdb5-dc94-46b4-a787-d0b88634fadf": "NOTICE & \nEXPLANATION \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nAn automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and \nexplanations as to how and why a decision was made or an action was taken by the system. These expectations are \nexplained below. \nProvide clear, timely, understandable, and accessible notice of use and explanations \u00ad\nGenerally accessible plain language documentation. The entity responsible for using the automated \nsystem should ensure that documentation describing the overall system (including any human components) is \npublic and easy to find. The documentation should describe, in plain language, how the system works and how", "3d97b11c-2ed2-4889-b752-391f0291a629": "Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \nsecurity and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles \nof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \ndevelop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implement \nthese principles.22\nThe National Science Foundation (NSF) funds extensive research to help foster the \ndevelopment of automated systems that adhere to and advance their safety, security and \neffectiveness. Multiple NSF programs support research that directly addresses many of these principles: \nthe National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable \nAI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe", "07c4f5ec-74a2-4d13-91b7-24e7f03335a0": "reinforce those legal protections but extend beyond them to ensure equity for underserved communities48 \neven in circumstances where a specific legal protection may not be clearly established. These protections \nshould be instituted throughout the design, development, and deployment process and are described below \nroughly in the order in which they would be instituted. \nProtect the public from algorithmic discrimination in a proactive and ongoing manner \nProactive assessment of equity in design. Those responsible for the development, use, or oversight of \nautomated systems should conduct proactive equity assessments in the design phase of the technology \nresearch and development or during its acquisition to review potential input data, associated historical \ncontext, accessibility for people with disabilities, and societal goals to identify potential discrimination and \neffects on equity resulting from the introduction of the technology. The assessed groups should be as inclusive", "1afec939-fedf-4622-b091-e4465e0875b5": "International Corporation \nInformation Technology and \nInnovation Foundation \nInformation Technology Industry \nCouncil \nInnocence Project \nInstitute for Human-Centered \nArtificial Intelligence at Stanford \nUniversity \nIntegrated Justice Information \nSystems Institute \nInternational Association of Chiefs \nof Police \nInternational Biometrics + Identity \nAssociation \nInternational Business Machines \nCorporation \nInternational Committee of the Red \nCross \nInventionphysics \niProov \nJacob Boudreau \nJennifer K. Wagner, Dan Berger, \nMargaret Hu, and Sara Katsanis \nJonathan Barry-Blocker \nJoseph Turow \nJoy Buolamwini \nJoy Mack \nKaren Bureau \nLamont Gholston \nLawyers\u2019 Committee for Civil \nRights Under Law \n60", "f1486495-75ff-443e-b15a-df643197c8c5": "individual consumers and communities in the context of a growing ecosystem of AI-enabled consumer \nproducts, advanced platforms and services, \u201cInternet of Things\u201d (IoT) devices, and smart city products and \nservices. \nWelcome:\n\u2022\nRashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\nTechnology Policy\n\u2022\nKaren Kornbluh, Senior Fellow and Director of the Digital Innovation and Democracy Initiative, German\nMarshall Fund\nModerator: \nDevin E. Willis, Attorney, Division of Privacy and Identity Protection, Bureau of Consumer Protection, Federal \nTrade Commission \nPanelists: \n\u2022\nTamika L. Butler, Principal, Tamika L. Butler Consulting\n\u2022\nJennifer Clark, Professor and Head of City and Regional Planning, Knowlton School of Engineering, Ohio\nState University\n\u2022\nCarl Holshouser, Senior Vice President for Operations and Strategic Initiatives, TechNet\n\u2022\nSurya Mattu, Senior Data Engineer and Investigative Data Journalist, The Markup\n\u2022", "cf51300a-fd0f-4a8b-aba6-71b8db414ee5": "About this Document \nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was \npublished by the White House Office of Science and Technology Policy in October 2022. This framework was \nreleased one year after OSTP announced the launch of a process to develop \u201ca bill of rights for an AI-powered \nworld.\u201d Its release follows a year of public engagement to inform this initiative. The framework is available \nonline at: https://www.whitehouse.gov/ostp/ai-bill-of-rights \nAbout the Office of Science and Technology Policy \nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology \nPolicy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office \nof the President with advice on the scientific, engineering, and technological aspects of the economy, national", "d7320b82-d133-49cd-babc-2edfa833533a": "WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nEnsuring accessibility during design, development, and deployment. Systems should be \ndesigned, developed, and deployed by organizations in ways that ensure accessibility to people with disabili\u00ad\nties. This should include consideration of a wide variety of disabilities, adherence to relevant accessibility \nstandards, and user experience research both before and after deployment to identify and address any accessi\u00ad\nbility barriers to the use or effectiveness of the automated system. \nDisparity assessment. Automated systems should be tested using a broad set of measures to assess wheth\u00ad\ner the system components, both in pre-deployment testing and in-context deployment, produce disparities.", "15f67fe3-092e-437b-8fba-86194cda745d": "biases in the generated content. \nInformation Security; Harmful Bias \nand Homogenization \nMG-2.2-005 \nEngage in due diligence to analyze GAI output for harmful content, potential \nmisinformation, and CBRN-related or NCII content. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content", "64d3b194-0d10-43b7-ac48-6b7fd3f5a728": "https://cdt.org/insights/how-automated-test-proctoring-software-discriminates-against-disabled\u00ad\nstudents/\n46. Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of\npopulations, 366 Science (2019), https://www.science.org/doi/10.1126/science.aax2342.\n66", "4a3166f2-fc7d-4645-914c-f71a8fd0fd88": "Inaccuracies in these labels can impact the \u201cstability\u201d or robustness of these benchmarks, which many \nGAI practitioners consider during the model selection process.  \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \nHarmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \n3. \nSuggested Actions to Manage GAI Risks \nThe following suggested actions target risks unique to or exacerbated by GAI. \nIn addition to the suggested actions below, AI risk management activities and actions set forth in the AI \nRMF 1.0 and Playbook are already applicable for managing GAI risks. Organizations are encouraged to \napply the activities suggested in the AI RMF and its Playbook when managing the risk of GAI systems.  \nImplementation of the suggested actions will vary depending on the type of risk, characteristics of GAI \nsystems, stage of the GAI lifecycle, and relevant AI actors involved.", "ef6d2ad8-ebcb-4fa2-bb91-b1ad2693f34a": "Scienti\ufb01c Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): \nConfabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; \nHarmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; \nData Privacy; Human-AI Con\ufb01guration; Obscene, Degrading, and/or Abusive Content; Information Integrity; \nInformation Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual \nProperty. We also note that some risks are cross-cutting between these categories.", "15f1b827-8da3-4e39-9696-1b9a845c86a5": "36. Andrew Thompson. Google\u2019s Sentiment Analyzer Thinks Being Gay Is Bad. Vice. Oct. 25, 2017. https://\nwww.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias\n37. Kaggle. Jigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of\nconversations. 2019. https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification\n38. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and\nMitigating Unintended Bias in Text Classification. Proceedings of AAAI/ACM Conference on AI, Ethics,\nand Society. Feb. 2-3, 2018. https://dl.acm.org/doi/pdf/10.1145/3278721.3278729\n39. Paresh Dave. Google cuts racy results by 30% for searches like 'Latina teenager'. Reuters. Mar. 30,\n2022. https://www.reuters.com/technology/google-cuts-racy-results-by-30-searches-like-latina\u00ad\nteenager-2022-03-30/\n40. Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.", "addaf767-7c86-46b7-80aa-28655ccafbe4": "such strategies might include data-driven approaches, but the focus on safety should be primary, and \ntechnology may or may not be part of an effective set of mechanisms to achieve safety. Various panelists raised \nconcerns about the validity of these systems, the tendency of adverse or irrelevant data to lead to a replication of \nunjust outcomes, and the confirmation bias and tendency of people to defer to potentially inaccurate automated \nsystems. Throughout, many of the panelists individually emphasized that the impact of these systems on \nindividuals and communities is potentially severe: the systems lack individualization and work against the \nbelief that people can change for the better, system use can lead to the loss of jobs and custody of children, and \nsurveillance can lead to chilling effects for communities and sends negative signals to community members \nabout how they're viewed.", "f4065dcf-bf73-4ae2-a6bb-16db538d597b": "BLUEPRINT FOR AN \nAI BILL OF \nRIGHTS \nMAKING AUTOMATED \nSYSTEMS WORK FOR \nTHE AMERICAN PEOPLE \nOCTOBER 2022", "82c714ee-ee32-4528-b3b5-1544183465af": "ever being notified that data was being collected and used as part of an algorithmic child maltreatment\nrisk assessment.84 The lack of notice or an explanation makes it harder for those performing child\nmaltreatment assessments to validate the risk assessment and denies parents knowledge that could help them\ncontest a decision.\n41", "7c72cb92-5744-44ef-963b-8b0601fd1959": "reporting that confirms your data decisions have been respected and provides an assessment of the \npotential impact of surveillance technologies on your rights, opportunities, or access. \nNOTICE AND EXPLANATION\nYou should know that an automated system is being used and understand how and why it \ncontributes to outcomes that impact you. Designers, developers, and deployers of automated systems \nshould provide generally accessible plain language documentation including clear descriptions of the overall \nsystem functioning and the role automation plays, notice that such systems are in use, the individual or organiza\u00ad\ntion responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice \nshould be kept up-to-date and people impacted by the system should be notified of significant use case or key \nfunctionality changes. You should know how and why an outcome impacting you was determined by an", "acf6667f-34ed-46ee-84b0-260cfe802491": "HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nAn automated system should provide demonstrably effective mechanisms to opt out in favor of a human alterna\u00ad\ntive, where appropriate, as well as timely human consideration and remedy by a fallback system, with additional \nhuman oversight and safeguards for systems used in sensitive domains, and with training and assessment for any \nhuman-based portions of the system to ensure effectiveness. \nProvide a mechanism to conveniently opt out from automated systems in favor of a human \nalternative, where appropriate \nBrief, clear, accessible notice and instructions. Those impacted by an automated system should be", "64997920-e3c3-43af-b3d2-8d09925627cc": "both the promises and potential harms of these technologies, and played a central role in shaping the \nBlueprint for an AI Bill of Rights. \nPanel Discussions to Inform the Blueprint for An AI Bill of Rights \nOSTP co-hosted a series of six panel discussions in collaboration with the Center for American Progress, \nthe Joint Center for Political and Economic Studies, New America, the German Marshall Fund, the Electronic \nPrivacy Information Center, and the Mozilla Foundation. The purpose of these convenings \u2013 recordings of \nwhich are publicly available online112 \u2013 was to bring together a variety of experts, practitioners, advocates \nand federal government officials to offer insights and analysis on the risks, harms, benefits, and \npolicy opportunities of automated systems. Each panel discussion was organized around a wide-ranging \ntheme, exploring current challenges and concerns and considering what an automated society that", "0024ba76-72ab-4845-a492-caa62b5e1747": "SAFE AND EFFECTIVE \nSYSTEMS \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \u00ad\nSome U.S government agencies have developed specific frameworks for ethical use of AI \nsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina-\ntion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \nethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence \nEthical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \nsecurity and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles", "1e88bf8f-3a6f-4116-bf99-f76291748b33": "technologies are respected.\n13. National Artificial Intelligence Initiative Office. Agency Inventories of AI Use Cases. Accessed Sept. 8,\n2022. https://www.ai.gov/ai-use-case-inventories/\n14. National Highway Traffic Safety Administration. https://www.nhtsa.gov/\n15. See, e.g., Charles Pruitt. People Doing What They Do Best: The Professional Engineers and NHTSA. Public\nAdministration Review. Vol. 39, No. 4. Jul.-Aug., 1979. https://www.jstor.org/stable/976213?seq=1\n16. The US Department of Transportation has publicly described the health and other benefits of these\n\u201ctraffic calming\u201d measures. See, e.g.: U.S. Department of Transportation. Traffic Calming to Slow Vehicle\nSpeeds. Accessed Apr. 17, 2022. https://www.transportation.gov/mission/health/Traffic-Calming-to-Slow\u00ad\nVehicle-Speeds\n17. Karen Hao. Worried about your firm\u2019s AI ethics? These startups are here to help.\nA growing ecosystem of \u201cresponsible AI\u201d ventures promise to help organizations monitor and fix their AI", "66619ab4-51be-4cf3-9585-49a8251fa615": "(such as where signi\ufb01cant negative impacts are imminent, severe harms are \nactually occurring, or large-scale risks could occur); and broad GAI negative risks, \nincluding: Immature safety or risk cultures related to AI and GAI design, \ndevelopment and deployment, public information integrity risks, including impacts \non democratic processes, unknown long-term performance characteristics of GAI. \nInformation Integrity; Dangerous, \nViolent, or Hateful Content; CBRN \nInformation or Capabilities \nGV-1.3-007 Devise a plan to halt development or deployment of a GAI system that poses \nunacceptable negative risk. \nCBRN Information and Capability; \nInformation Security; Information \nIntegrity \nAI Actor Tasks: Governance and Oversight \n \nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \ncontrols based on organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.4-001", "885c6001-eb5d-4b31-9314-4cca630d58eb": "SAFE AND EFFECTIVE \nSYSTEMS \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \nWhile technologies are being deployed to solve problems across a wide array of issues, our reliance on technology can \nalso lead to its use in situations where it has not yet been proven to work\u2014either at all or within an acceptable range \nof error. In other cases, technologies do not work as intended or as promised, causing substantial and unjustified harm. \nAutomated systems sometimes rely on data from other systems, including historical data, allowing irrelevant informa\u00ad\ntion from past decisions to infect decision-making in unrelated situations.  In some cases, technologies are purposeful\u00ad\nly designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intended \nor unintended uses lead to unintended harms.", "a9e910c3-8eaf-4524-a17f-0d9fa0bba947": "LISTENING TO THE AMERICAN PUBLIC\nThe White House Office of Science and Technology Policy has led a year-long process to seek and distill input \nfrom people across the country\u2014from impacted communities and industry stakeholders to technology develop-\ners and other experts across fields and sectors, as well as policymakers throughout the Federal government\u2014on \nthe issue of algorithmic and data-driven harms and potential remedies. Through panel discussions, public listen-\ning sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicized \nemail address, people throughout the United States, public servants across Federal agencies, and members of the \ninternational community spoke up about both the promises and potential harms of these technologies, and \nplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these", "26e5d365-ee16-4e37-9bfd-22be24c5e1a9": "The Office of Management and Budget (OMB) has called for an expansion of opportunities \nfor meaningful stakeholder engagement in the design of programs and services. OMB also \npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-\nBased Participatory Research Program developed by the National Institutes of Health and the participatory \ntechnology assessments developed by the National Oceanic and Atmospheric Administration.18\nThe National Institute of Standards and Technology (NIST) is developing a risk \nmanagement framework to better manage risks posed to individuals, organizations, and \nsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \nvoluntary use to help incorporate trustworthiness considerations into the design, development, use, and \nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-", "dcdf0c46-6835-45f4-8e33-e93b3790db3d": "the system, should be involved in establishing governance procedures. Responsibility should rest high enough \nin the organization that decisions about resources, mitigation, incident response, and potential rollback can be \nmade promptly, with sufficient weight given to risk mitigation objectives against competing concerns. Those \nholding this responsibility should be made aware of any use cases with the potential for meaningful impact on \npeople\u2019s rights, opportunities, or access as determined based on risk identification procedures.  In some cases, \nit may be appropriate for an independent ethics review to be conducted before deployment. \nAvoid inappropriate, low-quality, or irrelevant data use and the compounded harm of its \nreuse \nRelevant and high-quality data. Data used as part of any automated system\u2019s creation, evaluation, or \ndeployment should be relevant, of high quality, and tailored to the task at hand. Relevancy should be", "f8563a71-e404-4d29-86ec-63b4d95a1b9a": "the development and use of GAI are de\ufb01ned below.5 Each risk is labeled according to the outcome, \nobject, or source of the risk (i.e., some are risks \u201cto\u201d a subject or domain and others are risks \u201cof\u201d or \n\u201cfrom\u201d an issue or theme). These risks provide a lens through which organizations can frame and execute \nrisk management e\ufb00orts. To help streamline risk management e\ufb00orts, each risk is mapped in Section 3 \n(as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identi\ufb01ed in the AI RMF.  \n \n \n5 These risks can be further categorized by organizations depending on their unique approaches to risk de\ufb01nition \nand management. One possible way to further categorize these risks, derived in part from the UK\u2019s International \nScienti\ufb01c Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): \nConfabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration;", "d656cb70-6a01-42da-9ff8-447ad3efc799": "WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nDemonstrate that the system protects against algorithmic discrimination \nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow \nindependent evaluation of potential algorithmic discrimination caused by automated systems they use or \noversee. In the case of public sector uses, these independent evaluations should be made public unless law \nenforcement or national security restrictions prevent doing so. Care should be taken to balance individual \nprivacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and \ncontrols allow access to such data without compromising privacy.", "a1449222-e53d-41c4-9a7d-64c24fb004e5": "card access system fails. \nIn the criminal justice system, employment, education, healthcare, and other sensitive domains, automated systems \nare used for many purposes, from pre-trial risk assessments and parole decisions to technologies that help doctors \ndiagnose disease. Absent appropriate safeguards, these technologies can lead to unfair, inaccurate, or dangerous \noutcomes. These sensitive domains require extra protections. It is critically important that there is extensive human \noversight in such settings. \nThese critical protections have been adopted in some scenarios. Where automated systems have been introduced to \nprovide the public access to government benefits, existing human paper and phone-based processes are generally still \nin place, providing an important alternative to ensure access. Companies that have introduced automated call centers \noften retain the option of dialing zero to reach an operator. When automated identity controls are in place to board an", "15c61fc1-8c4f-4452-b6f2-127bdb367650": "21. Department of Defense. U.S Department of Defense Responsible Artificial Intelligence Strategy and\nImplementation Pathway. Jun. 2022. https://media.defense.gov/2022/Jun/22/2003022604/-1/-1/0/\nDepartment-of-Defense-Responsible-Artificial-Intelligence-Strategy-and-Implementation\u00ad\nPathway.PDF\n22. Director of National Intelligence. Principles of Artificial Intelligence Ethics for the Intelligence\nCommunity. https://www.dni.gov/index.php/features/2763-principles-of-artificial-intelligence-ethics-for\u00ad\nthe-intelligence-community\n64", "dd023346-71f3-4f53-8861-8768d815c002": "models-explained-part-1/ \nCanadian Centre for Cyber Security (2023) Generative arti\ufb01cial intelligence (AI) - ITSAP.00.041. \nhttps://www.cyber.gc.ca/en/guidance/generative-arti\ufb01cial-intelligence-ai-itsap00041 \nCarlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix. \nhttps://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting \nCarlini, N. et al. (2023) Quantifying Memorization Across Neural Language Models. ICLR 2023. \nhttps://arxiv.org/pdf/2202.07646 \nCarlini, N. et al. (2024) Stealing Part of a Production Language Model. arXiv. \nhttps://arxiv.org/abs/2403.06634 \nChandra, B. et al. (2023) Dismantling the Disinformation Business of Chinese In\ufb02uence Operations. \nRAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\nchinese.html \nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika.", "cf569b91-d97e-4871-b54f-690ba02f4707": "humans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \nassess validity or reliability risks.  \nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \ndatasets and in silico techniques that may not extrapolate well to\u2014or directly assess GAI impacts in real-\nworld conditions. For example, current measurement gaps for GAI make it di\ufb03cult to precisely estimate \nits potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \nsensitivity and broad heterogeneity of contexts of use. \nA.1.5. Structured Public Feedback", "e2ffcc20-0808-4994-8d78-18f3ab52d874": "be appropriate to take steps to mitigate or eliminate the disparity. In some cases, mitigation or elimination of \nthe disparity may be required by law. \nDisparities that have the potential to lead to algorithmic \ndiscrimination, cause meaningful harm, or violate equity49 goals should be mitigated. When designing and \nevaluating an automated system, steps should be taken to evaluate multiple models and select the one that \nhas the least adverse impact, modify data input choices, or otherwise identify a system with fewer \ndisparities. If adequate mitigation of the disparity is not possible, then the use of the automated system \nshould be reconsidered. One of the considerations in whether to use the system should be the validity of any \ntarget measure; unobservable targets may result in the inappropriate use of proxies. Meeting these \nstandards may require instituting mitigation procedures and other protective measures to address", "72f04af9-1c06-4d2f-ad95-9072a7fa2c3c": "therefore does not require compliance with the principles described herein. It also is not determinative of what \nthe U.S. government\u2019s position will be in any international negotiation. Adoption of these principles may not \nmeet the requirements of existing statutes, regulations, policies, or international instruments, or the \nrequirements of the Federal agencies that enforce them. These principles are not intended to, and do not, \nprohibit or limit any lawful activity of a government agency, including law enforcement, national security, or \nintelligence activities. \nThe appropriate application of the principles set forth in this white paper depends significantly on the \ncontext in which automated systems are being utilized. In some circumstances, application of these principles \nin whole or in part may not be appropriate given the intended use of automated systems to achieve government", "bab17963-2f97-4633-ad8e-0405e7bf0213": "the National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable \nAI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe \nautonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 \nprogram supports research on cybersecurity and privacy enhancing technologies in automated systems; the \nFormal Methods in the Field26 program supports research on rigorous formal verification and analysis of \nautomated systems and machine learning, and the Designing Accountable Software Systems27 program supports \nresearch on rigorous and reproducible methodologies for developing software systems with legal and regulatory \ncompliance in mind. \nSome state legislatures have placed strong transparency and validity requirements on \nthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a", "5d3f7e37-0ab4-47fa-b0b8-330a17b35163": "and quantitative evaluations of the system. This equity assessment should also be considered a core part of the \ngoals of the consultation conducted as part of the safety and efficacy review. \nRepresentative and robust data. Any data used as part of system development or assessment should be \nrepresentative of local communities based on the planned deployment setting and should be reviewed for bias \nbased on the historical and societal context of the data. Such data should be sufficiently robust to identify and \nhelp to mitigate biases and potential harms. \nGuarding against proxies.  Directly using demographic information in the design, development, or \ndeployment of an automated system (for purposes other than evaluating a system for discrimination or using \na system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should be \navoided. In many cases, attributes that are highly correlated with demographic features, known as proxies, can", "04de3548-c169-4f3b-915b-78dad74ae71c": "provenance, the number of unauthorized access attempts, inference, bypass, \nextraction, penetrations, or provenance veri\ufb01cation. \nInformation Integrity; Information \nSecurity \nMS-2.7-005 \nMeasure reliability of content authentication methods, such as watermarking, \ncryptographic signatures, digital \ufb01ngerprints, as well as access controls, \nconformity assessment, and model integrity veri\ufb01cation, which can help support \nthe e\ufb00ective implementation of content provenance techniques. Evaluate the \nrate of false positives and false negatives in content provenance, as well as true \npositives and true negatives for veri\ufb01cation. \nInformation Integrity \nMS-2.7-006 \nMeasure the rate at which recommendations from security checks and incidents \nare implemented. Assess how quickly the AI system can adapt and improve \nbased on lessons learned from security incidents and feedback. \nInformation Integrity; Information \nSecurity \nMS-2.7-007", "9c980d10-927d-46ae-a49c-657484e24a3d": "compliance in mind. \nSome state legislatures have placed strong transparency and validity requirements on \nthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a \ncause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any \npretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of \nindividuals protected from discrimination by state or federal law\", that any locality using a pretrial risk \nassessment must first formally validate the claim of its being free of bias, that \"all documents, records, and \ninformation used to build or validate the risk assessment shall be open to public inspection,\" and that assertions \nof trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\" \n22", "c58df9b0-8568-4fb9-b7f7-9a54396f10ce": "APPENDIX\nSystems that impact the safety of communities such as automated traffic control systems, elec \n-ctrical grid controls, smart city technologies, and industrial emissions and environmental\nimpact control algorithms; and\nSystems related to access to benefits or services or assignment of penalties such as systems that\nsupport decision-makers who adjudicate benefits such as collating or analyzing information or\nmatching records, systems which similarly assist in the adjudication of administrative or criminal\npenalties, fraud detection algorithms, services or benefits access control algorithms, biometric\nsystems used as access control, and systems which make benefits or services related decisions on a\nfully or partially autonomous basis (such as a determination to revoke benefits).\n54", "1cc15760-d963-4710-81b1-eab3d7f22ebc": "for an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 \non Advancing Racial Equity and Support for Underserved Communities Through the Federal Government. \nThese principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report \nof an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers, \nand the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these core \nprinciples for managing information about individuals have been incorporated into data privacy laws and \npolicies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are \nparticularly relevant to automated systems, without articulating a specific set of FIPPs or scoping \napplicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties,", "29ccc988-39a2-495c-9b9b-db6d0f698e52": "1 \n1. \nIntroduction \nThis document is a cross-sectoral pro\ufb01le of and companion resource for the AI Risk Management \nFramework (AI RMF 1.0) for Generative AI,1 pursuant to President Biden\u2019s Executive Order (EO) 14110 on \nSafe, Secure, and Trustworthy Arti\ufb01cial Intelligence.2 The AI RMF was released in January 2023, and is \nintended for voluntary use and to improve the ability of organizations to incorporate trustworthiness \nconsiderations into the design, development, use, and evaluation of AI products, services, and systems.  \nA pro\ufb01le is an implementation of the AI RMF functions, categories, and subcategories for a speci\ufb01c \nsetting, application, or technology \u2013 in this case, Generative AI (GAI) \u2013 based on the requirements, risk \ntolerance, and resources of the Framework user. AI RMF pro\ufb01les assist organizations in deciding how to \nbest manage AI risks in a manner that is well-aligned with their goals, considers legal/regulatory", "e2f8e10d-59e9-4db2-a953-5ebfe27bc88d": "ing a system from use. Automated systems should not be designed \nwith an intent or reasonably foreseeable possibility of endangering \nyour safety or the safety of your community. They should be designed \nto proactively protect you from harms stemming from unintended, \nyet foreseeable, uses or impacts of automated systems. You should be \nprotected from inappropriate or irrelevant data use in the design, de\u00ad\nvelopment, and deployment of automated systems, and from the \ncompounded harm of its reuse. Independent evaluation and report\u00ad\ning that confirms that the system is safe and effective, including re\u00ad\nporting of steps taken to mitigate potential harms, should be per\u00ad\nformed and the results made public whenever possible. \n15", "77cbc32d-fd24-4d74-9328-fb8117b79b5a": "GAI Risks \nGV4.3--001 \nEstablish policies for measuring the e\ufb00ectiveness of employed content \nprovenance methodologies (e.g., cryptography, watermarking, steganography, \netc.) \nInformation Integrity \nGV-4.3-002 \nEstablish organizational practices to identify the minimum set of criteria \nnecessary for GAI system incident reporting such as: System ID (auto-generated \nmost likely), Title, Reporter, System/Source, Data Reported, Date of Incident, \nDescription, Impact(s), Stakeholder(s) Impacted. \nInformation Security", "d2f0639d-87a7-4f85-a092-1986c7b71a08": "2022.\nhttps://www.ncsl.org/research/elections-and-campaigns/vopp-table-15-states-that-permit-voters-to\u00ad\ncorrect-signature-discrepancies.aspx\n112. White House Office of Science and Technology Policy. Join the Effort to Create A Bill of Rights for\nan Automated Society. Nov. 10, 2021.\nhttps://www.whitehouse.gov/ostp/news-updates/2021/11/10/join-the-effort-to-create-a-bill-of\u00ad\nrights-for-an-automated-society/\n113. White House Office of Science and Technology Policy. Notice of Request for Information (RFI) on\nPublic and Private Sector Uses of Biometric Technologies. Issued Oct. 8, 2021.\nhttps://www.federalregister.gov/documents/2021/10/08/2021-21975/notice-of-request-for\u00ad\ninformation-rfi-on-public-and-private-sector-uses-of-biometric-technologies\n114. National Artificial Intelligence Initiative Office. Public Input on Public and Private Sector Uses of\nBiometric Technologies. Accessed Apr. 19, 2022.\nhttps://www.ai.gov/86-fr-56300-responses/", "cdc2d327-aa60-4db1-8925-eb5a85c318bd": "potentially deceiving humans into believing they are speaking with another human. \nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \nrange of downstream impacts of GAI, it is di\ufb03cult to estimate the downstream scale and impact of \nconfabulations. \nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \nand Interpretable \n2.3. Dangerous, Violent, or Hateful Content \nGAI systems can produce content that is inciting, radicalizing, or threatening, or that glori\ufb01es violence, \nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \nviolent recommendations, and some models have generated actionable instructions for dangerous or \n \n \n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video", "2454b84e-7842-4241-a90f-8a8db64f9d43": "40 \nMANAGE 1.3: Responses to the AI risks deemed high priority, as identi\ufb01ed by the MAP function, are developed, planned, and \ndocumented. Risk response options can include mitigating, transferring, avoiding, or accepting. \nAction ID \nSuggested Action \nGAI Risks \nMG-1.3-001 \nDocument trade-o\ufb00s, decision processes, and relevant measurement and \nfeedback results for risks that do not surpass organizational risk tolerance, for \nexample, in the context of model release: Consider di\ufb00erent approaches for \nmodel release, for example, leveraging a staged release approach. Consider \nrelease approaches in the context of the model and its projected use cases. \nMitigate, transfer, or avoid risks that surpass organizational risk tolerances. \nInformation Security \nMG-1.3-002 \nMonitor the robustness and e\ufb00ectiveness of risk controls and mitigation plans \n(e.g., via red-teaming, \ufb01eld testing, participatory engagements, performance \nassessments, user feedback mechanisms). \nHuman-AI Con\ufb01guration", "f1c6beca-4fdd-4b12-9c6d-7e24a26bcad2": "44 \nMG-3.2-007 \nLeverage feedback and recommendations from organizational boards or \ncommittees related to the deployment of GAI applications and content \nprovenance when using third-party pre-trained models. \nInformation Integrity; Value Chain \nand Component Integration \nMG-3.2-008 \nUse human moderation systems where appropriate to review generated content \nin accordance with human-AI con\ufb01guration policies established in the Govern \nfunction, aligned with socio-cultural norms in the context of use, and for settings \nwhere AI models are demonstrated to perform poorly. \nHuman-AI Con\ufb01guration \nMG-3.2-009 \nUse organizational risk tolerance to evaluate acceptable risks and performance \nmetrics and decommission or retrain pre-trained models that perform outside of \nde\ufb01ned limits. \nCBRN Information or Capabilities; \nConfabulation \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities", "33da7cae-ae21-496b-8026-a45ac0a5d140": "https://openreview.net/forum?id=aX8ig9X2a7 \nKleinberg, J. et al. (May 2021) Algorithmic monoculture and social welfare. PNAS. \nhttps://www.pnas.org/doi/10.1073/pnas.2018340118 \nLakatos, S. (2023) A Revealing Picture. Graphika. https://graphika.com/reports/a-revealing-picture \nLee, H. et al. (2024) Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks. \narXiv. https://arxiv.org/pdf/2310.07879 \nLenaerts-Bergmans, B. (2024) Data Poisoning: The Exploitation of Generative AI. Crowdstrike. \nhttps://www.crowdstrike.com/cybersecurity-101/cyberattacks/data-poisoning/ \nLiang, W. et al. (2023) GPT detectors are biased against non-native English writers. arXiv. \nhttps://arxiv.org/abs/2304.02819 \nLuccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \nhttps://arxiv.org/pdf/2311.16863 \nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND.", "178bdbaf-625d-4777-aa5a-7f0c51ffcbcd": "ENDNOTES\n85. Mick Dumke and Frank Main. A look inside the watch list Chicago police fought to keep secret. The\nChicago Sun Times. May 18, 2017.\nhttps://chicago.suntimes.com/2017/5/18/18386116/a-look-inside-the-watch-list-chicago-police-fought\u00ad\nto-keep-secret\n86. Jay Stanley. Pitfalls of Artificial Intelligence Decisionmaking Highlighted In Idaho ACLU Case.\nACLU. Jun. 2, 2017.\nhttps://www.aclu.org/blog/privacy-technology/pitfalls-artificial-intelligence-decisionmaking\u00ad\nhighlighted-idaho-aclu-case\n87. Illinois General Assembly. Biometric Information Privacy Act. Effective Oct. 3, 2008.\nhttps://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57\n88. Partnership on AI. ABOUT ML Reference Document. Accessed May 2, 2022.\nhttps://partnershiponai.org/paper/about-ml-reference-document/1/\n89. See, e.g., the model cards framework: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker\nBarnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.", "d578bdf8-490c-4a99-81f1-9baf1338d684": "or lead to algorithmic discrimination. \nOversight. Human-based systems have the potential for bias, including automation bias, as well as other \nconcerns that may limit their effectiveness. The results of assessments of the efficacy and potential bias of \nsuch human-based systems should be overseen by governance structures that have the potential to update the \noperation of the human-based system in order to mitigate these effects. \n50", "9079ebe8-b9c4-422b-bd5b-473bc9792a4d": "MEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are di\ufb03cult to assess using currently available \nmeasurement techniques or where metrics are not yet available. \nAction ID \nSuggested Action \nGAI Risks \nMS-3.2-001 \nEstablish processes for identifying emergent GAI system risks including \nconsulting with external AI Actors. \nHuman-AI Con\ufb01guration; \nConfabulation  \nAI Actor Tasks: AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \n \nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are \nestablished and integrated into AI system evaluation metrics. \nAction ID \nSuggested Action \nGAI Risks \nMS-3.3-001 \nConduct impact assessments on how AI-generated content might a\ufb00ect \ndi\ufb00erent social, economic, and cultural groups. \nHarmful Bias and Homogenization \nMS-3.3-002 \nConduct studies to understand how end users perceive and interact with GAI", "5e2dbc3b-32b2-4158-8255-bfab23a5a84b": "59 \nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \n139-162. https://www.jstor.org/stable/26529441  \nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\ncontent/uploads/2015/08/Tufekci-\ufb01nal.pdf \nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \nPractices. AAAI/ACM Conference on AI, Ethics, and Society. \nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \nUrbina, F. et al. (2022) Dual use of arti\ufb01cial-intelligence-powered drug discovery. Nature Machine \nIntelligence. https://www.nature.com/articles/s42256-022-00465-9 \nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \nhttps://aclanthology.org/2023.\ufb01ndings-emnlp.607.pdf \nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv.", "fa6fde95-3cf3-4931-9167-e045cc46311d": "related to revealing decision-making information, such simplifications should be done in a scientifically \nsupportable way. Where appropriate based on the explanatory system, error ranges for the explanation should \nbe calculated and included in the explanation, with the choice of presentation of such information balanced \nwith usability and overall interface complexity concerns. \nDemonstrate protections for notice and explanation \nReporting. Summary reporting should document the determinations made based on the above consider\u00ad\nations, including: the responsible entities for accountability purposes; the goal and use cases for the system, \nidentified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of \nthe explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment \nof how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of", "9482da44-56b0-47e3-867f-e35af0504811": "https://partnershiponai.org/paper/about-ml-reference-document/1/\n89. See, e.g., the model cards framework: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker\nBarnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.\nModel Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and\nTransparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220\u2013229. https://\ndl.acm.org/doi/10.1145/3287560.3287596\n90. Sarah Ammermann. Adverse Action Notice Requirements Under the ECOA and the FCRA. Consumer\nCompliance Outlook. Second Quarter 2013.\nhttps://consumercomplianceoutlook.org/2013/second-quarter/adverse-action-notice-requirements\u00ad\nunder-ecoa-fcra/\n91. Federal Trade Commission. Using Consumer Reports for Credit Decisions: What to Know About\nAdverse Action and Risk-Based Pricing Notices. Accessed May 2, 2022.\nhttps://www.ftc.gov/business-guidance/resources/using-consumer-reports-credit-decisions-what\u00ad", "c3576568-c7a7-474b-be7d-784a7b6d5a99": "31 \nMS-2.3-004 \nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \nevaluate GAI trustworthy characteristics. \nCBRN Information or Capabilities; \nData Privacy; Confabulation; \nInformation Integrity; Information \nSecurity; Dangerous, Violent, or \nHateful Content; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, TEVV \n \nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \nconditions under which the technology was developed are documented. \nAction ID \nSuggested Action \nRisks \nMS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non-\nsystematic, and anecdotal assessments. \nHuman-AI Con\ufb01guration; \nConfabulation \nMS-2.5-002 \nDocument the extent to which human domain knowledge is employed to \nimprove GAI system performance, via, e.g., RLHF, \ufb01ne-tuning, retrieval-\naugmented generation, content moderation, business rules.", "474752d0-4f05-429a-8eb1-b2e9c2a145fd": "36 \nMEASURE 2.11: Fairness and bias \u2013 as identi\ufb01ed in the MAP function \u2013 are evaluated and results are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.11-001 \nApply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real \nHateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, \nstereotyping, denigration, and hateful content in GAI system outputs; \nDocument assumptions and limitations of benchmarks, including any actual or \npossible training/test data cross contamination, relative to in-context \ndeployment environment. \nHarmful Bias and Homogenization \nMS-2.11-002 \nConduct fairness assessments to measure systemic bias. Measure GAI system \nperformance across demographic groups and subgroups, addressing both \nquality of service and any allocation of services and resources. Quantify harms \nusing: \ufb01eld testing with sub-group populations to determine likelihood of \nexposure to generated content exhibiting harmful bias, AI red-teaming with", "28c283ca-bb93-4ee6-9549-610078737859": "27 \nMP-4.1-010 \nConduct appropriate diligence on training data use to assess intellectual property, \nand privacy, risks, including to examine whether use of proprietary or sensitive \ntraining data is consistent with applicable laws.  \nIntellectual Property; Data Privacy \nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities \n \nMAP 5.1: Likelihood and magnitude of each identi\ufb01ed impact (both potentially bene\ufb01cial and harmful) based on expected use, past \nuses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed \nthe AI system, or other data are identi\ufb01ed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities. \nInformation Integrity; Information \nSecurity \nMP-5.1-002", "191380bc-73da-4332-ae19-e34a3531f0a6": "Human-AI Con\ufb01guration; \nInformation Security \nGV-1.5-003 \nMaintain a document retention policy to keep history for test, evaluation, \nvalidation, and veri\ufb01cation (TEVV), and digital content transparency methods for \nGAI. \nInformation Integrity; Intellectual \nProperty \nAI Actor Tasks: Governance and Oversight, Operation and Monitoring \n \nGOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.6-001 Enumerate organizational GAI systems for incorporation into AI system inventory \nand adjust AI system inventory requirements to account for GAI risks. \nInformation Security \nGV-1.6-002 De\ufb01ne any inventory exemptions in organizational policies for GAI systems \nembedded into application software. \nValue Chain and Component \nIntegration \nGV-1.6-003 \nIn addition to general model, governance, and risk information, consider the", "91f77135-1c3e-48d6-8582-59cbd51aa3b7": "Sep. 18, 2015.\nhttps://www.metrotimes.com/news/uia-lawsuit-shows-how-the-state-criminalizes-the\u00ad\nunemployed-2369412\n103. Maia Szalavitz. The Pain Was Unbearable. So Why Did Doctors Turn Her Away? Wired. Aug. 11,\n2021. https://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/\n104. Spencer Soper. Fired by Bot at Amazon: \"It's You Against the Machine\". Bloomberg, Jun. 28, 2021.\nhttps://www.bloomberg.com/news/features/2021-06-28/fired-by-bot-amazon-turns-to-machine\u00ad\nmanagers-and-workers-are-losing-out\n105. Definitions of \u2018equity\u2019 and \u2018underserved communities\u2019 can be found in the Definitions section of\nthis document as well as in Executive Order on Advancing Racial Equity and Support for Underserved\nCommunities Through the Federal Government:\nhttps://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive-order\u00ad\nadvancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/", "54777858-c093-4e16-b64c-d83f8c91384f": "concrete steps that can be taken by many kinds of organizations\u2014from governments at all levels to companies of \nall sizes\u2014to uphold these values. Experts from across the private sector, governments, and international \nconsortia have published principles and frameworks to guide the responsible use of automated systems; this \nframework provides a national values statement and toolkit that is sector-agnostic to inform building these \nprotections into policy, practice, or the technological design process.  Where existing law or policy\u2014such as \nsector-specific privacy laws and oversight requirements\u2014do not already provide guidance, the Blueprint for an \nAI Bill of Rights should be used to inform policy decisions.\nLISTENING TO THE AMERICAN PUBLIC\nThe White House Office of Science and Technology Policy has led a year-long process to seek and distill input \nfrom people across the country\u2014from impacted communities and industry stakeholders to technology develop-", "6a960312-dd8d-4c68-9c02-e82e1faea9f5": "used. Systems should not employ user experience and design decisions that obfuscate user choice or burden \nusers with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases \nwhere it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable \nin plain language, and give you agency over data collection and the specific context of use; current hard-to\u00ad\nunderstand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and \nrestrictions for data and inferences related to sensitive domains, including health, work, education, criminal \njustice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data and \nrelated inferences should only be used for necessary functions, and you should be protected by ethical review \nand use prohibitions. You and your communities should be free from unchecked surveillance; surveillance", "ebd3b826-524a-47fc-ad2a-e1da632b69c5": "and red-teaming of GAI systems; GAI content moderation; GAI system \ndevelopment and engineering; Increased accessibility of GAI tools, interfaces, and \nsystems, Incident response and containment. \nHuman-AI Con\ufb01guration; \nInformation Security; Harmful Bias \nand Homogenization \nGV-3.2-003 \nDe\ufb01ne acceptable use policies for GAI interfaces, modalities, and human-AI \ncon\ufb01gurations (i.e., for chatbots and decision-making tasks), including criteria for \nthe kinds of queries GAI applications should refuse to respond to.  \nHuman-AI Con\ufb01guration \nGV-3.2-004 \nEstablish policies for user feedback mechanisms for GAI systems which include \nthorough instructions and any mechanisms for recourse. \nHuman-AI Con\ufb01guration  \nGV-3.2-005 \nEngage in threat modeling to anticipate potential risks from GAI systems. \nCBRN Information or Capabilities; \nInformation Security \nAI Actors: AI Design", "9c43c322-dfc9-4f06-a164-d6b37a9eca0f": "models; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after \ufb01ne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization \ncompliance; geopolitical alignment). \nData Privacy; Information Security; \nValue Chain and Component \nIntegration; Harmful Bias and \nHomogenization \nMG-3.1-003 \nRe-assess model risks after \ufb01ne-tuning or retrieval-augmented generation \nimplementation and for any third-party GAI models deployed for applications \nand/or use cases that were not evaluated in initial testing. \nValue Chain and Component \nIntegration \nMG-3.1-004 \nTake reasonable measures to review training data for CBRN information, and \nintellectual property, and where appropriate, remove it. Implement reasonable", "1d950592-0348-48b5-89a3-8ce33a2ca43c": "NOTICE & \nEXPLANATION \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nTailored to the level of risk. An assessment should be done to determine the level of risk of the auto\u00ad\nmated system. In settings where the consequences are high as determined by a risk assessment, or extensive \noversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should \nbe built into the system design so that the system\u2019s full behavior can be explained in advance (i.e., only fully \ntransparent models should be used), rather than as an after-the-decision interpretation. In other settings, the \nextent of explanation provided should be tailored to the risk level.", "d846bb0a-5558-4aca-91ca-962e6d1b5023": "ing as a check in the event there are shortcomings in automated monitoring systems. These monitoring proce\u00ad\ndures should be in place for the lifespan of the deployed automated system. \nClear organizational oversight. Entities responsible for the development or use of automated systems \nshould lay out clear governance structures and procedures.  This includes clearly-stated governance proce\u00ad\ndures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoing \nassessment and mitigation. Organizational stakeholders including those with oversight of the business process \nor operation being automated, as well as other organizational divisions that may be affected due to the use of \nthe system, should be involved in establishing governance procedures. Responsibility should rest high enough \nin the organization that decisions about resources, mitigation, incident response, and potential rollback can be", "042e40bc-137f-4663-9552-54961f9f757e": "NOTICE & \nEXPLANATION \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \u00ad\u00ad\u00ad\u00ad\u00ad\nPeople in Illinois are given written notice by the private sector if their biometric informa-\ntion is used. The Biometric Information Privacy Act enacted by the state contains a number of provisions \nconcerning the use of individual biometric data and identifiers. Included among them is a provision that no private \nentity may \"collect, capture, purchase, receive through trade, or otherwise obtain\" such information about an \nindividual, unless written notice is provided to that individual or their legally appointed representative. 87\nMajor technology companies are piloting new ways to communicate with the public about", "ce6be881-7637-4e38-ba95-44c2402d4eb5": "52 \n\u2022 \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n\u2022 \nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \nmaking tasks informed by GAI content), and how they react to applied provenance techniques \nsuch as overt disclosures. \nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \nprovenance data may be most useful. For instance, GAI systems used for content creation may require \nrobust watermarking techniques and corresponding detectors to identify the source of content or \nmetadata recording techniques and metadata management tools and repositories to trace content \norigins and modi\ufb01cations. Further narrowing of GAI task de\ufb01nitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management e\ufb00orts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback", "95ba3a07-5283-4935-9b69-446659d8cab3": "returns seized without any chance to explain themselves or receive a review by a person.102\n\u2022 A patient was wrongly denied access to pain medication when the hospital\u2019s software confused her medica\u00ad\ntion history with that of her dog\u2019s. Even after she tracked down an explanation for the problem, doctors\nwere afraid to override the system, and she was forced to go without pain relief due to the system\u2019s error.103\n\u2022 A large corporation automated performance evaluation and other HR functions, leading to workers being\nfired by an automated system without the possibility of human review, appeal or other form of recourse.104 \n48", "b547593a-a9df-4f8e-9881-2c5478a8e89f": "30 \nMEASURE 2.2: Evaluations involving human subjects meet applicable requirements (including human subject protection) and are \nrepresentative of the relevant population. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.2-001 Assess and manage statistical biases related to GAI content provenance through \ntechniques such as re-sampling, re-weighting, or adversarial training. \nInformation Integrity; Information \nSecurity; Harmful Bias and \nHomogenization \nMS-2.2-002 \nDocument how content provenance data is tracked and how that data interacts \nwith privacy and security. Consider: Anonymizing data to protect the privacy of \nhuman subjects; Leveraging privacy output \ufb01lters; Removing any personally \nidenti\ufb01able information (PII) to prevent potential harm or misuse. \nData Privacy; Human AI \nCon\ufb01guration; Information \nIntegrity; Information Security; \nDangerous, Violent, or Hateful \nContent \nMS-2.2-003 Provide human subjects with options to withdraw participation or revoke their", "d6a6f1b3-f544-43d6-b1be-89747c0706c2": "MP-5.1-003 \nConsider disclosing use of GAI to end users in relevant contexts, while considering \nthe objective of disclosure, the context of use, the likelihood and magnitude of the \nrisk posed, the audience of the disclosure, as well as the frequency of the \ndisclosures. \nHuman-AI Con\ufb01guration \nMP-5.1-004 Prioritize GAI structured public feedback processes based on risk assessment \nestimates. \nInformation Integrity; CBRN \nInformation or Capabilities; \nDangerous, Violent, or Hateful \nContent; Harmful Bias and \nHomogenization \nMP-5.1-005 Conduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \nidentify anomalous or unforeseen failure modes. \nInformation Security \nMP-5.1-006 \nPro\ufb01le threats and negative impacts arising from GAI systems interacting with, \nmanipulating, or generating content, and outlining known and potential \nvulnerabilities and the likelihood of their occurrence. \nInformation Security", "e13f8677-dc1f-427b-9c01-85ea077d0a7c": "human-based portions of the system to ensure effectiveness. \nProvide a mechanism to conveniently opt out from automated systems in favor of a human \nalternative, where appropriate \nBrief, clear, accessible notice and instructions. Those impacted by an automated system should be \ngiven a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out. \nInstructions should be provided in an accessible form and should be easily findable by those impacted by the \nautomated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g., \nvia user experience research). \nHuman alternatives provided when appropriate. In many scenarios, there is a reasonable expectation \nof human involvement in attaining rights, opportunities, or access. When automated systems make up part of \nthe attainment process, alternative timely human-driven processes should be provided. The use of a human", "0e74ae24-e850-4843-8bd0-46212443df76": "control over the various trade-o\ufb00s and cascading impacts of early-stage model decisions on downstream \nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \ncomplexity (the resources required to implement watermarking). Organizational risk management \ne\ufb00orts for enhancing content provenance include:  \n\u2022 \nTracking provenance of training data and metadata for GAI systems; \n\u2022 \nDocumenting provenance data limitations within GAI systems;", "c33626cb-a5bf-4fd8-b0f9-243a9d1e6c24": "Technology Engagement Center \nUber Technologies \nUniversity of Pittsburgh \nUndergraduate Student \nCollaborative \nUpturn \nUS Technology Policy Committee \nof the Association of Computing \nMachinery \nVirginia Puccio \nVisar Berisha and Julie Liss \nXR Association \nXR Safety Initiative \n\u2022 As an additional effort to reach out to stakeholders regarding the RFI, OSTP conducted two listening sessions\nfor members of the public. The listening sessions together drew upwards of 300 participants. The Science and\nTechnology Policy Institute produced a synopsis of both the RFI submissions and the feedback at the listening\nsessions.115\n61", "ca68c07f-0dc2-43aa-af85-b842e1c8a93e": "Panelists: \n\u2022\nSean Malinowski, Director of Policing Innovation and Reform, University of Chicago Crime Lab\n\u2022\nKristian Lum, Researcher\n\u2022\nJumana Musa, Director, Fourth Amendment Center, National Association of Criminal Defense Lawyers\n\u2022\nStanley Andrisse, Executive Director, From Prison Cells to PHD; Assistant Professor, Howard University\nCollege of Medicine\n\u2022\nMyaisha Hayes, Campaign Strategies Director, MediaJustice\nPanelists discussed uses of technology within the criminal justice system, including the use of predictive \npolicing, pretrial risk assessments, automated license plate readers, and prison communication tools. The \ndiscussion emphasized that communities deserve safety, and strategies need to be identified that lead to safety; \nsuch strategies might include data-driven approaches, but the focus on safety should be primary, and \ntechnology may or may not be part of an effective set of mechanisms to achieve safety. Various panelists raised", "aa66b36f-05f3-4224-abb7-ae217524332b": "From large companies to start-ups, industry is providing innovative solutions that allow \norganizations to mitigate risks to the safety and efficacy of AI systems, both before \ndeployment and through monitoring over time.17 These innovative solutions include risk \nassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \nmonitoring, documentation procedures specific to model assessments, and many other strategies that aim to \nmitigate risks posed by the use of AI to companies\u2019 reputation, legal responsibilities, and other product safety \nand effectiveness concerns. \nThe Office of Management and Budget (OMB) has called for an expansion of opportunities \nfor meaningful stakeholder engagement in the design of programs and services. OMB also \npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-", "393cbcfc-3eb0-423e-ad1f-4dafabe76b34": "apply the activities suggested in the AI RMF and its Playbook when managing the risk of GAI systems.  \nImplementation of the suggested actions will vary depending on the type of risk, characteristics of GAI \nsystems, stage of the GAI lifecycle, and relevant AI actors involved.  \nSuggested actions to manage GAI risks can be found in the tables below: \n\u2022 \nThe suggested actions are organized by relevant AI RMF subcategories to streamline these \nactivities alongside implementation of the AI RMF.  \n\u2022 \nNot every subcategory of the AI RMF is included in this document.13 Suggested actions are \nlisted for only some subcategories.  \n \n \n13 As this document was focused on the GAI PWG e\ufb00orts and primary considerations (see Appendix A), AI RMF \nsubcategories not addressed here may be added later.", "90a1f5d6-9a78-4491-8ebc-2e7bc1756dc5": "Know About AB 701. Zaller Law Group California Employment Law Report. Sept. 24, 2021.\nhttps://www.californiaemploymentlawreport.com/2021/09/california-passes-law-regulating-quotas\u00ad\nin-warehouses-what-employers-need-to-know-about-ab-701/\n94. National Institute of Standards and Technology. AI Fundamental Research \u2013 Explainability.\nAccessed Jun. 4, 2022.\nhttps://www.nist.gov/artificial-intelligence/ai-fundamental-research-explainability\n95. DARPA. Explainable Artificial Intelligence (XAI). Accessed July 20, 2022.\nhttps://www.darpa.mil/program/explainable-artificial-intelligence\n71", "0f26971c-5936-4a44-8cbb-787131e527c2": "6 \n2.2. Confabulation \n\u201cConfabulation\u201d refers to a phenomenon in which GAI systems generate and con\ufb01dently present \nerroneous or false content in response to prompts. Confabulations also include generated outputs that \ndiverge from the prompts or other input or that contradict previously generated statements in the same \ncontext. These phenomena are colloquially also referred to as \u201challucinations\u201d or \u201cfabrications.\u201d \nConfabulations can occur across GAI outputs and contexts.9,10 Confabulations are a natural result of the \nway generative models are designed: they generate outputs that approximate the statistical distribution \nof their training data; for example, LLMs predict the next token or word in a sentence or phrase. While \nsuch statistical prediction can produce factually accurate and consistent outputs, it can also produce \noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when", "ecbc1513-3de2-42fc-b21f-e591a7ca1359": "non-technical stakeholders understanding of GAI system functionality. \nHuman-AI Con\ufb01guration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, A\ufb00ected Individuals and Communities, End-Users, Operation and \nMonitoring, TEVV \n \nMANAGE 4.3: Incidents and errors are communicated to relevant AI Actors, including a\ufb00ected communities. Processes for tracking, \nresponding to, and recovering from incidents and errors are followed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.3-001 \nConduct after-action assessments for GAI system incidents to verify incident \nresponse and recovery processes are followed and e\ufb00ective, including to follow \nprocedures for communicating incidents to relevant AI Actors and where \napplicable, relevant legal and regulatory bodies.  \nInformation Security \nMG-4.3-002 Establish and maintain policies and procedures to record and track GAI system \nreported errors, near-misses, and negative impacts. \nConfabulation; Information \nIntegrity", "eb240bfb-5213-4b97-9193-c24ccd04989a": "Provide enhanced protections for data related to sensitive domains \nNecessary functions only. Sensitive data should only be used for functions strictly necessary for that \ndomain or for functions that are required for administrative reasons (e.g., school attendance records), unless \nconsent is acquired, if appropriate, and the additional expectations in this section are met. Consent for non-\nnecessary functions should be optional, i.e., should not be required, incentivized, or coerced in order to \nreceive opportunities or access to services. In cases where data is provided to an entity (e.g., health insurance \ncompany) in order to facilitate payment for such a need, that data should only be used for that purpose. \nEthical review and use prohibitions. Any use of sensitive data or decision process based in part on sensi-\ntive data that might limit rights, opportunities, or access, whether the decision is automated or not, should go", "59d661c7-7ed8-4a0b-b32b-6b55c13461a6": "HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \n\u2022\nAn unemployment benefits system in Colorado required, as a condition of accessing benefits, that applicants\nhave a smartphone in order to verify their identity. No alternative human option was readily available,\nwhich denied many people access to benefits.101\n\u2022\nA fraud detection system for unemployment insurance distribution incorrectly flagged entries as fraudulent,\nleading to people with slight discrepancies or complexities in their files having their wages withheld and tax\nreturns seized without any chance to explain themselves or receive a review by a person.102\n\u2022 A patient was wrongly denied access to pain medication when the hospital\u2019s software confused her medica\u00ad", "8acec6c1-fa7a-40d7-900f-1fe306e07aa6": "analytics in order to build profiles or infer personal information about individuals; and \nAny system that has the meaningful potential to lead to algorithmic discrimination. \n\u2022 Equal opportunities, including but not limited to:\nEducation-related systems such as algorithms that purport to detect student cheating or  \n    plagiarism, admissions algorithms, online or virtual reality student monitoring systems,  \nprojections of student progress or outcomes, algorithms that determine access to resources or  \n    rograms, and surveillance of classes (whether online or in-person); \nHousing-related systems such as tenant screening algorithms, automated valuation systems that  \n    estimate the value of homes used in mortgage underwriting or home insurance, and automated  \n    valuations from online aggregator websites; and \nEmployment-related systems such as workplace algorithms that inform all aspects of the terms", "1498c5ba-3791-49a5-867b-73b92217b4ed": "the federal government may use AI, as opposed to private sector use of AI\u2014require that AI is: (a) lawful and \nrespectful of our Nation\u2019s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \nsafe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar-\nent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \nAffected agencies across the federal government have released AI use case inventories13 and are implementing \nplans to bring those AI systems into compliance with the Executive Order or retire them. \nThe law and policy landscape for motor vehicles shows that strong safety regulations\u2014and \nmeasures to address harms when they occur\u2014can enhance innovation in the context of com-\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components.", "42b9801b-79f6-4a4b-896b-a05f7e6f938a": "54 \nAppendix B. References \nAcemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \nAI Incident Database. https://incidentdatabase.ai/ \nAtherton, D. (2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses. \nAI Incident Database. https://incidentdatabase.ai/blog/deepfakes-and-child-safety/ \nBadyal, N. et al. (2023) Intentional Biases in LLM Responses. arXiv. https://arxiv.org/pdf/2311.07611 \nBing Chat: Data Ex\ufb01ltration Exploit Explained. Embrace The Red. \nhttps://embracethered.com/blog/posts/2023/bing-chat-data-ex\ufb01ltration-poc-and-\ufb01x/ \nBommasani, R. et al. (2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome \nHomogenization? arXiv. https://arxiv.org/pdf/2211.13972 \nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \nDeployment. arXiv. https://arxiv.org/pdf/2011.13416 \nBrowne, D. et al. (2023) Securing the AI Pipeline. Mandiant.", "3f06930f-34ae-465f-9127-8a0510821b2c": "intelligence and provides examples of how and why it can chip away at public trust; identifies three categories \nof bias in AI \u2013 systemic, statistical, and human \u2013 and describes how and where they contribute to harms; and \ndescribes three broad challenges for mitigating bias \u2013 datasets, testing and evaluation, and human factors \u2013 and \nintroduces preliminary guidance for addressing them. Throughout, the special publication takes a socio-\ntechnical perspective to identifying and managing AI bias. \n29\nAlgorithmic \nDiscrimination \nProtections", "cb8f8727-af89-4193-9272-526395679ece": "copyright, intellectual property, data privacy). \nData Privacy; Intellectual \nProperty; Value Chain and \nComponent Integration \nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \nto promote best practices for managing GAI risks.  \nValue Chain and Component \nIntegration \nGV-6.1-003 \nDevelop and validate approaches for measuring the success of content \nprovenance management e\ufb00orts with third parties (e.g., incidents detected and \nresponse times). \nInformation Integrity; Value Chain \nand Component Integration \nGV-6.1-004 \nDraft and maintain well-de\ufb01ned contracts and service level agreements (SLAs) \nthat specify content ownership, usage rights, quality standards, security \nrequirements, and content provenance expectations for GAI systems. \nInformation Integrity; Information \nSecurity; Intellectual Property", "49b8a8d9-6be1-41bb-95fd-9599977b40e3": "Homogenization? arXiv. https://arxiv.org/pdf/2211.13972 \nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \nDeployment. arXiv. https://arxiv.org/pdf/2011.13416 \nBrowne, D. et al. (2023) Securing the AI Pipeline. Mandiant. \nhttps://www.mandiant.com/resources/blog/securing-ai-pipeline \nBurgess, M. (2024) Generative AI\u2019s Biggest Security Flaw Is Not Easy to Fix. WIRED. \nhttps://www.wired.com/story/generative-ai-prompt-injection-hacking/ \nBurtell, M. et al. (2024) The Surprising Power of Next Word Prediction: Large Language Models \nExplained, Part 1. Georgetown Center for Security and Emerging Technology. \nhttps://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-\nmodels-explained-part-1/ \nCanadian Centre for Cyber Security (2023) Generative arti\ufb01cial intelligence (AI) - ITSAP.00.041. \nhttps://www.cyber.gc.ca/en/guidance/generative-arti\ufb01cial-intelligence-ai-itsap00041", "127dabc6-591a-46f3-b983-4dc993bb6074": "most intimate sphere, including political opinions, sex life, or criminal convictions.  \n8 The notion of harm presumes some baseline scenario that the harmful factor (e.g., a GAI model) makes worse. \nWhen the mechanism for potential harm is a disparity between groups, it can be di\ufb03cult to establish what the \nmost appropriate baseline is to compare against, which can result in divergent views on when a disparity between \nAI behaviors for di\ufb00erent subgroups constitutes a harm. In discussing harms from disparities such as biased \nbehavior, this document highlights examples where someone\u2019s situation is worsened relative to what it would have \nbeen in the absence of any AI system, making the outcome unambiguously a harm of the system.", "40c08338-a614-4372-b193-ffc65bb8fa63": "Unions. Newsweek. Dec. 13, 2021.\nhttps://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust\u00ad\nunions-1658603\n68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and\nagainst Weight Watchers and their subsidiary Kurbo\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)\n69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109\n(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. \u00a7 1232g), Children's Online\nPrivacy Protection Act of 1998, 15 U.S.C. 6501\u20136505, and Confidential Information Protection and\nStatistical Efficiency Act (CIPSEA) (116 Stat. 2899)\n70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True. ProPublica. Nov.\n21, 2018.", "bcbe36ec-6b0a-4955-8d3e-34608c088928": "sensitive domains should meet these expectations: \nNarrowly scoped data and inferences. Human oversight should ensure that automated systems in \nsensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attri\u00ad\nbute as relevant to the specific use case. Data included should be carefully limited to avoid algorithmic \ndiscrimination resulting from, e.g., use of community characteristics, social network analysis, or group-based \ninferences. \nTailored to the situation. Human oversight should ensure that automated systems in sensitive domains \nare tailored to the specific use case and real-world deployment scenario, and evaluation testing should show \nthat the system is safe and effective for that specific situation. Validation testing performed based on one loca\u00ad\ntion or use case should not be assumed to transfer to another. \nHuman consideration before any high-risk decision. Automated systems, where they are used in", "10f9e473-83b3-48ff-be16-32db6b105992": "Your-Health-Final.pdf; Human Impact Partners and WWRC. The Public Health Crisis Hidden in Amazon\nWarehouses. HIP and WWRC report. Jan. 2021.\nhttps://humanimpact.org/wp-content/uploads/2021/01/The-Public-Health-Crisis-Hidden-In-Amazon\u00ad\nWarehouses-HIP-WWRC-01-21.pdf; Drew Harwell. Contract lawyers face a growing invasion of\nsurveillance programs that monitor their work. The Washington Post. Nov. 11, 2021. https://\nwww.washingtonpost.com/technology/2021/11/11/lawyer-facial-recognition-monitoring/;\nVirginia Doellgast and Sean O'Brady. Making Call Center Jobs Better: The Relationship between\nManagement Practices and Worker Stress. A Report for the CWA. June 2020. https://\nhdl.handle.net/1813/74307\n62. See, e.g., Federal Trade Commission. Data Brokers: A Call for Transparency and Accountability. May\n2014.\nhttps://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency-accountability\u00ad\nreport-federal-trade-commission-may-2014/140527databrokerreport.pdf; Cathy O\u2019Neil.", "b1143d64-22b0-413a-ac03-68bbd6392dee": "persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely \naffected by persistent poverty or inequality. \nRIGHTS, OPPORTUNITIES, OR ACCESS: \u201cRights, opportunities, or access\u201d is used to indicate the scoping \nof this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, \nvoting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of \nprivacy and other freedoms in both public and private sector contexts; equal opportunities, including equitable \naccess to education, housing, credit, employment, and other programs; or, access to critical resources or \nservices, such as healthcare, financial services, safety, social services, non-deceptive information about goods \nand services, and government benefits. \n10", "6fdd931f-8ab5-4ac4-99ba-95a2fa9988ca": "them. Both the Fair Credit Reporting Act and the Equal Credit Opportunity Act require in certain circumstances \nthat consumers who are denied credit receive \"adverse action\" notices. Anyone who relies on the information in a \ncredit report to deny a consumer credit must, under the Fair Credit Reporting Act, provide an \"adverse action\" \nnotice to the consumer, which includes \"notice of the reasons a creditor took adverse action on the application \nor on an existing credit account.\"90 In addition, under the risk-based pricing rule,91 lenders must either inform \nborrowers of their credit score, or else tell consumers when \"they are getting worse terms because of \ninformation in their credit report.\" The CFPB has also asserted that \"[t]he law gives every applicant the right to \na specific explanation if their application for credit was denied, and that right is not diminished simply because", "db772381-adfb-41e0-bfa8-e6d177d4fc4f": "ing their bail is informed by an automated system that labeled them \u201chigh risk.\u201d From correcting errors to contesting \ndecisions, people are often denied the knowledge they need to address the impact of automated systems on their lives. \nNotice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable\u00ad\nness of a recommendation before enacting it. \nIn order to guard against potential harms, the American public needs to know if an automated system is being used. \nClear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework. Like\u00ad\nwise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to a \nparticular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore, \nunaccountable, whether by design or by omission. These factors can make explanations both more challenging and", "39624cba-2c85-429f-a578-66871e590a49": "information, including facial likenesses of individuals. \nCBRN Information or Capabilities; \nIntellectual Property; Information \nSecurity; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content; Data \nPrivacy \nMP-4.1-006 Implement policies and practices de\ufb01ning how third-party intellectual property and \ntraining data will be used, stored, and protected. \nIntellectual Property; Value Chain \nand Component Integration \nMP-4.1-007 Re-evaluate models that were \ufb01ne-tuned or enhanced on top of third-party \nmodels. \nValue Chain and Component \nIntegration \nMP-4.1-008 \nRe-evaluate risks when adapting GAI models to new domains. Additionally, \nestablish warning systems to determine if a GAI system is being used in a new \ndomain where previous assumptions (relating to context of use or mapped risks \nsuch as security, and safety) may no longer hold.  \nCBRN Information or Capabilities; \nIntellectual Property; Harmful Bias \nand Homogenization; Dangerous,", "6fc9575a-0b2d-4162-9bf6-ddbb9ff3dcc2": "and also shown to have large disparities by race; Black students were as many as four times as likely as their\notherwise similar white peers to be deemed at high risk of dropping out. These risk scores are used by advisors \nto guide students towards or away from majors, and some worry that they are being used to guide\nBlack students away from math and science subjects.34\n\u2022\nA risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showed\nevidence of disparity in prediction. The tool overpredicts the risk of recidivism for some groups of color on the\ngeneral recidivism tools, and underpredicts the risk of recidivism for some groups of color on some of the\nviolent recidivism tools. The Department of Justice is working to reduce these disparities and has\npublicly released a report detailing its review of the tool.35 \n24", "a07f79fb-249b-4a1e-9bc1-fae65d99b374": "procedures to account for newly encountered uses; Review and maintenance of \npolicies and procedures for detection of unanticipated uses; Verify response \nand recovery plans account for the GAI system value chain; Verify response and \nrecovery plans are updated for and include necessary details to communicate \nwith downstream GAI system Actors: Points-of-Contact (POC), Contact \ninformation, noti\ufb01cation format. \nValue Chain and Component \nIntegration \nAI Actor Tasks: AI Deployment, Operation and Monitoring \n \nMANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or \ndeactivate AI systems that demonstrate performance or outcomes inconsistent with intended use. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.4-001 \nEstablish and maintain communication plans to inform AI stakeholders as part of \nthe deactivation or disengagement process of a speci\ufb01c GAI system (including for", "1c5821e8-25cd-455b-aa81-e84260072bab": "privileges after gaining system access. \nInformation security for GAI models and systems also includes maintaining availability of the GAI system \nand the integrity and (when applicable) the con\ufb01dentiality of the GAI code, training data, and model \nweights. To identify and secure potential attack points in AI systems or speci\ufb01c components of the AI \n \n \n12 See also https://doi.org/10.6028/NIST.AI.100-4, to be published.", "3ffcefc2-5dc7-4b37-92fb-eb8a5d78b8b3": "data and related inferences should only be used for necessary functions, and \nyou should be protected by ethical review and use prohibitions. You and your \ncommunities should be free from unchecked surveillance; surveillance tech\u00ad\nnologies should be subject to heightened oversight that includes at least \npre-deployment assessment of their potential harms and scope limits to pro\u00ad\ntect privacy and civil liberties. Continuous surveillance and monitoring \nshould not be used in education, work, housing, or in other contexts where the \nuse of such surveillance technologies is likely to limit rights, opportunities, or \naccess. Whenever possible, you should have access to reporting that confirms \nyour data decisions have been respected and provides an assessment of the \npotential impact of surveillance technologies on your rights, opportunities, or \naccess. \nDATA PRIVACY\n30", "580f2db8-8415-4aa6-9fe4-fcb63d5bcc3f": "ALGORITHMIC DISCRIMINATION PROTECTIONS\nYou should not face discrimination by algorithms and systems should be used and designed in \nan equitable way. Algorithmic discrimination occurs when automated systems contribute to unjustified \ndifferent treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including \npregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \norientation), religion, age, national origin, disability, veteran status, genetic information, or any other \nclassification protected by law. Depending on the specific circumstances, such algorithmic discrimination \nmay violate legal protections. Designers, developers, and deployers of automated systems should take \nproactive \nand \ncontinuous \nmeasures \nto \nprotect \nindividuals \nand \ncommunities \nfrom algorithmic \ndiscrimination and to use and design systems in an equitable way. This protection should include proactive", "37db406b-c270-45c6-b644-9a85a90bd617": "thorough instructions and any mechanisms for recourse. \nHuman-AI Con\ufb01guration  \nGV-3.2-005 \nEngage in threat modeling to anticipate potential risks from GAI systems. \nCBRN Information or Capabilities; \nInformation Security \nAI Actors: AI Design \n \nGOVERN 4.1: Organizational policies and practices are in place to foster a critical thinking and safety-\ufb01rst mindset in the design, \ndevelopment, deployment, and uses of AI systems to minimize potential negative impacts. \nAction ID \nSuggested Action \nGAI Risks \nGV-4.1-001 \nEstablish policies and procedures that address continual improvement processes \nfor GAI risk measurement. Address general risks associated with a lack of \nexplainability and transparency in GAI systems by using ample documentation and \ntechniques such as: application of gradient-based attributions, occlusion/term \nreduction, counterfactual prompts and prompt engineering, and analysis of \nembeddings; Assess and update risk measurement approaches at regular \ncadences.", "aaec4923-df0a-417c-ab69-41de41fa921f": "lowering the barriers for or easing automated exercise of o\ufb00ensive capabilities; simultaneously, it \nexpands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data \npoisoning.  \nO\ufb00ensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as \nhacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some \nvulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat \nactors might further these risks by developing GAI-powered security co-pilots for use in several parts of \nthe attack chain, including informing attackers on how to proactively evade threat detection and escalate \nprivileges after gaining system access. \nInformation security for GAI models and systems also includes maintaining availability of the GAI system \nand the integrity and (when applicable) the con\ufb01dentiality of the GAI code, training data, and model", "3c7a90aa-0ae3-4b47-aa92-5eabbddb08b6": "MS-2.7-006 \nMeasure the rate at which recommendations from security checks and incidents \nare implemented. Assess how quickly the AI system can adapt and improve \nbased on lessons learned from security incidents and feedback. \nInformation Integrity; Information \nSecurity \nMS-2.7-007 \nPerform AI red-teaming to assess resilience against: Abuse to facilitate attacks on \nother systems (e.g., malicious code generation, enhanced phishing content), GAI \nattacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, \ndata poisoning, membership inference, model extraction, sponge examples). \nInformation Security; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content \nMS-2.7-008 Verify \ufb01ne-tuning does not compromise safety and security controls. \nInformation Integrity; Information \nSecurity; Dangerous, Violent, or \nHateful Content", "a9e56adb-2851-467f-8c88-9ea345f2b538": "backgrounds, lived experiences, professions, and skills across the enterprise to \ninform and conduct risk measurement and management functions. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMP-1.2-002 \nVerify that data or benchmarks used in risk measurement, and users, \nparticipants, or subjects involved in structured GAI public feedback exercises \nare representative of diverse in-context user populations. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nAI Actor Tasks: AI Deployment", "972d5cc1-bd99-472a-a4c8-c715096b6e61": "Discrimination In New Surveillance Technologies: How new surveillance technologies in education,\npolicing, health care, and the workplace disproportionately harm disabled people. Center for Democracy\nand Technology Report. May 24, 2022.\nhttps://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how\u00ad\nnew-surveillance-technologies-in-education-policing-health-care-and-the-workplace\u00ad\ndisproportionately-harm-disabled-people/\n69", "74950177-49a9-46e8-a162-4a60ca08f965": "AI BILL OF RIGHTS\nFFECTIVE SYSTEMS\nineffective systems. Automated systems should be \ncommunities, stakeholders, and domain experts to identify \nSystems should undergo pre-deployment testing, risk \nthat demonstrate they are safe and effective based on \nincluding those beyond the intended use, and adherence to \nprotective measures should include the possibility of not \nAutomated systems should not be designed with an intent \nreasonably foreseeable possibility of endangering your safety or the safety of your community. They should \nstemming from unintended, yet foreseeable, uses or \n \n \n \n \n  \n \n \nSECTION TITLE\nBLUEPRINT FOR AN\nSAFE AND E \nYou should be protected from unsafe or \ndeveloped with consultation from diverse \nconcerns, risks, and potential impacts of the system. \nidentification and mitigation, and ongoing monitoring \ntheir intended use, mitigation of unsafe outcomes \ndomain-specific standards. Outcomes of these \ndeploying the system or removing a system from use. \nor", "0a7da557-a485-453d-8e1c-0c45b60597bb": "corresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback \u2013 before \nand after GAI systems and digital content transparency approaches are deployed \u2013 to gain insights about \nauthentication e\ufb03cacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \nconsequences resulting from the utilization of content provenance approaches on users and \ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \nsystem. \nA.1.8. Incident Disclosure \nOverview \nAI incidents can be de\ufb01ned as an \u201cevent, circumstance, or series of events where the development, use, \nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms:", "787bf7c1-d709-4dba-ab77-0b45d99cfcde": "ENDNOTES\n47. Darshali A. Vyas et al., Hidden in Plain Sight \u2013 Reconsidering the Use of Race Correction in Clinical\nAlgorithms, 383 N. Engl. J. Med.874, 876-78 (Aug. 27, 2020), https://www.nejm.org/doi/full/10.1056/\nNEJMms2004740.\n48. The definitions of 'equity' and 'underserved communities' can be found in the Definitions section of\nthis framework as well as in Section 2 of The Executive Order On Advancing Racial Equity and Support\nfor Underserved Communities Through the Federal Government. https://www.whitehouse.gov/\nbriefing-room/presidential-actions/2021/01/20/executive-order-advancing-racial-equity-and-support\u00ad\nfor-underserved-communities-through-the-federal-government/\n49. Id.\n50. Various organizations have offered proposals for how such assessments might be designed. See, e.g.,\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf.\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. Data & Society", "d855dc71-56b0-4c05-9109-5ff9268d5351": "bility barriers to the use or effectiveness of the automated system. \nDisparity assessment. Automated systems should be tested using a broad set of measures to assess wheth\u00ad\ner the system components, both in pre-deployment testing and in-context deployment, produce disparities. \nThe demographics of the assessed groups should be as inclusive as possible of race, color, ethnicity, sex \n(including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual \norientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifi\u00ad\ncation protected by law. The broad set of measures assessed should include demographic performance mea\u00ad\nsures, overall and subgroup parity assessment, and calibration. Demographic data collected for disparity \nassessment should be separated from data used for the automated system and privacy protections should be", "39d2f1be-0856-4873-a601-b43b3364c09c": "Ethical review and use prohibitions. Any use of sensitive data or decision process based in part on sensi-\ntive data that might limit rights, opportunities, or access, whether the decision is automated or not, should go \nthrough a thorough ethical review and monitoring, both in advance and by periodic review (e.g., via an indepen-\ndent ethics committee or similarly robust process). In some cases, this ethical review may determine that data \nshould not be used or shared for specific uses even with consent. Some novel uses of automated systems in this \ncontext, where the algorithm is dynamically developing and where the science behind the use case is not well \nestablished, may also count as human subject experimentation, and require special review under organizational \ncompliance bodies applying medical, scientific, and academic human subject experimentation ethics rules and \ngovernance procedures.", "975a8b11-da95-46a4-972d-a52efcf70eeb": "across all subgroups, which could leave the groups facing underperformance with worse outcomes than \nif no GAI system were used. Disparate or reduced performance for lower-resource languages also \npresents challenges to model adoption, inclusion, and accessibility, and may make preservation of \nendangered languages more di\ufb03cult if GAI systems become embedded in everyday processes that would \notherwise have been opportunities to use these languages.  \nBias is mutually reinforcing with the problem of undesired homogenization, in which GAI systems \nproduce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles", "1947e184-b336-459e-89e6-b2e4d1ca7d0d": "46 \nMG-4.3-003 \nReport GAI incidents in compliance with legal and regulatory requirements (e.g., \nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \ncrash reporting requirements. \nInformation Security; Data Privacy \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring", "6bdfd516-e0bd-4ad3-bda4-5c6cf908a3af": "Information Integrity \nMS-1.1-002 \nIntegrate tools designed to analyze content provenance and detect data \nanomalies, verify the authenticity of digital signatures, and identify patterns \nassociated with misinformation or manipulation. \nInformation Integrity \nMS-1.1-003 \nDisaggregate evaluation metrics by demographic factors to identify any \ndiscrepancies in how content provenance mechanisms work across diverse \npopulations. \nInformation Integrity; Harmful \nBias and Homogenization \nMS-1.1-004 Develop a suite of metrics to evaluate structured public feedback exercises \ninformed by representative AI Actors. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization; CBRN \nInformation or Capabilities \nMS-1.1-005 \nEvaluate novel methods and technologies for the measurement of GAI-related \nrisks including in content provenance, o\ufb00ensive cyber, and CBRN, while \nmaintaining the models\u2019 ability to produce valid, reliable, and factually accurate \noutputs. \nInformation Integrity; CBRN", "385954bc-4c86-4c7d-a4b6-1a9d195907be": "Civil Rights. May, 2016. https://obamawhitehouse.archives.gov/sites/default/files/microsites/\nostp/2016_0504_data_discrimination.pdf; Cathy O\u2019Neil. Weapons of Math Destruction. Penguin Books.\n2017. https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction; Ruha Benjamin. Race After\nTechnology: Abolitionist Tools for the New Jim Code. Polity. 2019. https://www.ruhabenjamin.com/race\u00ad\nafter-technology\n31. See, e.g., Kashmir Hill. Another Arrest, and Jail Time, Due to a Bad Facial Recognition Match: A New\nJersey man was accused of shoplifting and trying to hit an officer with a car. He is the third known Black man\nto be wrongfully arrested based on face recognition. New York Times. Dec. 29, 2020, updated Jan. 6, 2021.\nhttps://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html; Khari\nJohnson. How Wrongful Arrests Based on AI Derailed 3 Men's Lives. Wired. Mar. 7, 2022. https://\nwww.wired.com/story/wrongful-arrests-ai-derailed-3-mens-lives/", "79e08302-c037-4b11-a38c-dcf039f5345e": "with disabilities. \nIn addition to being able to opt out and use a human alternative, the American public deserves a human fallback \nsystem in the event that an automated system fails or causes harm. No matter how rigorously an automated system is \ntested, there will always be situations for which the system fails. The American public deserves protection via human \nreview against these outlying or unexpected scenarios. In the case of time-critical systems, the public should not have \nto wait\u2014immediate human consideration and fallback should be available. In many time-critical systems, such a \nremedy is already immediately available, such as a building manager who can open a door in the case an automated \ncard access system fails. \nIn the criminal justice system, employment, education, healthcare, and other sensitive domains, automated systems \nare used for many purposes, from pre-trial risk assessments and parole decisions to technologies that help doctors", "830c7c3d-78f8-4e74-b6e6-1ffbae8bd8be": "10 \nGAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading \ncontent (misinformation) at scale, particularly if the content stems from confabulations.  \nGAI systems can also ease the deliberate production or dissemination of false or misleading information \n(disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even \nvery subtle changes to text or images can manipulate human and machine perception. \nSimilarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce \ndisinformation that is targeted towards speci\ufb01c demographics. Current and emerging multimodal models \nmake it possible to generate both text-based disinformation and highly realistic \u201cdeepfakes\u201d \u2013 that is, \nsynthetic audiovisual content and photorealistic images.12 Additional disinformation threats could be \nenabled by future GAI models trained on new data modalities.", "146f38dc-15a3-4385-8408-ac43c18c0639": "13 \n\u2022 \nNot every suggested action applies to every AI Actor14 or is relevant to every AI Actor Task. For \nexample, suggested actions relevant to GAI developers may not be relevant to GAI deployers. \nThe applicability of suggested actions to relevant AI actors should be determined based on \norganizational considerations and their unique uses of GAI systems. \nEach table of suggested actions includes: \n\u2022 \nAction ID: Each Action ID corresponds to the relevant AI RMF function and subcategory (e.g., GV-\n1.1-001 corresponds to the \ufb01rst suggested action for Govern 1.1, GV-1.1-002 corresponds to the \nsecond suggested action for Govern 1.1). AI RMF functions are tagged as follows: GV = Govern; \nMP = Map; MS = Measure; MG = Manage. \n\u2022 \nSuggested Action: Steps an organization or AI actor can take to manage GAI risks.  \n\u2022 \nGAI Risks: Tags linking suggested actions with relevant GAI risks.  \n\u2022 \nAI Actor Tasks: Pertinent AI Actor Tasks for each subcategory. Not every AI Actor Task listed will", "4bfcc364-839f-4848-a922-2e0c1e79f9e5": "addressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \ncontrols for foundation models, \ufb01ne-tuned models, and embedded tools, enhanced processes for \ninteracting with external GAI technologies or service providers. Organizations can apply standard or \nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party \nservice providers, including acquisition and procurement due diligence, requests for software bills of \nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \nGAI systems.", "94a65f59-f102-4309-a9bd-b5c443b1dd1b": "Mat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \nA.1. Governance \nA.1.1. Overview \nLike any other technology system, governance principles and techniques can be used to manage risks \nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \nthese unique GAI risks. This section describes how organizational governance regimes may be re-\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \nthe AI value chain.  \nA.1.2. Organizational Governance \nGAI opportunities, risks and long-term performance characteristics are typically less well-understood \nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \nAccordingly, GAI may call for di\ufb00erent levels of oversight from AI Actors or di\ufb00erent human-AI", "da601a31-dfab-49e0-86cf-eef8e55a4f98": "or insurance health risk assessments, drug addiction risk assessments and associated access alg \n-orithms, wearable technologies, wellness apps, insurance care allocation algorithms, and health\ninsurance cost and underwriting algorithms;\nFinancial system algorithms such as loan allocation algorithms, financial system access determi-\nnation algorithms, credit scoring systems, insurance algorithms including risk assessments, auto\n-mated interest rate determinations, and financial algorithms that apply penalties (e.g., that can\ngarnish wages or withhold tax returns);\n53", "d7881d3c-24e7-4585-93af-8695a597ba95": "TABLE OF CONTENTS\nFROM PRINCIPLES TO PRACTICE: A TECHNICAL COMPANION TO THE BLUEPRINT \nFOR AN AI BILL OF RIGHTS \n \nUSING THIS TECHNICAL COMPANION\n \nSAFE AND EFFECTIVE SYSTEMS\n \nALGORITHMIC DISCRIMINATION PROTECTIONS\n \nDATA PRIVACY\n \nNOTICE AND EXPLANATION\n \nHUMAN ALTERNATIVES, CONSIDERATION, AND FALLBACK\nAPPENDIX\n \nEXAMPLES OF AUTOMATED SYSTEMS\n \nLISTENING TO THE AMERICAN PEOPLE\nENDNOTES \n12\n14\n15\n23\n30\n40\n46\n53\n53\n55\n63\n13", "fde0a57d-613d-4e91-8b21-36327dcf8774": "2 \nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST\u2019s \nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative \nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to \ninform NIST\u2019s approach. \nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content \nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the \nsuggested actions in this document primarily address these considerations. \nFuture revisions of this pro\ufb01le will include additional AI RMF subcategories, risks, and suggested actions based \non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A \nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST\u2019s Trustworthy &", "554181be-cf89-49c9-b571-9e39219b45ed": "related inferences should only be used for necessary functions, and you should be protected by ethical review \nand use prohibitions. You and your communities should be free from unchecked surveillance; surveillance \ntechnologies should be subject to heightened oversight that includes at least pre-deployment assessment of their \npotential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring \nshould not be used in education, work, housing, or in other contexts where the use of such surveillance \ntechnologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access to \nreporting that confirms your data decisions have been respected and provides an assessment of the \npotential impact of surveillance technologies on your rights, opportunities, or access. \nNOTICE AND EXPLANATION\nYou should know that an automated system is being used and understand how and why it", "0955182b-b64b-4db9-a8dc-d4ddf81fe014": "ENDNOTES\n35. Carrie Johnson. Flaws plague a tool meant to help low-risk federal prisoners win early release. NPR.\nJan. 26, 2022. https://www.npr.org/2022/01/26/1075509175/flaws-plague-a-tool-meant-to-help-low\u00ad\nrisk-federal-prisoners-win-early-release.; Carrie Johnson. Justice Department works to curb racial bias\nin deciding who's released from prison. NPR. Apr. 19, 2022. https://\nwww.npr.org/2022/04/19/1093538706/justice-department-works-to-curb-racial-bias-in-deciding\u00ad\nwhos-released-from-pris; National Institute of Justice. 2021 Review and Revalidation of the First Step Act\nRisk Assessment Tool. National Institute of Justice NCJ 303859. Dec., 2021. https://www.ojp.gov/\npdffiles1/nij/303859.pdf\n36. Andrew Thompson. Google\u2019s Sentiment Analyzer Thinks Being Gay Is Bad. Vice. Oct. 25, 2017. https://\nwww.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias\n37. Kaggle. Jigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of", "242e0ec6-efbd-48cf-bcb1-54a0191361d0": "abuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \none example of adapting existing governance protocols for GAI contexts.  \nA.1.3. Third-Party Considerations \nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party \nGAI models, systems, or generated data for various applications across an enterprise. Use of these GAI \ntools and inputs has implications for all functions of the organization \u2013 including but not limited to \nacquisition, human resources, legal, compliance, and IT services \u2013 regardless of whether they are carried \nout by employees or third parties. Many of the actions cited above are relevant and options for \naddressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding", "6f23a99d-bce0-4da9-bd89-c5350c1c9644": "Dev Technology Group \nDigital Therapeutics Alliance \nDigital Welfare State & Human \nRights Project and Center for \nHuman Rights and Global Justice at \nNew York University School of \nLaw, and Temple University \nInstitute for Law, Innovation & \nTechnology \nDignari \nDouglas Goddard \nEdgar Dworsky \nElectronic Frontier Foundation \nElectronic Privacy Information \nCenter, Center for Digital \nDemocracy, and Consumer \nFederation of America \nFaceTec \nFight for the Future \nGanesh Mani \nGeorgia Tech Research Institute \nGoogle \nHealth Information Technology \nResearch and Development \nInteragency Working Group \nHireVue \nHR Policy Association \nID.me \nIdentity and Data Sciences \nLaboratory at Science Applications \nInternational Corporation \nInformation Technology and \nInnovation Foundation \nInformation Technology Industry \nCouncil \nInnocence Project \nInstitute for Human-Centered \nArtificial Intelligence at Stanford \nUniversity \nIntegrated Justice Information \nSystems Institute", "c079913d-5668-44ef-b6c7-4e2660f94099": "79. ACLU of New York. What You Need to Know About New York\u2019s Temporary Ban on Facial\nRecognition in Schools. Accessed May 2, 2022.\nhttps://www.nyclu.org/en/publications/what-you-need-know-about-new-yorks-temporary-ban-facial\u00ad\nrecognition-schools\n80. New York State Assembly. Amendment to Education Law. Enacted Dec. 22, 2020.\nhttps://nyassembly.gov/leg/?default_fld=&leg_video=&bn=S05140&term=2019&Summary=Y&Text=Y\n81. U.S Department of Labor. Labor-Management Reporting and Disclosure Act of 1959, As Amended.\nhttps://www.dol.gov/agencies/olms/laws/labor-management-reporting-and-disclosure-act (Section\n203). See also: U.S Department of Labor. Form LM-10. OLMS Fact Sheet, Accessed May 2, 2022. https://\nwww.dol.gov/sites/dolgov/files/OLMS/regs/compliance/LM-10_factsheet.pdf\n82. See, e.g., Apple. Protecting the User\u2019s Privacy. Accessed May 2, 2022.\nhttps://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy; Google Developers.", "f87b9cf8-7404-429d-94f2-28b22ad8e1bc": "the public deserve to understand why and how such a system is making these determinations.\n\u2022\nA system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entry\nerrors and other system flaws. These flaws were only revealed when an explanation of the system\nwas demanded and produced.86 The lack of an explanation made it harder for errors to be corrected in a\ntimely manner.\n42", "61d43bdc-f423-4809-ad08-b21856b62465": "even if the inferences are not accurate (e.g., confabulations), and especially if they reveal information \nthat the individual considers sensitive or that is used to disadvantage or harm them. \nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate \ninferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive \ninferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, \nleading to representational or allocative harms to individuals or groups (see Harmful Bias and \nHomogenization below).", "3c7f560e-924d-4931-a830-1f1162ff97d1": "APPENDIX\nPanel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of \ntechnology that impact equity of opportunity in employment, education, and housing. \nWelcome: \n\u2022\nRashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\nTechnology Policy\n\u2022\nDominique Harrison, Director for Technology Policy, The Joint Center for Political and Economic\nStudies\nModerator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor \nPanelists: \n\u2022\nChristo Wilson, Associate Professor of Computer Science, Northeastern University\n\u2022\nFrida Polli, CEO, Pymetrics\n\u2022\nKaren Levy, Assistant Professor, Department of Information Science, Cornell University\n\u2022\nNatasha Duarte, Project Director, Upturn\n\u2022\nElana Zeide, Assistant Professor, University of Nebraska College of Law\n\u2022\nFabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community", "104fca8a-92dc-4c38-94d7-0b08437f3cd7": "Some panelists pointed out that companies need clear guidelines to have a consistent environment for \ninnovation, with principles and guardrails being the key to fostering responsible innovation. \nPanel 2: The Criminal Justice System. This event explored current and emergent uses of technology in \nthe criminal justice system and considered how they advance or undermine public safety, justice, and \ndemocratic values. \nWelcome: \n\u2022\nSuresh Venkatasubramanian, Assistant Director for Science and Justice, White House Office of Science\nand Technology Policy\n\u2022\nBen Winters, Counsel, Electronic Privacy Information Center\nModerator: Chiraag Bains, Deputy Assistant to the President on Racial Justice & Equity \nPanelists: \n\u2022\nSean Malinowski, Director of Policing Innovation and Reform, University of Chicago Crime Lab\n\u2022\nKristian Lum, Researcher\n\u2022\nJumana Musa, Director, Fourth Amendment Center, National Association of Criminal Defense Lawyers\n\u2022", "b3a70cda-c86a-4c60-b737-ba600bc9b1d6": "established, may also count as human subject experimentation, and require special review under organizational \ncompliance bodies applying medical, scientific, and academic human subject experimentation ethics rules and \ngovernance procedures. \nData quality. In sensitive domains, entities should be especially careful to maintain the quality of data to \navoid adverse consequences arising from decision-making based on flawed or inaccurate data. Such care is \nnecessary in a fragmented, complex data ecosystem and for datasets that have limited access such as for fraud \nprevention and law enforcement. It should be not left solely to individuals to carry the burden of reviewing and \ncorrecting data. Entities should conduct regular, independent audits and take prompt corrective measures to \nmaintain accurate, timely, and complete data. \nLimit access to sensitive data and derived data. Sensitive data and derived data should not be sold,", "9ea42846-3aba-4bbe-b340-aa710755a097": "Which Ones? New York Times. Oct. 7, 2020.\nhttps://www.nytimes.com/interactive/2020/10/07/upshot/mail-voting-ballots-signature\u00ad\nmatching.html\n100. Rachel Orey and Owen Bacskai. The Low Down on Ballot Curing. Nov. 04, 2020.\nhttps://bipartisanpolicy.org/blog/the-low-down-on-ballot-curing/\n101. Andrew Kenney. 'I'm shocked that they need to have a smartphone': System for unemployment\nbenefits exposes digital divide. USA Today. May 2, 2021.\nhttps://www.usatoday.com/story/tech/news/2021/05/02/unemployment-benefits-system-leaving\u00ad\npeople-behind/4915248001/\n102. Allie Gross. UIA lawsuit shows how the state criminalizes the unemployed. Detroit Metro-Times.\nSep. 18, 2015.\nhttps://www.metrotimes.com/news/uia-lawsuit-shows-how-the-state-criminalizes-the\u00ad\nunemployed-2369412\n103. Maia Szalavitz. The Pain Was Unbearable. So Why Did Doctors Turn Her Away? Wired. Aug. 11,\n2021. https://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/", "3f777ea5-23d5-444e-abfc-8b7659d25ca5": "Dangerous, Violent, or Hateful \nContent \nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \nassess risks that may arise from unreliable downstream decision-making. \nValue Chain and Component \nIntegration; Dangerous, Violent, or \nHateful Content \nMS-2.6-005 \nVerify that GAI system architecture can monitor outputs and performance, and \nhandle, recover from, and repair errors when security anomalies, threats and \nimpacts are detected. \nConfabulation; Information \nIntegrity; Information Security \nMS-2.6-006 \nVerify that systems properly handle queries that may give rise to inappropriate, \nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \nimpersonation, cyber-attacks, and weapons creation. \nCBRN Information or Capabilities; \nInformation Security \nMS-2.6-007 Regularly evaluate GAI system vulnerabilities to possible circumvention of safety \nmeasures.  \nCBRN Information or Capabilities; \nInformation Security", "0e25fc5a-9c40-4d0e-8460-4eb68b9b8c12": "use, and/or who are representative of the populations associated with the \ncontext of use. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization; CBRN \nInformation or Capabilities \nMS-1.3-003 \nVerify those conducting structured human feedback exercises are not directly \ninvolved in system development tasks for the same GAI model. \nHuman-AI Con\ufb01guration; Data \nPrivacy \nAI Actor Tasks: AI Deployment, AI Development, AI Impact Assessment, A\ufb00ected Individuals and Communities, Domain Experts, \nEnd-Users, Operation and Monitoring, TEVV", "9d856b4f-a199-48d0-9ebb-0a3e149854e4": "HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \nThe federal government is working to combat discrimination in mortgage lending. The Depart\u00ad\nment of Justice has launched a nationwide initiative to combat redlining, which includes reviewing how \nlenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.51 \nThis initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial \nProtection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation \nEquity includes a commitment from the agencies that oversee mortgage lending to include a \nnondiscrimination standard in the proposed rules for Automated Valuation Models.52", "3f60f140-2a94-4e5a-aae4-f54530704075": "licensed works, or personal, privileged, proprietary or sensitive data; Underlying \nfoundation models, versions of underlying models, and access modes. \nData Privacy; Human-AI \nCon\ufb01guration; Information \nIntegrity; Intellectual Property; \nValue Chain and Component \nIntegration \nAI Actor Tasks: Governance and Oversight", "1c651545-95e0-4ab4-953c-1ef65ddb2be3": "32 \nMEASURE 2.6: The AI system is evaluated regularly for safety risks \u2013 as identi\ufb01ed in the MAP function. The AI system to be \ndeployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if \nmade to operate beyond its knowledge limits. Safety metrics re\ufb02ect system reliability and robustness, real-time monitoring, and \nresponse times for AI system failures. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.6-001 \nAssess adverse impacts, including health and wellbeing impacts for value chain \nor other AI Actors that are exposed to sexually explicit, o\ufb00ensive, or violent \ninformation during GAI training and maintenance. \nHuman-AI Con\ufb01guration; Obscene, \nDegrading, and/or Abusive \nContent; Value Chain and \nComponent Integration; \nDangerous, Violent, or Hateful \nContent \nMS-2.6-002 \nAssess existence or levels of harmful bias, intellectual property infringement,", "4e35fafd-81a3-43e1-b271-9d5c4da66e60": "can serve multiple purposes, including improving data quality and preprocessing, bolstering governance \ndecision making, and enhancing system documentation and debugging practices. When implementing \nfeedback activities, organizations should follow human subjects research requirements and best \npractices such as informed consent and subject compensation.", "3b72284f-1c72-4114-9c41-64be27da5b6a": "56 \nKarasavva, V. et al. (2021) Personality, Attitudinal, and Demographic Predictors of Non-consensual \nDissemination of Intimate Images. NIH. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554400/ \nKatzman, J., et al. (2023) Taxonomizing and measuring representational harms: a look at image tagging. \nAAAI. https://dl.acm.org/doi/10.1609/aaai.v37i12.26670 \nKhan, T. et al. (2024) From Code to Consumer: PAI\u2019s Value Chain Analysis Illuminates Generative AI\u2019s Key \nPlayers. AI. https://partnershiponai.org/from-code-to-consumer-pais-value-chain-analysis-illuminates-\ngenerative-ais-key-players/ \nKirchenbauer, J. et al. (2023) A Watermark for Large Language Models. OpenReview. \nhttps://openreview.net/forum?id=aX8ig9X2a7 \nKleinberg, J. et al. (May 2021) Algorithmic monoculture and social welfare. PNAS. \nhttps://www.pnas.org/doi/10.1073/pnas.2018340118 \nLakatos, S. (2023) A Revealing Picture. Graphika. https://graphika.com/reports/a-revealing-picture", "62cae94c-7d87-4966-8848-cc0feafa2d37": "concerns, risks, and potential impacts of the system. \nidentification and mitigation, and ongoing monitoring \ntheir intended use, mitigation of unsafe outcomes \ndomain-specific standards. Outcomes of these \ndeploying the system or removing a system from use. \nor \nbe designed to proactively protect you from harms \nimpacts of automated systems. You should be protected from inappropriate or irrelevant data use in the \ndesign, development, and deployment of automated systems, and from the compounded harm of its reuse. \nIndependent evaluation and reporting that confirms that the system is safe and effective, including reporting of \nsteps taken to mitigate potential harms, should be performed and the results made public whenever possible. \nALGORITHMIC DISCRIMINATION PROTECTIONS\nYou should not face discrimination by algorithms and systems should be used and designed in \nan equitable way. Algorithmic discrimination occurs when automated systems contribute to unjustified", "8a6654b7-221b-453a-ac99-17e5dd712b6f": "https://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html; Khari\nJohnson. How Wrongful Arrests Based on AI Derailed 3 Men's Lives. Wired. Mar. 7, 2022. https://\nwww.wired.com/story/wrongful-arrests-ai-derailed-3-mens-lives/\n32. Student Borrower Protection Center. Educational Redlining. Student Borrower Protection Center\nReport. Feb. 2020. https://protectborrowers.org/wp-content/uploads/2020/02/Education-Redlining\u00ad\nReport.pdf\n33. Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Oct.\n10, 2018. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps\u00ad\nsecret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\n34. Todd Feathers. Major Universities Are Using Race as a \u201cHigh Impact Predictor\u201d of Student Success:\nStudents, professors, and education experts worry that that\u2019s pushing Black students in particular out of math", "4204f35f-98dd-4508-b0b4-ebab0c7c8f33": "48 \n\u2022 Data protection \n\u2022 Data retention  \n\u2022 Consistency in use of de\ufb01ning key terms \n\u2022 Decommissioning \n\u2022 Discouraging anonymous use \n\u2022 Education  \n\u2022 Impact assessments  \n\u2022 Incident response \n\u2022 Monitoring \n\u2022 Opt-outs  \n\u2022 Risk-based controls \n\u2022 Risk mapping and measurement \n\u2022 Science-backed TEVV practices \n\u2022 Secure software development practices \n\u2022 Stakeholder engagement \n\u2022 Synthetic content detection and \nlabeling tools and techniques \n\u2022 Whistleblower protections \n\u2022 Workforce diversity and \ninterdisciplinary teams\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-AI teaming settings \nas well as di\ufb00erent levels of human-AI con\ufb01gurations can help to decrease risks arising from misuse, \nabuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \none example of adapting existing governance protocols for GAI contexts.  \nA.1.3. Third-Party Considerations", "3d0f790b-1797-4aa3-beac-f6cd44ba4898": "APPENDIX\nPanelists discussed the benefits of AI-enabled systems and their potential to build better and more \ninnovative infrastructure. They individually noted that while AI technologies may be new, the process of \ntechnological diffusion is not, and that it was critical to have thoughtful and responsible development and \nintegration of technology within communities. Some panelists suggested that the integration of technology \ncould benefit from examining how technological diffusion has worked in the realm of urban planning: \nlessons learned from successes and failures there include the importance of balancing ownership rights, use \nrights, and community health, safety and welfare, as well ensuring better representation of all voices, \nespecially those traditionally marginalized by technological advances. Some panelists also raised the issue of \npower structures \u2013 providing examples of how strong transparency requirements in smart city projects", "449961a6-bf39-4c7c-baff-149d99cb9549": "Vehicle-Speeds\n17. Karen Hao. Worried about your firm\u2019s AI ethics? These startups are here to help.\nA growing ecosystem of \u201cresponsible AI\u201d ventures promise to help organizations monitor and fix their AI\nmodels. MIT Technology Review. Jan 15., 2021.\nhttps://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top Progressive\nCompanies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021. https://\nwww.analyticsinsight.net/top-progressive-companies-building-ethical-ai-to-look-out-for\u00ad\nin-2021/ https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/; Disha Sinha. Top\nProgressive Companies Building Ethical AI to Look Out for in 2021. Analytics Insight. June 30, 2021.\n18. Office of Management and Budget. Study to Identify Methods to Assess Equity: Report to the President.\nAug. 2021. https://www.whitehouse.gov/wp-content/uploads/2021/08/OMB-Report-on-E013985\u00ad\nImplementation_508-Compliant-Secure-v1.1.pdf", "c379510c-9dbc-42d0-9263-4b16bc2a374d": "groups and have di\ufb03culty producing non-stereotyped content even when the prompt speci\ufb01cally \nrequests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which \nmay stem from their training data, can also cause representational harms or perpetuate or exacerbate \nbias based on race, gender, disability, or other protected classes.  \nHarmful bias in GAI systems can also lead to harms via disparities between how a model performs for \ndi\ufb00erent subgroups or languages (e.g., an LLM may perform less well for non-English languages or \ncertain dialects). Such disparities can contribute to discriminatory decision-making or ampli\ufb01cation of \nexisting societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly \nacross all subgroups, which could leave the groups facing underperformance with worse outcomes than \nif no GAI system were used. Disparate or reduced performance for lower-resource languages also", "7775d86c-c343-4e74-808f-28244456986e": "MAP 1.1: Intended purposes, potentially bene\ufb01cial uses, context speci\ufb01c laws, norms and expectations, and prospective settings in \nwhich the AI system will be deployed are understood and documented. Considerations include: the speci\ufb01c set or types of users \nalong with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, \nsociety, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or \nproduct AI lifecycle; and related TEVV and system metrics. \nAction ID \nSuggested Action \nGAI Risks \nMP-1.1-001 \nWhen identifying intended purposes, consider factors such as internal vs. \nexternal use, narrow vs. broad application scope, \ufb01ne-tuning, and varieties of \ndata sources (e.g., grounding, retrieval-augmented generation). \nData Privacy; Intellectual \nProperty", "0cefe1b6-a0f3-4eb6-abcb-439999b00962": "DATA PRIVACY \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \n\u2022\nAn insurer might collect data from a person's social media presence as part of deciding what life\ninsurance rates they should be offered.64\n\u2022\nA data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of\nthousands of people to potential identity theft. 65\n\u2022\nA local public housing authority installed a facial recognition system at the entrance to housing complexes to\nassist law enforcement with identifying individuals viewed via camera when police reports are filed, leading\nthe community, both those living in the housing complex and not, to have videos of them sent to the local\npolice department and made available for scanning by its facial recognition software.66\n\u2022", "a5edf3db-6076-4475-818a-2d1f77d528a0": "the prevalence of denigration in generated content in deployment (e.g., sub-\nsampling a fraction of tra\ufb03c and manually annotating denigrating content). \nHarmful Bias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMS-2.11-003 \nIdentify the classes of individuals, groups, or environmental ecosystems which \nmight be impacted by GAI systems through direct engagement with potentially \nimpacted communities. \nEnvironmental; Harmful Bias and \nHomogenization \nMS-2.11-004 \nReview, document, and measure sources of bias in GAI training and TEVV data: \nDi\ufb00erences in distributions of outcomes across and within groups, including \nintersecting groups; Completeness, representativeness, and balance of data \nsources; demographic group and subgroup coverage in GAI system training \ndata; Forms of latent systemic bias in images, text, audio, embeddings, or other \ncomplex or unstructured data; Input data features that may serve as proxies for", "b760ccb4-e8ea-456b-b851-4ba450881099": "that the system is safe and effective for that specific situation. Validation testing performed based on one loca\u00ad\ntion or use case should not be assumed to transfer to another. \nHuman consideration before any high-risk decision. Automated systems, where they are used in \nsensitive domains, may play a role in directly providing information or otherwise providing positive outcomes \nto impacted people. However, automated systems should not be allowed to directly intervene in high-risk \nsituations, such as sentencing decisions or medical care, without human consideration. \nMeaningful access to examine the system. Designers, developers, and deployers of automated \nsystems should consider limited waivers of confidentiality (including those related to trade secrets) where \nnecessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating mea\u00ad\nsures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. This", "5ed69716-2ef9-4c3e-ad2e-6944a6b2de5a": "SAFE AND EFFECTIVE \nSYSTEMS \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nIn order to ensure that an automated system is safe and effective, it should include safeguards to protect the \npublic from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the task \nat hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness of \nthe system. These expectations are explained below. \nProtect the public from harm in a proactive and ongoing manner \nConsultation. The public should be consulted in the design, implementation, deployment, acquisition, and \nmaintenance phases of automated system development, with emphasis on early-stage consultation before a", "c99a31b8-88fc-4c6a-b194-8dcfd89d0a44": "in some cases. Many states have also enacted consumer data privacy protection regimes to address some of these \nharms. \nHowever, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory \nframework governing the rights of the public when it comes to personal data. While a patchwork of laws exists to \nguide the collection and use of personal data in specific contexts, including health, employment, education, and credit, \nit can be unclear how these laws apply in other contexts and in an increasingly automated society. Additional protec\u00ad\ntions would assure the American public that the automated systems they use are not monitoring their activities, \ncollecting information on their lives, or otherwise surveilling them without context-specific consent or legal authori\u00ad\nty. \n31", "52b58f7e-ada8-4cb4-8703-7deda473a155": "over-rely on GAI systems or may unjusti\ufb01ably perceive GAI content to be of higher quality than that \nproduced by other sources. This phenomenon is an example of automation bias, or excessive deference \nto automated systems. Automation bias can exacerbate other risks of GAI, such as risks of confabulation \nor risks of bias or homogenization. \nThere may also be concerns about emotional entanglement between humans and GAI systems, which \ncould lead to negative psychological impacts. \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \nHarmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable \n2.8. Information Integrity \nInformation integrity describes the \u201cspectrum of information and associated patterns of its creation, \nexchange, and consumption in society.\u201d High-integrity information can be trusted; \u201cdistinguishes fact \nfrom \ufb01ction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of", "2c1216c9-c47f-43e8-956a-520644a74ef3": "WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nAny automated system should be tested to help ensure it is free from algorithmic discrimination before it can be \nsold or used. Protection against algorithmic discrimination should include designing to ensure equity, broadly \nconstrued.  Some algorithmic discrimination is already prohibited under existing anti-discrimination law. The \nexpectations set out below describe proactive technical and policy steps that can be taken to not only \nreinforce those legal protections but extend beyond them to ensure equity for underserved communities48 \neven in circumstances where a specific legal protection may not be clearly established. These protections", "717d3c41-4562-40e9-83f7-99c53a4f65ed": "APPENDIX\nExamples of Automated Systems \nThe below examples are meant to illustrate the breadth of automated systems that, insofar as they have the \npotential to meaningfully impact rights, opportunities, or access to critical resources or services, should \nbe covered by the Blueprint for an AI Bill of Rights. These examples should not be construed to limit that \nscope, which includes automated systems that may not yet exist, but which fall under these criteria. \nExamples of automated systems for which the Blueprint for an AI Bill of Rights should be considered include \nthose that have the potential to meaningfully impact: \n\u2022 Civil rights, civil liberties, or privacy, including but not limited to:\nSpeech-related systems such as automated content moderation tools; \nSurveillance and criminal justice system algorithms such as risk assessments, predictive  \n    policing, automated license plate readers, real-time facial recognition systems (especially", "f3b812ba-146c-4d00-ae12-e183b088588b": "37 \nMS-2.11-005 \nAssess the proportion of synthetic to non-synthetic training data and verify \ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \nmodel collapse. \nHarmful Bias and Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, \nOperation and Monitoring, TEVV \n \nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities \u2013 as identi\ufb01ed in the MAP \nfunction \u2013 are assessed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.12-001 Assess safety to physical environments when deploying GAI systems. \nDangerous, Violent, or Hateful \nContent \nMS-2.12-002 Document anticipated environmental impacts of model development, \nmaintenance, and deployment in product design decisions. \nEnvironmental \nMS-2.12-003 \nMeasure or estimate environmental impacts (e.g., energy and water", "7cffdf09-eed0-40e8-b85b-d15725e4c866": "communicate their privacy risks and goals to support ethical decision-making in system, product, and service \ndesign or deployment, as well as the measures they are taking to demonstrate compliance with applicable laws \nor regulations. It has been voluntarily adopted by organizations across many different sectors around the world.78\nA school board\u2019s attempt to surveil public school students\u2014undertaken without \nadequate community input\u2014sparked a state-wide biometrics moratorium.79 Reacting to a plan in \nthe city of Lockport, New York, the state\u2019s legislature banned the use of facial recognition systems and other \n\u201cbiometric identifying technology\u201d in schools until July 1, 2022.80 The law additionally requires that a report on \nthe privacy, civil rights, and civil liberties implications of the use of such technologies be issued before \nbiometric identification technologies can be used in New York schools.", "626702a6-9a32-4e5a-98dd-9aaa21410ccc": "material (CSAM), and nonconsensual intimate images (NCII) of adults. \n12. Value Chain and Component Integration: Non-transparent or untraceable integration of \nupstream third-party components, including data that has been improperly obtained or not \nprocessed and cleaned due to increased automation from GAI; improper supplier vetting across \nthe AI lifecycle; or other issues that diminish transparency or accountability for downstream \nusers. \n2.1. CBRN Information or Capabilities \nIn the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant \nknowledge, information, materials, tools, or technologies that could be misused to assist in the design, \ndevelopment, production, or use of CBRN weapons or other dangerous materials or agents. While \nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \ncould facilitate its analysis or synthesis, particularly by individuals without formal scienti\ufb01c training or", "a0110c5f-bbbf-47be-bff9-d339431192d8": "transformers, etc.); Optimization objectives; Training algorithms; RLHF \napproaches; Fine-tuning or retrieval-augmented generation approaches; \nEvaluation data; Ethical considerations; Legal and regulatory requirements. \nInformation Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 2.10: Privacy risk of the AI system \u2013 as identi\ufb01ed in the MAP function \u2013 is examined and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.10-001 \nConduct AI red-teaming to assess issues such as: Outputting of training data \nsamples, and subsequent reverse engineering, model extraction, and \nmembership inference risks; Revealing biometric, con\ufb01dential, copyrighted, \nlicensed, patented, personal, proprietary, sensitive, or trade-marked information; \nTracking or revealing location information of users or members of training \ndatasets. \nHuman-AI Con\ufb01guration;", "7b4f17aa-fdb5-4748-82db-c09fa44acf51": "impact the American public. Government agencies, particularly law enforcement agencies, also use and help develop \na variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input \ninto other automated systems that directly impact people\u2019s lives. Federal law has not grown to address the expanding \nscale of private data collection, or of the ability of governments at all levels to access that data and leverage the means \nof private collection.  \nMeanwhile, members of the American public are often unable to access their personal data or make critical decisions \nabout its collection and use. Data brokers frequently collect consumer data from numerous sources without \nconsumers\u2019 permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to \nmake decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillance", "b0719b6b-2236-4fc9-a0e9-1f7cccdf5792": "APPENDIX\n\u2022\nJulia Simon-Mishel, Supervising Attorney, Philadelphia Legal Assistance\n\u2022\nDr. Zachary Mahafza, Research & Data Analyst, Southern Poverty Law Center\n\u2022\nJ. Khadijah Abdurahman, Tech Impact Network Research Fellow, AI Now Institute, UCLA C2I1, and\nUWA Law School\nPanelists separately described the increasing scope of technology use in providing for social welfare, including \nin fraud detection, digital ID systems, and other methods focused on improving efficiency and reducing cost. \nHowever, various panelists individually cautioned that these systems may reduce burden for government \nagencies by increasing the burden and agency of people using and interacting with these technologies. \nAdditionally, these systems can produce feedback loops and compounded harm, collecting data from \ncommunities and using it to reinforce inequality. Various panelists suggested that these harms could be", "a2650578-6e24-4412-b436-9de809c63351": "or produces unexpected outputs. \nConfabulation \nMG-4.1-005 \nShare transparency reports with internal and external stakeholders that detail \nsteps taken to update the GAI system to enhance transparency and \naccountability. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMG-4.1-006 \nTrack dataset modi\ufb01cations for provenance by monitoring data deletions, \nrecti\ufb01cation requests, and other changes that may impact the veri\ufb01ability of \ncontent origins. \nInformation Integrity", "e0057a35-9370-43bd-a0b9-22f30e45dc9b": "preferences and data provenance and lineage, context of use and access-specific tags, and training models for \nassessing privacy risk. \nDemonstrate that data privacy and user control are protected \nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow \nindependent evaluation of the claims made regarding data policies. These independent evaluations should be \nmade public whenever possible. Care will need to be taken to balance individual privacy with evaluation data \naccess needs. \nReporting. When members of the public wish to know what data about them is being used in a system, the \nentity responsible for the development of the system should respond quickly with a report on the data it has \ncollected or stored about them. Such a report should be machine-readable, understandable by most users, and \ninclude, to the greatest extent allowable under law, any data and metadata about them or collected from them,", "d629d92a-d422-46de-9249-555ddf9683da": "practical ways that were shown to reduce this bias, such as focusing specifically on active chronic health \nconditions or avoidable future costs related to emergency visits and hospitalization.54 \nLarge employers have developed best practices to scrutinize the data and models used \nfor hiring. An industry initiative has developed Algorithmic Bias Safeguards for the Workforce, a structured \nquestionnaire that businesses can use proactively when procuring software to evaluate workers. It covers \nspecific technical questions such as the training data used, model training process, biases identified, and \nmitigation steps employed.55 \nStandards organizations have developed guidelines to incorporate accessibility criteria \ninto technology design processes. The most prevalent in the United States is the Access Board\u2019s Section \n508 regulations,56 which are the technical standards for federal information communication technology (software,", "6087332b-1112-4e02-8446-f01fd5bf86ec": "\u2022 \nField Testing: Methods used to determine how people interact with, consume, use, and make \nsense of AI-generated information, and subsequent actions and e\ufb00ects, including UX, usability, \nand other structured, randomized experiments.  \n\u2022 \nAI Red-teaming: A structured testing exercise used to probe an AI system to \ufb01nd \ufb02aws and \nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \nenvironment and in collaboration with system developers. \nInformation gathered from structured public feedback can inform design, implementation, deployment \napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises \ncan serve multiple purposes, including improving data quality and preprocessing, bolstering governance \ndecision making, and enhancing system documentation and debugging practices. When implementing \nfeedback activities, organizations should follow human subjects research requirements and best", "2489697d-9053-482f-84a2-a1d89bfa6f81": "MS-2.7-002 \nBenchmark GAI system security and resilience related to content provenance \nagainst industry standards and best practices. Compare GAI system security \nfeatures and content provenance methods against industry state-of-the-art. \nInformation Integrity; Information \nSecurity \nMS-2.7-003 \nConduct user surveys to gather user satisfaction with the AI-generated content \nand user perceptions of content authenticity. Analyze user feedback to identify \nconcerns and/or current literacy levels related to content provenance and \nunderstanding of labels on content. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMS-2.7-004 \nIdentify metrics that re\ufb02ect the e\ufb00ectiveness of security measures, such as data \nprovenance, the number of unauthorized access attempts, inference, bypass, \nextraction, penetrations, or provenance veri\ufb01cation. \nInformation Integrity; Information \nSecurity \nMS-2.7-005 \nMeasure reliability of content authentication methods, such as watermarking,", "283378e4-cef6-4bfb-a632-be8e5ae7189e": "34 \nMS-2.7-009 Regularly assess and verify that security measures remain e\ufb00ective and have not \nbeen compromised. \nInformation Security \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \n \nMEASURE 2.8: Risks associated with transparency and accountability \u2013 as identi\ufb01ed in the MAP function \u2013 are examined and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.8-001 \nCompile statistics on actual policy violations, take-down requests, and intellectual \nproperty infringement for organizational GAI systems: Analyze transparency \nreports across demographic groups, languages groups. \nIntellectual Property; Harmful Bias \nand Homogenization \nMS-2.8-002 Document the instructions given to data annotators or AI red-teamers. \nHuman-AI Con\ufb01guration \nMS-2.8-003 \nUse digital content transparency solutions to enable the documentation of each \ninstance where content is generated, modi\ufb01ed, or shared to provide a tamper-", "d6c7d674-1330-451a-87a9-9a09888c83bd": "infrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \ncan help actors address those barriers.  \nFurthermore, chemical and biological design tools (BDTs) \u2013 highly specialized AI systems trained on \nscienti\ufb01c data that aid in chemical and biological design \u2013 may augment design capabilities in chemistry \nand biology beyond what text-based LLMs are able to provide. As these models become more \ne\ufb03cacious, including for bene\ufb01cial uses, it will be important to assess their potential to be used for \nharm, such as the ideation and design of novel harmful chemical or biological agents.  \nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing \nassessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN", "a13d387e-9ee8-45b3-a8a3-7d6ee3696b0f": "Attn: NIST AI Innovation Lab, Information Technology Laboratory \n100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \nAdditional Information \nAdditional information about this publication and other NIST AI publications are available at \nhttps://airc.nist.gov/Home. \n \nDisclaimer: Certain commercial entities, equipment, or materials may be identi\ufb01ed in this document in \norder to adequately describe an experimental procedure or concept. Such identi\ufb01cation is not intended to \nimply recommendation or endorsement by the National Institute of Standards and Technology, nor is it \nintended to imply that the entities, materials, or equipment are necessarily the best available for the \npurpose. Any mention of commercial, non-pro\ufb01t, academic partners, or their products, or references is \nfor information only; it is not intended to imply endorsement or recommendation by any U.S. \nGovernment agency.", "b7e4c49d-2cba-48a0-9f0f-f7ca461723e2": "trustworthy development and use of AI. \nAcknowledgments: This report was accomplished with the many helpful comments and contributions \nfrom the community, including the NIST Generative AI Public Working Group, and NIST sta\ufb00 and guest \nresearchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin \nStanley, and Elham Tabassi. \nNIST Technical Series Policies \nCopyright, Use, and Licensing Statements \nNIST Technical Series Publication Identifier Syntax \nPublication History \nApproved by the NIST Editorial Review Board on 07-25-2024 \nContact Information \nai-inquiries@nist.gov \nNational Institute of Standards and Technology \nAttn: NIST AI Innovation Lab, Information Technology Laboratory \n100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \nAdditional Information \nAdditional information about this publication and other NIST AI publications are available at \nhttps://airc.nist.gov/Home.", "86a6b7d2-3d05-42ee-b044-111013d2585f": "7 \nunethical behavior. Text-to-image models also make it easy to create images that could be used to \npromote dangerous or violent messages. Similar concerns are present for other GAI media, including \nvideo and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.  \nMany current systems restrict model outputs to limit certain content or in response to certain prompts, \nbut this approach may still produce harmful recommendations in response to other less-explicit, novel \nprompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and \nObscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as \n\u201cjailbreaking,\u201d or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be \nharmful or dangerous in certain contexts. Studies have observed that users may disclose mental health", "c3d4b574-885a-4c0b-9256-9ec82a72a7f9": "Shoshana Zuboff. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of\nPower. Public Affairs. 2019.\n64. Angela Chen. Why the Future of Life Insurance May Depend on Your Online Presence. The Verge. Feb.\n7, 2019.\nhttps://www.theverge.com/2019/2/7/18211890/social-media-life-insurance-new-york-algorithms-big\u00ad\ndata-discrimination-online-records\n68", "25166c1a-9f42-441b-8a55-62f5a027ff9a": "nology as both a vehicle to deliver healthcare and a tool to enhance the quality of care. On the issue of \ndelivery, various panelists pointed to a number of concerns including access to and expense of broadband \nservice, the privacy concerns associated with telehealth systems, the expense associated with health \nmonitoring devices, and how this can exacerbate equity issues.  On the issue of technology enhanced care, \nsome panelists spoke extensively about the way in which racial biases and the use of race in medicine \nperpetuate harms and embed prior discrimination, and the importance of ensuring that the technologies used \nin medical care were accountable to the relevant stakeholders. Various panelists emphasized the importance \nof having the voices of those subjected to these technologies be heard.\n59", "d6cff776-43c6-4cd1-8ac1-e9767d4c92f6": "target measure; unobservable targets may result in the inappropriate use of proxies. Meeting these \nstandards may require instituting mitigation procedures and other protective measures to address \nalgorithmic discrimination, avoid meaningful harm, and achieve equity goals. \nOngoing monitoring and mitigation. Automated systems should be regularly monitored to assess algo\u00ad\nrithmic discrimination that might arise from unforeseen interactions of the system with inequities not \naccounted for during the pre-deployment testing, changes to the system after deployment, or changes to the \ncontext of use or associated data. Monitoring and disparity assessment should be performed by the entity \ndeploying or using the automated system to examine whether the system has led to algorithmic discrimina\u00ad\ntion when deployed. This assessment should be performed regularly and whenever a pattern of unusual", "61833493-95e3-444c-8824-33af09d23751": "proxy; if needed, it may be possible to identify alternative attributes that can be used instead. At a minimum, \norganizations should ensure a proxy feature is not given undue weight and should monitor the system closely \nfor any resulting algorithmic discrimination.   \n26\nAlgorithmic \nDiscrimination \nProtections", "fe339b6e-df99-458e-b3dc-9e0831f88d2e": "APPENDIX\n\u2022 OSTP conducted meetings with a variety of stakeholders in the private sector and civil society. Some of these\nmeetings were specifically focused on providing ideas related to the development of the Blueprint for an AI\nBill of Rights while others provided useful general context on the positive use cases, potential harms, and/or\noversight possibilities for these technologies. Participants in these conversations from the private sector and\ncivil society included:\nAdobe \nAmerican Civil Liberties Union \n(ACLU) \nThe Aspen Commission on \nInformation Disorder \nThe Awood Center \nThe Australian Human Rights \nCommission \nBiometrics Institute \nThe Brookings Institute \nBSA | The Software Alliance \nCantellus Group \nCenter for American Progress \nCenter for Democracy and \nTechnology \nCenter on Privacy and Technology \nat Georgetown Law \nChristiana Care \nColor of Change \nCoworker \nData Robot \nData Trust Alliance \nData and Society Research Institute \nDeepmind \nEdSAFE AI Alliance", "8d28e5eb-06a0-4f34-8900-87da71236434": "NOTICE & \nEXPLANATION \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \n\u2022\nA predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of\ngun violence (based on automated analysis of social ties to gang members, criminal histories, previous experi\u00ad\nences of gun violence, and other factors) and led to individuals being placed on a watch list with no\nexplanation or public transparency regarding how the system came to its conclusions.85 Both police and\nthe public deserve to understand why and how such a system is making these determinations.\n\u2022\nA system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entry\nerrors and other system flaws. These flaws were only revealed when an explanation of the system", "31e88228-f2c5-4fb8-9e69-4fe4693c4915": "training or \ufb01ne-tuning, levels of model access or availability of model weights, and application or use \ncase context. \nOrganizations may choose to tailor how they measure GAI risks based on these characteristics. They may \nadditionally wish to allocate risk management resources relative to the severity and likelihood of \nnegative impacts, including where and how these risks manifest, and their direct and material impacts \nharms in the context of GAI use. Mitigations for model or system level risks may di\ufb00er from mitigations \nfor use-case or ecosystem level risks. \nImportantly, some GAI risks are unknown, and are therefore di\ufb03cult to properly scope or evaluate given \nthe uncertainty about potential GAI scale, complexity, and capabilities. Other risks may be known but \ndi\ufb03cult to estimate given the wide range of GAI stakeholders, uses, inputs, and outputs. Challenges with \nrisk estimation are aggravated by a lack of visibility into GAI training data, and the generally immature", "5189c194-6890-4fe8-8d79-05849fb8f99d": "FROM \nPRINCIPLES \nTO PRACTICE \nA TECHINCAL COMPANION TO\nTHE Blueprint for an \nAI BILL OF RIGHTS\n12", "50ad7d3d-22a8-404d-8505-90f73aee2073": "health, family planning and care, employment, education, criminal justice, and personal finance. In the context \nof this framework, such domains are considered sensitive whether or not the specifics of a system context \nwould necessitate coverage under existing law, and domains and data that are considered sensitive are under\u00ad\nstood to change over time based on societal norms and context. \nSURVEILLANCE TECHNOLOGY: \u201cSurveillance technology\u201d refers to products or services marketed for \nor that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/or \nretain data, identifying information, or communications concerning individuals or groups. This framework \nlimits its focus to both government and commercial use of surveillance technologies when juxtaposed with \nreal-time or subsequent automated analysis and when such systems have a potential for meaningful impact \non individuals\u2019 or communities\u2019 rights, opportunities, or access.", "0584125c-261e-4f68-bc6a-8f7dbdb48f30": "distinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \ninformation access about both authentic and synthetic content to users, enabling better knowledge of \ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \ndigital content transparency approaches can enable processes to trace negative outcomes back to their \nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic \ncontent detection mechanisms provide information about the origin and history of content to assist in \nGAI risk management e\ufb00orts. \nProvenance metadata can include information about GAI model developers or creators of GAI content,", "269fa491-ab38-4a83-8630-e92d6754008d": "ENDNOTES\n57. ISO Technical Management Board. ISO/IEC Guide 71:2014. Guide for addressing accessibility in\nstandards. International Standards Organization. 2021. https://www.iso.org/standard/57385.html\n58. World Wide Web Consortium. Web Content Accessibility Guidelines (WCAG) 2.0. Dec. 11, 2008.\nhttps://www.w3.org/TR/WCAG20/\n59. Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, and Andrew Bert. NIST Special\nPublication 1270: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. The\nNational Institute of Standards and Technology. March, 2022. https://nvlpubs.nist.gov/nistpubs/\nSpecialPublications/NIST.SP.1270.pdf\n60. See, e.g., the 2014 Federal Trade Commission report \u201cData Brokers A Call for Transparency and\nAccountability\u201d. https://www.ftc.gov/system/files/documents/reports/data-brokers-call-transparency\u00ad\naccountability-report-federal-trade-commission-may-2014/140527databrokerreport.pdf", "24bc7db7-96e9-4bb4-9858-b34090f6bd77": "communities that may not be direct users of the automated system, risks resulting from purposeful misuse of \nthe system, and other concerns identified via the consultation process. Assessment and, where possible, mea\u00ad\nsurement of the impact of risks should be included and balanced such that high impact risks receive attention \nand mitigation proportionate with those impacts. Automated systems with the intended purpose of violating \nthe safety of others should not be developed or used; systems with such safety violations as identified unin\u00ad\ntended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessi\u00ad\ntate rollback or significant modification to a launched automated system. \n18", "97a49933-d6c4-4031-99c7-23828939f326": "ing individuals\u2019 rights. These reporting expectations are important for transparency, so the American people can have\nconfidence that their rights, opportunities, and access as well as their expectations about technologies are respected. \n3\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE: \nThis section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices. \nIt describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access. \nThe examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help \nprovide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing these \nprocesses require the cooperation of and collaboration among industry, civil society, researchers, policymakers, \ntechnologists, and the public. \n14", "bfe0b27a-754a-4543-a665-86c6860254f0": "NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1"}}
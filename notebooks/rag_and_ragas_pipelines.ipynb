{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal AI Risk Management: Chatbot for Safety and Governance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### This notebook has all the code EXCEPT Model Finetuning (which is in a separate notebook)\n",
        ">\n",
        "> Here is the link to the notebook that has all the code used for finetuning.  \n",
        ">\n",
        "> [Click Here](finetuning_embeddings_model.ipynb) to access the finetuning notebook.\n",
        ">\n",
        "> (NOTE - this notebook and the finetuning notebook leverage a number of utilities that are in [this](myutils) folder.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NOTE - May need to pin langchain_core version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN13TZlSCv4",
        "outputId": "424a6920-0cea-4e28-dce0-3de6f0a4cc3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-experimental 0.3.4 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.2.16 which is incompatible.\n",
            "langchain-experimental 0.3.4 requires langchain-core<0.4.0,>=0.3.28, but you have langchain-core 0.2.38 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# NOTE!!!\n",
        "# May need to pin version: langchain_core==0.2.38\n",
        "! pip install -U -q langchain langchain-openai langchain_core==0.2.38 langchain-community langchainhub langchain-qdrant langchain_huggingface   langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -qU openai ragas qdrant-client pymupdf pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -q ragas==0.1.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -qU qdrant-client pymupdf pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -qU faiss-cpu unstructured==0.15.7 python-pptx==1.0.2 nltk==3.9.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note - pin the version of pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip uninstall -y pyarrow\n",
        "! pip install -qU sentence_transformers datasets pyarrow==14.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key here: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install --upgrade langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\AI Legal Risk\\AIE4_Midterm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "\n",
        "from myutils.rag_pipeline_utils import SimpleTextSplitter, SemanticTextSplitter, VectorStore, AdvancedRetriever\n",
        "from myutils.ragas_pipeline import RagasPipeline\n",
        "from myutils.finetuning import FineTuneModelAndEvaluateRetriever\n",
        "from myutils.rag_pipeline_utils import load_all_pdfs, get_vibe_check_on_list_of_questions\n",
        "\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1 - Load the Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Make a local copy of the two pdfs needed for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf -O ./data/docs_for_rag/Blueprint-for-an-AI-Bill-of-Rights.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# !wget https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf -O ./data/docs_for_rag/NIST.AI.600-1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load pdfs into Langchain Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from myutils.rag_pipeline_utils import load_all_pdfs, get_vibe_check_on_list_of_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_file_paths = [\n",
        "    './data/docs_for_rag/Blueprint-for-an-AI-Bill-of-Rights.pdf',\n",
        "    './data/docs_for_rag/NIST.AI.600-1.pdf'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded ./data/docs_for_rag/Blueprint-for-an-AI-Bill-of-Rights.pdf with 73 pages \n",
            "loaded ./data/docs_for_rag/NIST.AI.600-1.pdf with 64 pages \n",
            "loaded all files: total number of pages: 137 \n"
          ]
        }
      ],
      "source": [
        "documents = load_all_pdfs(pdf_file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quick Overview of Documents\n",
        "\n",
        "a.  2022: Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People\n",
        "    \n",
        "This is really two docs in one\n",
        "first doc sets up five principles and practices\n",
        "second one is labeled a technical companion; it expands on each principle as well as how to operationalize it; each principle is reiterated, followed by an articulation of what the principle is important, what should be expected of automated systems in regard to following this principle, and examples of how these principles can move into practice.\n",
        "\n",
        "\n",
        "b.  2024: National Institute of Standards and Technology (NIST) Artificial Intelligent Risk Management Framework\n",
        "\n",
        "First part describes the risks as well as Trustworthy AI characteristics to mitigate the risk\n",
        "Second part, in tabular form, describes mitigation plan for each risks; each risk is identified in the table by a serial number based on the first part of the document rather than by the actual name of the risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunking Strategy\n",
        "\n",
        "It is clear that chunking strategies should account for the semantics in the document, as well as the fact that there are strong connections between the first and second parts of the document.  This comment applies to both documents in this assignment.\n",
        "\n",
        "I will examine two alternatives:\n",
        "\n",
        "(a) BASELINE: use the Swiss-army-knife chunking approach: RecursiveCharacterTextSplitter\n",
        "\n",
        "(b) ADVANCED: Semantic Chunking\n",
        "\n",
        "\n",
        "\n",
        "WHY I CHOSE THESE TWO CHUNKING STRATEGIES\n",
        "1. RecursiveCharacterTextSplitter: if the chunk_size and chunk_overlap are set to reasonable numbers, this approach is surprisingly effective across a range of document content.  It is cost-effective, relatively easy to tune if needed, is well-suited for answering queries that are SIMPLE and those that require MULTI-CONTEXT.\n",
        "\n",
        "\n",
        "2. Semantic chunking has great appeal as it groups content that is contiguous and semantically similar in a single chunk.  To that end, the chunk sizes may be rather uneven.  Advantage: It avoids artificially splitting content that may be very similar into multiple chunks which would make the retriever work harder during the retrieval process and/or perhaps miss relevant context.  The downside is that it is not as cost-effective as it requires the use of an LLM during the chunking process.  It is likely to perform well for MULTI-CONTEXT and potentially queries that require REASONING."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Formulate and Load My Test Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_test_questions(filename):\n",
        "    \"\"\"\n",
        "    Loads a text file with questions\n",
        "\n",
        "    Input\n",
        "        name of file which contains a set of questions to test the RAG pipeline\n",
        "    \n",
        "    Output\n",
        "        List of questions\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        all_q = f.read()\n",
        "        all_q_list = all_q.split('\\n')\n",
        "    return all_q_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What process was followed to generate the AI Bill of Rights?',\n",
              " 'What is the AI Bill of Rights?',\n",
              " 'What are the set of five principles in the AI bill of Rights?',\n",
              " 'Who led the formulation of the AI Bill of Rights?',\n",
              " 'What rights do I have to ensure protection against algorithmic discrimination?',\n",
              " 'What rights do I have to ensure that my data stays private?',\n",
              " 'What rights do I have to ensure safe and effective systems?',\n",
              " 'What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?',\n",
              " 'What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?',\n",
              " 'How can organizations put data privacy into practice?',\n",
              " 'How can organizations put into practice protection against algorithmic discrimination',\n",
              " 'How can foreign actors spread misinformation through the use of AI?',\n",
              " 'How can US entities counter the use of AI to spread misinformation during the elections?',\n",
              " 'According to NIST, what are the major risks of generative AI?',\n",
              " 'How can AI developers reduce the risk of hallucinations?',\n",
              " 'What can be done to prevent AI from being used to harm society?',\n",
              " 'Does generative AI have bad environmental impacts?',\n",
              " 'How can we prevent the bad environmental impact of AI?',\n",
              " 'How can we safeguard against AI generating toxic or harmful content?',\n",
              " 'Is AI likely to be biased against certain types of people?  If so, how can we prevent this?']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_test_questions = load_test_questions(filename='./data/rag_questions_and_answers/my_test_questions.txt')\n",
        "my_test_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2 - Quick End-to-end Prototype RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Up RAG Template and RAG Prompt\n",
        "> NOTE that the RAG template and RAG Prompt below will be used throughout this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_template = \"\"\"\n",
        "Use the provided context to answer the following question.\n",
        "If you can't answer the question based on the context, say you don't know.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(template=rag_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Up OpenAI Embeddings and Chat Model For Use in Prototype and for Comparison Throughout This Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_embeddings_small = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "openai_embeddings_small_dimension = 1536\n",
        "openai_embeddings_small_context_window = 8191\n",
        "\n",
        "openai_chat_gpt4omini = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the large embeddings in Semantic Chunking below!!!\n",
        "openai_embeddings_large = OpenAIEmbeddings(model='text-embedding-3-large')\n",
        "openai_embeddings_large_dimension = 3072\n",
        "openai_embeddings_large_context_window = 8191\n",
        "\n",
        "# Set up the lmore performant chat model just in case I decide to use it later...\n",
        "openai_chat_gpt4o = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Snowflake-arctic-embed-m Model \n",
        "#### (Will be Finetuned Later in The Exercise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Why I Chose This Model\n",
        ">\n",
        "> On the AIE4 midterm, we are asked to state why we chose the particular embedding model that we did for finetuning.  These are the criteria I used:\n",
        ">\n",
        "> 1.  PARSIMONY: This model has approx 110 million parameters, so we can feasibly finetune the model with consumer-grade access to GPU and memory resources.  It can be done very quickly in a Colab notebook, for instance, with access to their GPU.  I chose to use the A100 to speed up the process, but the training would work just as well with other GPUs like T4 etc.\n",
        ">\n",
        "> 2.  PERFORMANCE: Despite the far fewer parameters, the model holds its own in terms of performance on benchmark tasks.\n",
        ">\n",
        "> 3.  CONVENIENT ACCESS: This model is conveniently available via Huggingface, so I could leverage the model hub as well as all the libraries that support access to this type of model (SentenceTransformer) as well as all the training/finetuning capabilities.\n",
        ">\n",
        "> 4.  NO-BRAINER REASON: It is an open-source model so we have access to all parameters and configurations needed for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-m\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "arctic_original_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-m\")\n",
        "arctic_original_embeddings_dimension = 768\n",
        "arctic_original_context_window_in_tokens = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Recursive Character Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "chunk_overlap = 300\n",
        "\n",
        "# instantiate baseline text splitter -\n",
        "# NOTE!!! The `SimpleTextSplitter` below is my wrapper around Langchain RecursiveCharacterTextSplitter!!!!\n",
        "# (see module for the code if needed)\n",
        "baseline_text_splitter = \\\n",
        "    SimpleTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, documents=documents)\n",
        "\n",
        "# split text for baseline case\n",
        "baseline_text_splits = baseline_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "557"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(baseline_text_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Semantic Chunking - NOTE Using OpenAI Embeddings Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded 137 to be split \n",
            "returning docs split into 266 chunks \n"
          ]
        }
      ],
      "source": [
        "# instantiate semantic text splitter\n",
        "#  NOTE!!!! SemanticTextSplitter is my wrapper around Langchain SemanticChunker\n",
        "#  see my module for code if needed\n",
        "# NOTE!!! I use openai large embeddings model to get the best possible representation of the semantics of sentences\n",
        "# and to ensure high-quality semantic chunking\n",
        "sem_text_splitter = \\\n",
        "    SemanticTextSplitter(llm_embeddings=openai_embeddings_large, threshold_type=\"interquartile\", documents=documents)\n",
        "\n",
        "# split text for semantic-chunking case\n",
        "sem_text_splits = sem_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vibe Check on My Test Questions - Read This First!!!\n",
        "\n",
        "NOTE:  Four RAG Pipelines are run below!!!  These are:\n",
        "\n",
        "1.  `Demo_Baseline_OpenAI`: This uses baseline chunking (`RecursiveCharacterTextSplitter`) and OpenAI embeddings as a Demo.\n",
        "\n",
        "2.  `Demo_Semantic_OpenAI`: uses semantic chunking (`SemanticChunker`) and OpenAI embeddings as a Demo.\n",
        "\n",
        "3.  `Baseline_Arctic_Original`: uses baseline chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "4.  `Semantic_Arctic_Original`: uses semantic chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "NOTE!!!\n",
        "Later in this notebook, I will finetune the `Snowflake/snowflake-arctic-embed-m` model embeddings and will then compare the finetuned embeddings from this model against the runs in 3. and 4. above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The AI Bill of Rights was generated through extensive consultation with the American public. It consists of five principles and associated practices designed to guide the design, use, and deployment of automated systems, ensuring they align with democratic values and protect civil rights, civil liberties, and privacy. The process involved input from experts across various sectors, including the private sector, governments, and international organizations.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. The framework was developed through extensive consultation with the American public and includes guidance for various organizations, from governments to companies, on how to uphold these values.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not specify the exact five principles in the AI Bill of Rights. Therefore, I don't know the specific principles.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have the right to protection against algorithmic discrimination, which includes the following measures:\n",
            "\n",
            "1. **Equitable Design and Use**: Automated systems should be designed and used in an equitable way, ensuring that they do not contribute to unjustified different treatment based on protected classifications such as race, gender, age, disability, and more.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are required to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design, which includes using representative data and protecting against proxies for demographic features.\n",
            "\n",
            "4. **Accessibility**: Systems should be designed to ensure accessibility for people with disabilities.\n",
            "\n",
            "5. **Disparity Testing**: There should be pre-deployment and ongoing disparity testing and mitigation to identify and address potential biases.\n",
            "\n",
            "6. **Organizational Oversight**: Clear organizational oversight is necessary to ensure compliance with these protections.\n",
            "\n",
            "7. **Independent Evaluation**: Independent evaluations and plain language reporting, including algorithmic impact assessments and disparity testing results, should be performed and made public whenever possible to confirm these protections.\n",
            "\n",
            "These rights aim to ensure that all individuals are treated fairly and equitably in their interactions with automated systems.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Protection from Abusive Data Practices**: You should be protected from abusive data practices through built-in protections.\n",
            "\n",
            "2. **Agency Over Data Use**: You have the right to have agency over how data about you is used, including the ability to make decisions regarding the collection, use, access, transfer, and deletion of your data.\n",
            "\n",
            "3. **Consent and Design Choices**: Designers and developers of automated systems should seek your permission and respect your decisions. Systems should not use design choices that obscure user choice or impose privacy-invasive defaults.\n",
            "\n",
            "4. **Limitations on Data Collection**: Data collection should conform to reasonable expectations, and only data that is strictly necessary for a specific context should be collected.\n",
            "\n",
            "5. **Access to Your Data**: You have the right to access and correct your data, as outlined in laws like the Privacy Act of 1974.\n",
            "\n",
            "6. **Enhanced Protections for Sensitive Data**: There are additional protections for data related to sensitive domains such as health, employment, and education.\n",
            "\n",
            "7. **Oversight of Surveillance Technologies**: You should be free from unchecked surveillance, and surveillance technologies should be subject to heightened oversight.\n",
            "\n",
            "These rights are designed to protect your privacy and ensure that your data is handled ethically and responsibly.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. They should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective based on their intended use. Additionally, these systems should not be designed with the intent or foreseeable possibility of endangering your safety or the safety of your community. You should also have meaningful access to examine the system, and independent evaluations should confirm that the system is safe and effective, with results made public whenever possible.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed when an automated system is being used and to understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of these systems are required to provide accessible documentation that includes clear descriptions of the system's functioning, the role of automation, notice of its use, and explanations of outcomes that are clear, timely, and accessible. Additionally, you should be notified of significant changes in the use case or key functionalities of the system. This right to notice and explanation is essential for ensuring transparency and accountability in the use of AI systems.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This means you should have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. This human consideration should be accessible, equitable, effective, and not impose an unreasonable burden on the public. In some cases, a human alternative may be required by law, especially for reasonable accommodations for people with disabilities.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adhering to Legal Frameworks**: Organizations should comply with laws such as the Privacy Act of 1974, which mandates privacy protections for personal information, limits data retention, and grants individuals the right to access and correct their data.\n",
            "\n",
            "2. **Limiting Data Retention**: They should only retain data that is \"relevant and necessary\" for their statutory purposes, thereby minimizing the scope of data retention.\n",
            "\n",
            "3. **Proactive Risk Management**: Organizations should identify potential privacy harms and manage them to avoid or mitigate risks. This includes deciding not to process data when privacy risks outweigh benefits.\n",
            "\n",
            "4. **Implementing Privacy-Preserving Security Practices**: Utilizing privacy-enhancing technologies, such as cryptography, and establishing fine-grained permissions and access control mechanisms can help protect data and metadata from unauthorized access.\n",
            "\n",
            "5. **Independent Evaluation**: Allowing for independent evaluations of data policies and making these evaluations public can enhance transparency and accountability.\n",
            "\n",
            "6. **User Reporting**: Organizations should provide users with clear, machine-readable reports on what data is being collected or stored about them, ensuring that this information is understandable.\n",
            "\n",
            "7. **Design Choices**: Systems should be designed to include built-in privacy protections by default, ensuring that data collection aligns with reasonable expectations and only collects necessary data. User consent should be sought respectfully, and design choices should not obscure user choices or impose invasive defaults.\n",
            "\n",
            "By following these practices, organizations can better protect data privacy and empower users regarding their personal information.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by taking several proactive and continuous measures, including:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct equity assessments as part of the system design process to identify and address potential biases.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used in automated systems is representative of the diverse populations they affect, avoiding reliance on proxies for demographic features.\n",
            "\n",
            "3. **Accessibility Considerations**: Design and develop systems that are accessible to people with disabilities.\n",
            "\n",
            "4. **Disparity Testing and Mitigation**: Implement pre-deployment and ongoing testing to identify and mitigate any disparities in how different groups are treated by the automated systems.\n",
            "\n",
            "5. **Organizational Oversight**: Establish clear oversight within the organization to monitor and address issues related to algorithmic discrimination.\n",
            "\n",
            "6. **Independent Evaluation**: Conduct independent evaluations and provide plain language reporting through algorithmic impact assessments, which should include results from disparity testing and information on mitigation efforts. These assessments should be made public whenever possible to ensure transparency.\n",
            "\n",
            "By following these practices, organizations can work towards designing and using automated systems in a more equitable manner.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI (GAI) systems to create both text-based disinformation and highly realistic deepfakes, which are synthetic audiovisual content and photorealistic images. These systems can facilitate the deliberate production and dissemination of false or misleading information at scale, especially when the intent is to deceive or cause harm. \n",
            "\n",
            "Additionally, GAI can assist in creating compelling imagery and propaganda that supports disinformation campaigns, potentially increasing their reach and engagement on social media platforms. The ability to manipulate text or images subtly can also enhance the sophistication of these disinformation efforts, allowing for targeted messaging aimed at specific demographics. Overall, the capabilities of GAI make it easier for malicious actors to produce and spread misinformation effectively.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "US entities can counter the use of AI to spread misinformation during elections by integrating tools designed to analyze content provenance and detect data anomalies, verifying the authenticity of digital signatures, and identifying patterns associated with misinformation or manipulation. Additionally, they can disaggregate evaluation metrics by demographic factors to identify discrepancies in how content provenance mechanisms work across diverse populations. Developing a suite of metrics to evaluate structured public feedback exercises and evaluating novel methods and technologies for measuring GAI-related risks can also be effective strategies.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI (GAI) can be categorized into three main areas:\n",
            "\n",
            "1. **Technical / Model Risks**: These include risks from malfunction such as confabulation, dangerous or violent recommendations, data privacy issues, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses malicious uses of GAI, including the dissemination of chemical, biological, radiological, and nuclear (CBRN) information or capabilities, data privacy violations, and the generation of obscene, degrading, or abusive content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These are systemic risks that affect broader societal contexts, including data privacy concerns, environmental impacts, and issues related to intellectual property.\n",
            "\n",
            "Additionally, some risks may cross-cut these categories, indicating their interconnected nature.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "The provided context does not contain specific strategies or methods for AI developers to reduce the risk of hallucinations. Therefore, I don't know how AI developers can reduce the risk of hallucinations based on the given information.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be implemented:\n",
            "\n",
            "1. **Establishing Ethical Principles**: Organizations and governments can adopt principles for the ethical use of AI, such as those outlined in the OECD's recommendations and the U.S. Executive Orders on trustworthy AI.\n",
            "\n",
            "2. **Regulatory Compliance**: AI systems should be lawful, respectful of national values, purposeful, accurate, reliable, safe, understandable, responsible, transparent, and accountable. Regular monitoring and compliance with these standards can help mitigate risks.\n",
            "\n",
            "3. **Governance Tools**: Organizations can apply governance tools and protocols to AI systems, including auditing, assessment, and change-management controls to ensure alignment with organizational values and risk tolerances.\n",
            "\n",
            "4. **Public Engagement**: Engaging with the public and stakeholders to discuss the potential harms and benefits of AI can help shape policies and frameworks that protect society.\n",
            "\n",
            "5. **Innovative Guardrails**: Researchers and companies can develop additional protections and guardrails in the design and use of AI systems to prevent misuse.\n",
            "\n",
            "6. **Frameworks for Ethical Use**: Government agencies can create specific frameworks for the ethical use of AI, as seen with the Department of Energy and the Department of Defense, which oversee the implementation of ethical AI strategies.\n",
            "\n",
            "By implementing these measures, the risks associated with AI can be managed effectively, promoting its benefits while minimizing potential harms.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The context indicates that training, maintaining, and operating generative AI systems are resource-intensive activities with potentially large energy and environmental footprints. For instance, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive compared to non-generative tasks.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, several strategies can be implemented:\n",
            "\n",
            "1. **Model Distillation and Compression**: Creating smaller versions of trained models can reduce energy consumption and carbon emissions during inference. This involves techniques like model distillation or compression.\n",
            "\n",
            "2. **Environmental Impact Assessment**: Organizations should measure or estimate the environmental impacts, such as energy and water consumption, associated with training, fine-tuning, and deploying AI models. This includes verifying trade-offs between resources used at inference time versus those required during training.\n",
            "\n",
            "3. **Incorporating Environmental Considerations in Design**: Documenting anticipated environmental impacts during model development, maintenance, and deployment can guide product design decisions to minimize negative effects.\n",
            "\n",
            "4. **Carbon Capture and Offset Programs**: Verifying the effectiveness of carbon capture or offset programs for AI training and applications can help mitigate environmental impacts. It's also important to address concerns related to green-washing.\n",
            "\n",
            "5. **Governance and Monitoring**: Implementing governance tools and protocols, such as auditing and assessment, can help organizations manage the environmental impacts of AI systems effectively.\n",
            "\n",
            "By adopting these measures, organizations can work towards reducing the environmental footprint of AI technologies.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Feedback Mechanisms**: Use feedback from internal and external users to assess the impact of AI-generated content. Structured feedback mechanisms can help capture user input to detect shifts in quality or alignment with community values.\n",
            "\n",
            "2. **Real-time Auditing**: Implement real-time auditing tools to track and validate the lineage and authenticity of AI-generated data.\n",
            "\n",
            "3. **Content Filters**: Establish content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content. These filters can be rule-based or utilize additional machine learning models to flag problematic inputs and outputs.\n",
            "\n",
            "4. **Monitoring Processes**: Conduct regular monitoring of AI systems and analyze generated content in real-time to identify and address harmful outputs.\n",
            "\n",
            "5. **Incident Response Plans**: Develop and practice incident response plans for addressing the generation of inappropriate or harmful content, adapting processes based on findings to prevent future occurrences.\n",
            "\n",
            "6. **User Reporting**: Evaluate user-reported problematic content and integrate this feedback into system updates to improve content generation.\n",
            "\n",
            "7. **Transparency and Disclosure**: Consider disclosing the use of generative AI to end users in relevant contexts, taking into account the risks and the audience.\n",
            "\n",
            "By implementing these strategies, organizations can better manage the risks associated with AI-generated content and work towards minimizing the potential for harm.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. This bias can manifest in various forms, such as underrepresentation of women and racial minorities in generated images of professionals, and it can perpetuate harmful stereotypes. \n",
            "\n",
            "To prevent this bias, several measures can be taken:\n",
            "1. **Bias Testing**: Companies can implement bias testing as part of their product quality assessment and launch procedures. This can help identify and mitigate biases before products are released to the public.\n",
            "2. **Standards and Guidance**: Federal government agencies can develop standards and guidance for the use of automated systems to help prevent bias.\n",
            "3. **Ongoing Oversight**: Human-based systems should be overseen by governance structures that can update operations to mitigate bias effects.\n",
            "4. **Addressing Datasets and Human Factors**: It is important to focus on the quality of datasets used for training AI and to consider human factors that contribute to bias.\n",
            "\n",
            "These steps can help ensure that AI systems are designed and deployed in a way that minimizes discrimination and promotes fairness.\n"
          ]
        }
      ],
      "source": [
        "baseline_openai_retrieval_chain, baseline_openai_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Demo_Baseline_OpenAI\",\n",
        "                                        embeddings=openai_embeddings_small,  # <- openai embeddings\n",
        "                                        embed_dim=openai_embeddings_small_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=baseline_text_splits, # <- baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public over the course of a year. The White House Office of Science and Technology Policy led this initiative, seeking input from a diverse range of stakeholders, including impacted communities, industry representatives, technology developers, experts from various fields, and policymakers. This input was gathered through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The insights and experiences shared during these engagements played a central role in shaping the Blueprint for an AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, as outlined in the \"Blueprint for an AI Bill of Rights,\" is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems. Its purpose is to protect the rights of the American public in the age of artificial intelligence. The framework aims to ensure that civil rights, civil liberties, and privacy are upheld, and that individuals have equitable access to opportunities and critical resources or services. It addresses concerns related to algorithmic discrimination and emphasizes the importance of protecting both individual and community rights in the context of automated systems. The document serves as a national values statement and toolkit to inform policy and practice regarding the responsible use of AI technologies.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not specify the exact set of five principles in the AI Bill of Rights. Therefore, I don't know the specific principles.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "The White House Office of Science and Technology Policy led the formulation of the AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Equitable Design and Use**: Automated systems should be designed and used in an equitable way, ensuring that they do not contribute to unjustified different treatment based on protected classifications such as race, gender, age, and disability.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are expected to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design, which includes using representative data and protecting against proxies for demographic features.\n",
            "\n",
            "4. **Accessibility**: Systems should be designed to ensure accessibility for people with disabilities.\n",
            "\n",
            "5. **Disparity Testing**: There should be pre-deployment and ongoing disparity testing and mitigation to identify and address any potential biases.\n",
            "\n",
            "6. **Independent Evaluation**: Independent evaluations of automated systems should be conducted to assess potential algorithmic discrimination, and the results should be made public whenever possible.\n",
            "\n",
            "7. **Algorithmic Impact Assessment**: Entities responsible for automated systems should perform and publicly report on algorithmic impact assessments, which include disparity testing results and mitigation information.\n",
            "\n",
            "These rights aim to ensure that you are treated fairly and protected from discrimination in various aspects of life, including hiring, healthcare, and interactions with the criminal justice system.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how your data is collected, used, accessed, transferred, and deleted. This includes the right to give or withhold consent for data collection.\n",
            "\n",
            "2. **Informed Consent**: Consent for data collection should be meaningful and understandable, provided in plain language, and specific to the context of use. Consent requests should not be complicated or misleading.\n",
            "\n",
            "3. **Protection from Abusive Practices**: You should be protected from abusive data practices through built-in protections and design choices that prioritize privacy.\n",
            "\n",
            "4. **Limited Data Collection**: Data collection should conform to reasonable expectations, and only data that is strictly necessary for a specific context should be collected.\n",
            "\n",
            "5. **Enhanced Protections for Sensitive Data**: There are additional protections for data related to sensitive domains such as health, education, and finance. Your data in these areas should only be used for necessary functions and should be subject to ethical review.\n",
            "\n",
            "6. **Transparency and Reporting**: You should have access to reporting that confirms your data decisions have been respected and provides an assessment of the impact of surveillance technologies on your rights.\n",
            "\n",
            "7. **Legal Rights**: Under laws like the Privacy Act of 1974, you have the right to access and correct your data held by federal agencies, and you can seek legal relief if your privacy rights are violated.\n",
            "\n",
            "These rights are designed to protect your privacy and ensure that your data is handled ethically and responsibly.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective automated systems. This includes the following specific rights:\n",
            "\n",
            "1. **Consultation**: You should be consulted during the design, implementation, deployment, acquisition, and maintenance phases of automated systems, particularly with diverse communities and stakeholders to identify concerns and risks.\n",
            "\n",
            "2. **Pre-deployment Testing**: Automated systems should undergo thorough pre-deployment testing, risk identification, and mitigation to ensure they are safe and effective.\n",
            "\n",
            "3. **Ongoing Monitoring**: There should be ongoing monitoring of systems to demonstrate their safety and effectiveness based on intended use and adherence to domain-specific standards.\n",
            "\n",
            "4. **Opt-out Options**: You should have the ability to opt out of automated systems in favor of human alternatives when appropriate.\n",
            "\n",
            "5. **Access to Human Consideration**: You should have access to timely human consideration and remedy if an automated system fails or produces an error.\n",
            "\n",
            "6. **Independent Evaluation**: There should be independent evaluations of automated systems to confirm their safety and effectiveness, with results made public whenever possible.\n",
            "\n",
            "7. **Protection from Harm**: Automated systems should be designed to proactively protect you from foreseeable harms stemming from unintended uses or impacts.\n",
            "\n",
            "These rights aim to ensure that automated systems are developed and operated in a manner that prioritizes your safety and well-being.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice when an automated system is being used that impacts you. This includes being informed about how and why decisions are made by such systems. The entity responsible for the automated system must provide documentation that describes how the system works, the role of automation in decision-making, and the identity of the responsible parties. \n",
            "\n",
            "You should also receive explanations that are tailored to your specific situation, which should be available at the time of the decision or shortly thereafter. These explanations should be designed to help you understand the outcome and, if necessary, contest or appeal decisions made by the automated system. Additionally, the notice and explanations should be kept up-to-date, and you should be notified of any significant changes in the use of the system. \n",
            "\n",
            "Overall, the expectation is that you will have access to information that allows you to understand the impact of automated systems on your rights and opportunities.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This means you should have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. These human alternatives should be accessible, equitable, effective, and not impose an unreasonable burden on the public. Additionally, in sensitive domains such as criminal justice, employment, education, and health, there should be tailored human oversight and consideration for high-risk decisions.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key principles and practices:\n",
            "\n",
            "1. **Privacy by Design and Default**: Automated systems should be designed with privacy protections built in from the outset. This includes assessing privacy risks throughout the development lifecycle and ensuring that data collection is minimized and clearly communicated.\n",
            "\n",
            "2. **Data Minimization**: Organizations should only collect data that is strictly necessary for specific, identified goals. This helps avoid \"mission creep\" where data is used for purposes beyond its original intent.\n",
            "\n",
            "3. **User Consent and Control**: Organizations should seek user permission for data collection and respect their decisions regarding access, use, transfer, and deletion of their data. Consent requests should be clear, brief, and understandable.\n",
            "\n",
            "4. **Transparency**: Entities should provide clear information about what data is being collected, how it will be used, and who has access to it. Users should be able to access their data and correct it if necessary.\n",
            "\n",
            "5. **Data Retention Policies**: Clear timelines for data retention should be established, with data deleted as soon as possible in accordance with legal or policy-based limitations.\n",
            "\n",
            "6. **Risk Identification and Mitigation**: Organizations should proactively identify potential harms related to data collection and implement measures to mitigate these risks.\n",
            "\n",
            "7. **Independent Evaluation and Reporting**: Organizations should allow for independent evaluations of their data practices and provide reports to users about what data is being collected and how it is being used.\n",
            "\n",
            "8. **Enhanced Protections for Sensitive Data**: There should be stricter protections for data related to sensitive domains such as health, education, and finance, ensuring that such data is only used for necessary functions.\n",
            "\n",
            "By adopting these practices, organizations can better protect individual privacy and build trust with their users.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key measures:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct assessments as part of the system design process to identify and address potential biases.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used in algorithms is representative of the diverse populations that the systems will affect, and protect against proxies for demographic features that could lead to discrimination.\n",
            "\n",
            "3. **Accessibility**: Design and develop systems that are accessible to people with disabilities, ensuring that all users can interact with the technology equitably.\n",
            "\n",
            "4. **Disparity Testing and Mitigation**: Perform pre-deployment and ongoing testing to identify disparities in outcomes and take steps to mitigate any identified biases.\n",
            "\n",
            "5. **Organizational Oversight**: Establish clear oversight mechanisms within the organization to monitor the use and impact of automated systems.\n",
            "\n",
            "6. **Independent Evaluation and Reporting**: Conduct independent evaluations and provide plain language reporting, including algorithmic impact assessments that detail disparity testing results and mitigation efforts. This information should be made public whenever possible to ensure transparency.\n",
            "\n",
            "7. **Adoption of Best Practices**: Follow industry best practices for audits and impact assessments to identify potential algorithmic discrimination and enhance public trust.\n",
            "\n",
            "By integrating these practices, organizations can work towards designing and deploying automated systems in a more equitable manner, reducing the risk of algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI systems to produce and disseminate false or misleading content at scale. These systems can create both text-based disinformation and highly realistic deepfakes, which are synthetic audiovisual content and photorealistic images. The sophistication of these AI models allows malicious actors to target specific demographics with tailored disinformation campaigns. For instance, subtle manipulations in text or images can significantly influence human and machine perception, making the misinformation more effective.\n",
            "\n",
            "Additionally, generative AI can assist in creating compelling imagery and propaganda that enhances the reach and engagement of disinformation on social media platforms. This can include fraudulent content designed to impersonate others, further complicating the landscape of trust and information integrity. The potential for AI to generate realistic and persuasive content poses significant risks to public trust in valid information and can have downstream effects on societal stability.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "US entities can counter the use of AI to spread misinformation during elections by implementing several strategies:\n",
            "\n",
            "1. **Establishing Ethical Frameworks**: Government agencies, such as the Department of Defense and the U.S. Intelligence Community, have developed ethical principles for the use of AI. These frameworks can guide the responsible development and deployment of AI systems to prevent misuse in spreading misinformation.\n",
            "\n",
            "2. **Promoting Transparency and Accountability**: The Executive Order on Promoting the Use of Trustworthy Artificial Intelligence requires federal agencies to adhere to principles that ensure AI systems are lawful, accurate, reliable, and transparent. This can help mitigate the risks of misinformation by ensuring that AI-generated content is subject to scrutiny.\n",
            "\n",
            "3. **Research and Development**: Funding research through organizations like the National Science Foundation (NSF) can foster the development of AI systems that are safe, trustworthy, and fair. This includes research on algorithms that can detect and counter misinformation.\n",
            "\n",
            "4. **Risk Management Frameworks**: The National Institute of Standards and Technology (NIST) is developing a risk management framework to address the risks posed by AI, including misinformation. This framework aims to incorporate trustworthiness considerations into the design and evaluation of AI systems.\n",
            "\n",
            "5. **Stakeholder Engagement**: Encouraging meaningful stakeholder engagement in the design and implementation of AI systems can help ensure that diverse perspectives are considered, which may lead to more effective strategies for combating misinformation.\n",
            "\n",
            "6. **Monitoring and Auditing**: Implementing ongoing monitoring and auditing mechanisms for AI systems can help identify and address potential misuse or the spread of misinformation in real-time.\n",
            "\n",
            "By combining these approaches, US entities can create a robust strategy to counter the use of AI in spreading misinformation during elections.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI (GAI) can be categorized into three main areas:\n",
            "\n",
            "1. **Technical / Model Risks**: These include risks such as confabulation, dangerous or violent recommendations, data privacy issues, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses risks related to malicious use, such as the dissemination of chemical, biological, radiological, and nuclear (CBRN) information or capabilities, data privacy violations, and the generation of obscene or degrading content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These risks pertain to broader societal impacts, including data privacy concerns, environmental effects, and issues related to intellectual property.\n",
            "\n",
            "Additionally, some risks are cross-cutting between these categories, and there are challenges in estimating risks due to the complexity and uncertainty surrounding GAI systems.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by implementing several strategies, including:\n",
            "\n",
            "1. **Establishing Policies and Procedures**: Organizations should have clear policies that define roles and responsibilities for oversight of AI systems, including independent evaluations or assessments of generative AI models.\n",
            "\n",
            "2. **Testing and Evaluation**: Regular testing, validation, and red-teaming of generative AI systems can help identify and mitigate risks associated with hallucinations.\n",
            "\n",
            "3. **User Feedback Mechanisms**: Implementing robust user feedback mechanisms allows for the collection of insights on AI performance, which can inform improvements and adjustments to the systems.\n",
            "\n",
            "4. **Risk Measurement and Improvement**: Continuous improvement processes for risk measurement should be established, focusing on the explainability and transparency of AI systems. This includes using techniques like gradient-based attributions and prompt engineering.\n",
            "\n",
            "5. **Standardized Measurement Protocols**: Developing standardized protocols for risk measurement in the context of use can help ensure that evaluations are consistent and effective.\n",
            "\n",
            "By following these strategies, AI developers can better manage the risks associated with hallucinations in generative AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be implemented:\n",
            "\n",
            "1. **Establish Ethical Frameworks**: Government agencies, such as the Department of Energy and the Department of Defense, have developed ethical principles and frameworks for the responsible use of AI. These frameworks guide the development and deployment of AI systems to ensure they align with societal values and ethical standards.\n",
            "\n",
            "2. **Risk Management Frameworks**: The National Institute of Standards and Technology (NIST) is developing a risk management framework that incorporates trustworthiness considerations into the design, development, and evaluation of AI systems. This framework aims to address risks related to accuracy, explainability, reliability, and bias.\n",
            "\n",
            "3. **Transparency and Accountability**: Implementing strong transparency requirements, such as those seen in Idaho's legislation on pretrial risk assessments, can help ensure that AI systems are free from bias and that their decision-making processes are open to public scrutiny.\n",
            "\n",
            "4. **Stakeholder Engagement**: Encouraging meaningful stakeholder engagement in the design and implementation of AI systems can help identify potential risks and ensure that diverse perspectives are considered.\n",
            "\n",
            "5. **Regular Monitoring and Auditing**: AI systems should be regularly monitored and audited to assess their performance and mitigate risks over time. This includes establishing policies for independent evaluations and assessments of AI models.\n",
            "\n",
            "6. **Education and Training**: Fostering a critical thinking and safety-first mindset among developers and users of AI systems can help minimize potential negative impacts.\n",
            "\n",
            "7. **Legal and Regulatory Compliance**: Developing rigorous methodologies for software systems that ensure compliance with legal and regulatory standards can help mitigate risks associated with AI deployment.\n",
            "\n",
            "By implementing these strategies, society can work towards ensuring that AI technologies are used safely and ethically, minimizing the potential for harm.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The training, maintenance, and operation of generative AI systems are resource-intensive and can have large energy and environmental footprints. For instance, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive compared to non-generative tasks. While there are methods to reduce environmental impacts, such as model distillation or compression, the training and tuning of these models still contribute to their overall environmental footprint.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, several measures can be taken:\n",
            "\n",
            "1. **Assess Environmental Impacts**: Document and assess the anticipated environmental impacts of AI model development, maintenance, and deployment during product design decisions.\n",
            "\n",
            "2. **Measure Resource Consumption**: Measure or estimate the environmental impacts, such as energy and water consumption, for training, fine-tuning, and deploying AI models. This includes verifying trade-offs between resources used during inference versus those required during training.\n",
            "\n",
            "3. **Implement Carbon Capture Programs**: Verify the effectiveness of carbon capture or offset programs for AI training and applications, and address concerns related to green-washing.\n",
            "\n",
            "4. **Develop Smaller Models**: Utilize methods such as model distillation or compression to create smaller versions of trained models, which can reduce environmental impacts during inference.\n",
            "\n",
            "5. **Establish Frameworks for Ethical Use**: Follow frameworks like the NIST AI Risk Management Framework, which incorporates trustworthiness considerations, including environmental sustainability, into the design, development, and evaluation of AI systems.\n",
            "\n",
            "By implementing these strategies, organizations can mitigate the environmental footprint associated with AI technologies.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Monitoring and Testing**: Regularly monitor the robustness and effectiveness of risk controls and mitigation plans through methods like red-teaming, field testing, and user feedback mechanisms.\n",
            "\n",
            "2. **Content Comparison**: Compare AI system outputs against pre-defined organizational risk tolerance, guidelines, and principles, and review AI-generated content to ensure compliance with these standards.\n",
            "\n",
            "3. **Documenting Data Sources**: Maintain documentation of training data sources to trace the origin and provenance of AI-generated content, which helps in understanding potential biases and harmful outputs.\n",
            "\n",
            "4. **Feedback Loops**: Evaluate and implement feedback loops between AI content provenance and human reviewers, updating processes as necessary to ensure ongoing effectiveness.\n",
            "\n",
            "5. **Bias Evaluation**: Assess AI-generated content for representational biases and employ techniques such as re-sampling, re-ranking, or adversarial training to mitigate these biases.\n",
            "\n",
            "6. **Due Diligence**: Conduct thorough analyses of AI outputs for harmful content, misinformation, and other dangerous material, ensuring that any problematic content is identified and addressed.\n",
            "\n",
            "7. **Content Filters**: Implement content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content, utilizing both rule-based systems and machine learning models to flag problematic inputs and outputs.\n",
            "\n",
            "8. **Real-Time Monitoring**: Establish real-time monitoring processes to analyze generated content for performance and trustworthiness, triggering alerts for human intervention when deviations from desired standards occur.\n",
            "\n",
            "By applying these strategies, organizations can better manage the risks associated with AI-generated content and reduce the likelihood of producing toxic or harmful outputs.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. The context provides several examples of how biases can manifest in AI systems, such as healthcare algorithms discriminating against Black patients, automated sentiment analyzers being biased against Jews and gay individuals, and hiring algorithms reinforcing racial and gender stereotypes.\n",
            "\n",
            "To prevent this bias, the context suggests several strategies:\n",
            "\n",
            "1. **Bias Testing**: Companies can implement bias testing as part of their product quality assessment and launch procedures to identify and mitigate biases before products are released.\n",
            "\n",
            "2. **Algorithmic Bias Safeguards**: Initiatives have been developed to create structured questionnaires for businesses to evaluate the data and models used in their AI systems, focusing on training data, biases identified, and mitigation steps.\n",
            "\n",
            "3. **Standards and Guidelines**: Organizations have developed guidelines to incorporate accessibility criteria into technology design processes, which can help address biases.\n",
            "\n",
            "4. **Impact Assessments and Audits**: Non-profits and companies can conduct audits and impact assessments to identify potential algorithmic discrimination and provide transparency in the mitigation of such biases.\n",
            "\n",
            "5. **Socio-Technical Perspective**: Addressing bias requires understanding the socio-technical context in which AI operates, including the datasets used, testing and evaluation processes, and human factors involved in AI development.\n",
            "\n",
            "Overall, ongoing efforts are needed to ensure that AI systems are designed and deployed in an equitable manner to protect against algorithmic discrimination.\n"
          ]
        }
      ],
      "source": [
        "sem_openai_retrieval_chain, sem_openai_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Demo_Semantic_OpenAI\",\n",
        "                                        embeddings=openai_embeddings_small, # <- openai embeddings\n",
        "                                        embed_dim=openai_embeddings_small_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=sem_text_splits, # <- semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The context does not provide specific details about the process followed to generate the AI Bill of Rights. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework designed to assist governments and the private sector in implementing principles that protect civil rights, civil liberties, and privacy in the context of automated systems. It aims to ensure that the transformative potential of AI technologies is harnessed to improve lives while preventing potential harms. The framework includes expectations and recommendations for developing technical standards and practices tailored to specific sectors and contexts. It serves as a guide for moving from principles to practice in the regulation and use of AI technologies.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Organizations should conduct assessments during the design phase of automated systems to identify and mitigate potential biases.\n",
            "\n",
            "2. **Use of Representative Data**: Systems should be designed using data that accurately represents the demographics of the population to avoid discrimination.\n",
            "\n",
            "3. **Protection Against Proxies**: Measures should be taken to ensure that proxies for demographic features do not lead to discrimination.\n",
            "\n",
            "4. **Accessibility**: Systems should be designed to be accessible to people with disabilities.\n",
            "\n",
            "5. **Disparity Testing and Mitigation**: Ongoing testing for disparities should be conducted both before deployment and continuously after, with measures taken to address any identified issues.\n",
            "\n",
            "6. **Organizational Oversight**: There should be clear oversight within organizations to ensure compliance with these protections.\n",
            "\n",
            "7. **Independent Evaluation**: An independent evaluation of the systems should be performed, including algorithmic impact assessments that report on disparity testing results and mitigation efforts, which should be made public whenever possible.\n",
            "\n",
            "These rights aim to protect individuals and communities from algorithmic discrimination and ensure that automated systems are used and designed equitably.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how data about you is used, including the ability to give or withdraw consent for data collection and use.\n",
            "\n",
            "2. **Built-in Protections**: Systems should include default protections against abusive data practices, ensuring that only data strictly necessary for a specific context is collected.\n",
            "\n",
            "3. **Transparency and Clarity**: Designers and developers should seek your permission and respect your decisions regarding the collection, use, access, transfer, and deletion of your data.\n",
            "\n",
            "4. **Privacy by Design**: Systems should be designed to avoid obfuscating user choices and should not burden users with defaults that are invasive to privacy.\n",
            "\n",
            "5. **Ethical Oversight**: There should be ethical reviews and prohibitions on the use of data and related inferences, protecting you from unchecked surveillance.\n",
            "\n",
            "6. **Access to Reporting**: You should have access to reports that confirm your data decisions have been respected and assess the impact of surveillance technologies on your rights and opportunities.\n",
            "\n",
            "These rights aim to protect you from violations of privacy and ensure that your data is handled responsibly.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns and risks. These systems must undergo pre-deployment testing, risk identification, and ongoing monitoring to ensure they are safe and effective for their intended use. You also have the right to not have a system deployed if it poses a risk to your safety or the safety of your community. Additionally, automated systems should be designed to proactively protect you from potential harms.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed when an automated system is being used and to understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of these systems are required to provide accessible documentation that includes:\n",
            "\n",
            "1. Clear descriptions of the overall system functioning and the role of automation.\n",
            "2. Notice that such systems are in use.\n",
            "3. Information about the individual or organization responsible for the system.\n",
            "4. Explanations of outcomes that are clear, timely, and accessible.\n",
            "\n",
            "Additionally, you should be notified of significant changes in use cases or key functionalities. You have the right to know how and why an outcome affecting you was determined by the automated system, including when it is not the sole input determining the outcome. The explanations provided should be technically valid, meaningful, and useful to you and others who need to understand the system.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by proactively identifying potential harms and managing them to avoid, mitigate, and respond appropriately to identified risks. This includes determining not to process data when privacy risks outweigh the benefits and implementing measures to mitigate acceptable risks. Organizations should also follow privacy-preserving security best practices, such as using privacy-enhancing technologies and fine-grained permissions and access control mechanisms to ensure that data does not leak beyond the specific consented use case. Additionally, consent for data collection should be meaningful, understandable, and provided in plain language, ensuring that users have agency over their data. Enhanced protections should be in place for sensitive domains, and organizations should avoid design choices that obscure user choice or impose privacy-invasive defaults.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key strategies:\n",
            "\n",
            "1. **Monitoring and Assessment**: Regularly assess automated systems to determine if they have led to algorithmic discrimination. This should be done frequently, especially for riskier and higher-impact systems, and whenever unusual patterns in results are observed.\n",
            "\n",
            "2. **Use of Proxy Features**: Ensure that any proxy features used in the algorithms are not given undue weight. Organizations should also explore alternative attributes that can be used instead of potentially discriminatory ones.\n",
            "\n",
            "3. **Demographic Analysis**: Conduct assessments that take into account demographic information of impacted individuals. This can involve testing with a sample of users or conducting qualitative user experience research to understand the impact of the automated systems.\n",
            "\n",
            "4. **Disparity Mitigation**: If assessments reveal disparities, organizations should implement additional measures to mitigate these disparities. If equity standards are not being met and cannot be improved, reverting to earlier procedures that adhered better to equity standards may be necessary.\n",
            "\n",
            "5. **Guidance and Principles**: Follow established principles and guidelines that help incorporate protections against algorithmic discrimination into policy and practice, ensuring that these considerations are integrated into the technological design process.\n",
            "\n",
            "By following these steps, organizations can better protect against algorithmic discrimination and promote equity in their automated systems.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, strategies might include focusing on safety as a primary concern, implementing data-driven approaches, ensuring valid and accurate systems, and addressing issues such as confirmation bias and the potential for unjust outcomes. Additionally, there should be an emphasis on the impact of AI systems on individuals and communities, with mechanisms in place to mitigate negative effects such as job loss and surveillance. Effective oversight, human review, and management of AI technologies are also essential to manage their risks effectively.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts due to high compute resource utilization in training or operating models, which can adversely affect ecosystems.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is important to address three broad challenges for mitigating bias: improving datasets, enhancing testing and evaluation processes, and considering human factors. Additionally, a socio-technical perspective can help in identifying and managing AI bias effectively.\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_original_retrieval_chain, baseline_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=baseline_text_splits, # <- baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to seek and distill input from various stakeholders, including impacted communities, industry representatives, technology developers, experts across different fields, and policymakers. This input was gathered through panel discussions, public listening sessions, meetings, a formal request for information, and contributions to a publicly accessible email address. The discussions highlighted both the transformative potential of AI and the necessity of preventing its harms, which played a central role in shaping the Blueprint for an AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to align automated systems with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to incorporate these protections into their policies and practices.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not provide the specific set of five principles in the AI Bill of Rights. Therefore, I don't know the answer.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "Based on the provided context, you have the right to ensure safe and effective systems through the following means:\n",
            "\n",
            "1. **Risk Management**: You can mitigate, transfer, or avoid risks that exceed your organization's risk tolerances, as indicated in the NIST document.\n",
            "\n",
            "2. **Human Oversight**: The context emphasizes the importance of extensive human oversight in settings involving artificial intelligence, which suggests that you have the right to demand such oversight to ensure safety and effectiveness.\n",
            "\n",
            "These rights imply a proactive approach to managing risks and ensuring that human judgment plays a critical role in the deployment of AI systems.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by ensuring that data is only used for its intended purpose, as highlighted in the context. For example, if data is provided to an entity like a health insurance company for payment facilitation, it should only be utilized for that specific purpose. Additionally, organizations should implement measures to mitigate, transfer, or avoid risks that exceed their risk tolerances, which is essential for maintaining data privacy. Regular information sharing, change management records, and maintaining version history and metadata can also support effective data privacy practices.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "The context does not provide specific measures to prevent AI from being used to harm society. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement reasonable measures that can prevent, flag, or take other actions in response to outputs that reproduce particular training data. This includes monitoring for outputs that may be plagiarized, trademarked, patented, licensed content, or trade secret material. Regular information sharing, change management records, version history, and metadata can also empower AI actors in responding to and managing AI incidents effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is critically important to ensure extensive human oversight in AI systems. Additionally, implementing regular information sharing, change management records, and maintaining version history and metadata can help manage and respond to AI incidents effectively.\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_original_retrieval_chain, sem_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=sem_text_splits, # <- semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Summary of The Anecdotal Responses to My Questions Above\n",
        "\n",
        "Note that when I use OpenAI Embeddings, the RAG Pipeline does a pretty decent job of responding to the test questions.  This is true for the baseline chunking as well as semantic chunking.\n",
        "\n",
        "However, the results appear to be only marginally ok for the two cases when I used the `snowflake-arctic-embed-m` embeddings out-of-the-box.  Of course, it is not as good as OpenAI embeddings.  But the other thing I noticed is that the context window for this model's embeddings is only 512 (compared to 8191 for OpenAI embeddings).  We should expect that in the formal RAGAS evaluation (coming up next), this model does pretty poorly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save Test Questions and Answers in File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def save_df_to_csv(q_a_data, csvfilename):\n",
        "    qa_df = pd.DataFrame(q_a_data, \n",
        "                         columns=['questions', 'answers'])\n",
        "    \n",
        "    filepath = Path(csvfilename)\n",
        "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "    qa_df.to_csv(filepath, index=False)\n",
        "    return\n",
        "\n",
        "\n",
        "save_df_to_csv(baseline_openai_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/baseline_openai_test_q_and_a.csv')\n",
        "\n",
        "save_df_to_csv(sem_openai_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/sem_openai_test_q_and_a.csv')\n",
        "\n",
        "save_df_to_csv(baseline_arctic_original_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/baseline_arctic_original_test_q_and_a.csv')\n",
        "\n",
        "save_df_to_csv(sem_arctic_original_q_and_a, \n",
        "               csvfilename='./data/rag_questions_and_answers/sem_arctic_original_test_q_and_a.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DETOUR - TASK 2\n",
        "\n",
        "> At this stage, we have built quite a bit of the functionality needed for the Fast Prototype.\n",
        ">\n",
        "> There is a separate `app_v1.py` script and other resources around it (such as `Dockerfile`, `requirements.txt`, etc.) that were created at this stage.  *A fast prototype of the app was deployed to Huggingface Spaces.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loom Video to Demo the Fast Prototype of Working App on HF Spaces\n",
        "\n",
        "1.  Here is a link to the Loom video showing a demo of the prototype:\n",
        "\n",
        "        https://www.loom.com/share/3396b23b33f445ffb531ddcc8858487e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The stack I chose and some thoughts on it!!\n",
        "\n",
        "Here’s my stack:\n",
        "1. PDF document loader:  `PyMuPDF` to load pdf documents – I’ve found it to be acceptable as a general-purpose PDF loader; it is also conveniently packaged with Langchain tools as one of several PDF loaders.\n",
        "\n",
        "2. Chunking:  `Langchain`: for general purpose chunking (recursive character text splitter) as well as semantic chunking (it is their implementation of an open-source idea).  Extremely convenient and easy to use their different text splitters.\n",
        "\n",
        "3. Vector Store: `Qdrant`: I only implemented an in-memory vector store for this project, but I chose this application because it can potentially also be scaled very easily for industrial-strength use-cases.\n",
        "\n",
        "4. Retrieval chain (retriever, prompt and LLM): `Langchain’s LCEL`: to build out the retrieval chain; this is extremely convenient not only for fast prototyping but also scales very easily.\n",
        "\n",
        "5. Embeddings to vectorize the text\n",
        "-       OpenAI Embeddings – for the fast prototype, I used OpenAI text-embedding-3-small model embeddings.  These are very good as a general-purpose set of embeddings.  They are medium-sized vectors (dimension of 1536) and have decent context length (8191), so they can be used to encode fairly long chunks of text well.\n",
        "-       Finetuned Snowflake/snowflake-arctic-embed-m Embeddings: the base embeddings perform quite well; the model is parsimonious (110 million parameters) so it can be easily finetuned with consumer-grade resources; model is conveniently distributed via Huggingface\n",
        "-       Important to note – it was necessary to finetune the embeddings as the content of the corpus has fairly unique vocabulary that is unique to this domain, so in my stack I use the finetuned version of the model.\n",
        "\n",
        "6. OpenAI Chat Model: I used `gpt-4o-mini` as the LLM chat model throughout this project.  It is highly performant, cost-effective and quite fast.\n",
        "\n",
        "7. Web app: `Chainlit`: A very easy-to-use LLM-customized web-application; using Chainlit made it very easy to deploy the app on a hosting service such as Huggingface Spaces.\n",
        "\n",
        "8. Web hosting: `Hugging Face spaces`: HF has set up HF spaces as a Github repo that automatically detects when there are pushes or changes to the underlying app and immediately restarts the app.  For our purposes, this web hosting service was quite adequate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3 - Synthetically Generate Test Questions Using the RAGAS Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Up RAGAS Pipeline Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM models used in RAGAS pipeline\n",
        "ragas_generator_llm_model = 'gpt-4o-mini'\n",
        "ragas_critic_llm_model = 'gpt-4o-mini'\n",
        "\n",
        "# embeddings used for RAGAS pipeline\n",
        "ragas_openai_embeddings_model = 'text-embedding-3-small'\n",
        "\n",
        "# text splitter params\n",
        "ragas_chunk_size = 1500\n",
        "ragas_chunk_overlap = 500\n",
        "\n",
        "# number of qa pairs needed - reduce if running into rate limit issues\n",
        "ragas_number_of_qa_pairs = 20\n",
        "\n",
        "# initialize distributions - desired distribution of question types\n",
        "distributions = {\n",
        "    simple: 0.5,\n",
        "    multi_context: 0.4,\n",
        "    reasoning: 0.1\n",
        "}\n",
        "\n",
        "# name of file to persist RAGAS Q&A on disk\n",
        "ragas_testset_filename = \"./data/rag_questions_and_answers/ragas_questions_and_answers.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FLAG TO INDICATE IF RAGAS TESTSET SHOULD BE GENERATED IN THIS RUN\n",
        "# IF it is run, note the cost and time estimate below!!!\n",
        "generate_ragas_testset_now = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up list of RAGAS metrics used below\n",
        "ragas_metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Instantiate RAGAS Pipeline, Run Pipeline, Generate Test Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE - this cell will incur significant cost due to SDG's use of OpenAI models\n",
        "# Time taken on my local machine: ~ 15 mins\n",
        "\n",
        "ragas_pipeline = RagasPipeline(\n",
        "        generator_llm_model=ragas_generator_llm_model,\n",
        "        critic_llm_model=ragas_critic_llm_model,\n",
        "        embedding_model=ragas_openai_embeddings_model,\n",
        "        number_of_qa_pairs=ragas_number_of_qa_pairs,\n",
        "        chunk_size=ragas_chunk_size,\n",
        "        chunk_overlap=ragas_chunk_overlap,\n",
        "        documents=documents,\n",
        "        distributions=distributions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if generate_ragas_testset_now is True:\n",
        "    ragas_testset_df = ragas_pipeline.generate_testset()\n",
        "    ragas_testset_df.to_csv(ragas_testset_filename)\n",
        "else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load RAGAS Q&A from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "ragas_test_df = pd.read_csv(ragas_testset_filename)\n",
        "ragas_test_questions = ragas_test_df[\"question\"].values.tolist()\n",
        "ragas_test_groundtruths = ragas_test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate RAG Pipeline Using RAGAS Generated Synthetic Questions\n",
        "\n",
        "NOTE!!!\n",
        "\n",
        "The four cells below evaluate the four RAG pipelines built above:\n",
        "1.  Baseline chunking plus OpenAI embeddings\n",
        "2.  Semantic chunking plus OpenAI Embeddings\n",
        "3.  Baseline chunking plus Snowflake/snowflake-arctic-embed-m embeddings\n",
        "4.  Semantic chunking plus Snowflake/snowflake-arctic-embed-m embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [01:05<00:00,  1.22it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_openai_results, baseline_openai_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(baseline_openai_retrieval_chain, # <- baseline chunking + openai embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [01:36<00:00,  1.21s/it]\n"
          ]
        }
      ],
      "source": [
        "sem_openai_results, sem_openai_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(sem_openai_retrieval_chain, # <- semantic chunking + openai embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:54<00:00,  1.48it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_original_results, baseline_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(baseline_arctic_original_retrieval_chain, # <- baseline chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:55<00:00,  1.44it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_original_results, sem_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(sem_arctic_original_retrieval_chain, # <- semantic chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compare The Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>BaselineChunkOpenAI</th>\n",
              "      <th>SemanticChunkOpenAI</th>\n",
              "      <th>BaselineChunkArcticOrig</th>\n",
              "      <th>SemanticChunkArcticOrig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.928955</td>\n",
              "      <td>0.951712</td>\n",
              "      <td>0.645179</td>\n",
              "      <td>0.280258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.974849</td>\n",
              "      <td>0.969732</td>\n",
              "      <td>0.729138</td>\n",
              "      <td>0.297121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.933542</td>\n",
              "      <td>0.961042</td>\n",
              "      <td>0.614514</td>\n",
              "      <td>0.378542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.715000</td>\n",
              "      <td>0.154167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric  BaselineChunkOpenAI  SemanticChunkOpenAI  \\\n",
              "0       faithfulness             0.928955             0.951712   \n",
              "1   answer_relevancy             0.974849             0.969732   \n",
              "2  context_precision             0.933542             0.961042   \n",
              "3     context_recall             0.875000             0.916667   \n",
              "\n",
              "   BaselineChunkArcticOrig  SemanticChunkArcticOrig  \n",
              "0                 0.645179                 0.280258  \n",
              "1                 0.729138                 0.297121  \n",
              "2                 0.614514                 0.378542  \n",
              "3                 0.715000                 0.154167  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_baseline_openai = pd.DataFrame(list(baseline_openai_results.items()), columns=['Metric', 'BaselineChunkOpenAI'])\n",
        "df_sem_openai = pd.DataFrame(list(sem_openai_results.items()), columns=['Metric', 'SemanticChunkOpenAI'])\n",
        "df_merged_openai = pd.merge(df_baseline_openai, df_sem_openai, on='Metric')\n",
        "\n",
        "df_baseline_arctic_original = pd.DataFrame(list(baseline_arctic_original_results.items()), columns=['Metric', 'BaselineChunkArcticOrig'])\n",
        "df_sem_arctic_original = pd.DataFrame(list(sem_arctic_original_results.items()), columns=['Metric', 'SemanticChunkArcticOrig'])\n",
        "df_merged_arctic_original = pd.merge(df_baseline_arctic_original, df_sem_arctic_original, on='Metric')\n",
        "\n",
        "df_all_merged = pd.merge(df_merged_openai, df_merged_arctic_original, on='Metric')\n",
        "\n",
        "df_all_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of RAGAS Evaluation of RAG Pipelines Built So Far\n",
        "\n",
        "The table above shows the results of the four pipelines that I’ve carried this far.  \n",
        "\n",
        "The two on the left are using OpenAI embeddings (baseline chunking and semantic chunking) and the two on the right are using the original downloaded version of “snowflake-arctic-embed-m” model embeddings.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Takeaways:\n",
        "---------\n",
        "1.  OpenAI dominates the snowflake-arctic-embed-m embedding based pipelines; not at all a surprise.\n",
        "\n",
        "2.  Retrieval-based measures show a slight improvement for OpenAI embeddings when we use semantic chunking rather than simple chunks on text splitting.  This is to be expected as the semantic chunks are organizing chunks based on semantic content precisely so that retrieval is better.\n",
        "\n",
        "3.  Generation-based measures such as faithfulness (measuring factual accuracy of generated answer) and answer relevancy (relevance of answer to question) also suffer with poor retrieval performance.  Notice the poor performace of the snowflake-arctic-embed-m model’s generation measures and how the retrieval measures are also pretty low.\n",
        "\n",
        "4.  Semantic chunking adversely affects the performance of snowflake-arctic-embed-m model.  I suspect it might be due to the context window of the model being rather low at 512 tokens.  It is possible that semantic chunks, at least some of them, are long.  The recursive text splitter may be better suited to smaller context length embedding models as one can control the size of the chunks relatively easily.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions about Effectiveness and Performance of RAG Ppelines so far\n",
        "\n",
        "1.  OpenAI performs very well out-of-the-box and is a great default choice for many such applications.\n",
        "\n",
        "2.  If we want to use open-source models like snowflake-arctic-embed-m in specialized RAG pipelines, we will need to finetune the model.\n",
        "\n",
        "3.  We enter the finetuning process (below) with healthy skepticism as the base model does not perform well and its context window is rather small (512 compared to 8191).  Nonetheless, it is worth a shot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4 - Fine-tuning Embeddings for RAG and Pull Down Finetuned Embeddings\n",
        "\n",
        "#### *NOTE: As mentioned at start of this notebook, I built a separate pipeline to do the finetuning of the embedding model.  Please refer to that notebook for the full code for finetuning.  Below, I pull down the finetuned model embeddings from my HF repo for use in the remainder of this notebook.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### [Here](vc_completed_aie4_midterm_finetuning_embeddings_pipeline.ipynb) is a link to the notebook that has the finetuning pipeline end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### And [here](https://huggingface.co/vincha77/finetuned_arctic) is a link to the Huggingface Hub where I have placed the results of my finetuned model called `vincha77/finetuned_arctic` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why I Chose to Finetune the `snowflake-arctic-embed-m` Model\n",
        "\n",
        "On the AIE4 midterm, we are asked to state why we chose the particular embedding model that we did for finetuning.  These are the criteria I used:\n",
        "\n",
        "1.  PARSIMONY: This model has approx 110 million parameters, so we can feasibly finetune the model with consumer-grade access to GPU and memory resources.  It can be done very quickly in a Colab notebook, for instance, with access to their GPU.  I chose to use the A100 to speed up the process, but the training would work just as well with other GPUs like T4 etc.\n",
        "\n",
        "2.  PERFORMANCE: Despite the far fewer parameters, the model holds its own in terms of performance on benchmark tasks.\n",
        "\n",
        "3.  CONVENIENT ACCESS: This model is conveniently available via Huggingface, so I could leverage the model hub as well as all the libraries that support access to this type of model (SentenceTransformer) as well as all the training/finetuning capabilities.\n",
        "\n",
        "4.  NO-BRAINER REASON: It is an open-source model so we have access to all parameters and configurations needed for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "## code here to pull from hub\n",
        "model_id = \"Vira21/finetuned_arctic\"\n",
        "arctic_finetuned_model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "arctic_finetuned_embeddings = HuggingFaceEmbeddings(model_name=\"Vira21/finetuned_arctic\")\n",
        "arctic_finetuned_embeddings_dimension = 768\n",
        "arctic_finetuned_context_window_in_tokens = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the TEST SET that was created during model finetuning (training and validation also saved here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('./data/finetuning_data/test_dataset.jsonl', \"r\") as f:\n",
        "    test_json = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiate a model evaluator to compute hit rate using testdata and different embeddings models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE that the class being instantiated below is used extensively during the finetuning process\n",
        "# I am only instantiating it to use the method defined there to run the Evaluations on the test dataset\n",
        "evr = FineTuneModelAndEvaluateRetriever(train_data=None, val_data=None, test_data=test_json, batch_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 414/414 [02:51<00:00,  2.41it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9371980676328503"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "te3_results = evr.evaluate_embeddings_model(openai_embeddings_small, top_k_for_retrieval=5)\n",
        "\n",
        "te3_results_df = pd.DataFrame(te3_results)\n",
        "\n",
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 414/414 [00:08<00:00, 51.63it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5265700483091788"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arctic_embed_m_results = evr.evaluate_embeddings_model(arctic_original_embeddings, top_k_for_retrieval=5)\n",
        "\n",
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)\n",
        "\n",
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 414/414 [00:07<00:00, 55.74it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9710144927536232"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetuned_results = evr.evaluate_embeddings_model(arctic_finetuned_embeddings, top_k_for_retrieval=5)\n",
        "\n",
        "finetuned_results_df = pd.DataFrame(finetuned_results)\n",
        "\n",
        "finetuned_hit_rate = finetuned_results_df[\"is_hit\"].mean()\n",
        "finetuned_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of Hit Rate Metric for the Three Pipelines\n",
        "\n",
        "1.  OpenAI `text-embeddings-3-small` model hit rate:    0.937\n",
        "2.  Snowflake `snowflake-arctic-embed-m` hit rate:      0.526\n",
        "3.  Finetuned version `finetuned_arctic` hit rate:      0.971"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway from these results\n",
        "\n",
        "1.  Another confirmation that OpenAI `text-embeddings-3-small` model is pretty good out-of-the-box.\n",
        "\n",
        "2.  Another confirmation that `snowflake-arctic-embed-m` model embeddings are not that great out-of-the-box.\n",
        "\n",
        "3.  The key takeaway though is that `FINETUNING WORKS`!!!  The `finetuned_arctic` model embeddings outperform OpenAI embeddings on this test corpus, quite an incredible feat!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vibe Check on My Test Questions\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on my test set of questions that I formulated first!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Recursive Character Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_chunk_size = 600\n",
        "new_chunk_overlap = 200\n",
        "\n",
        "# instantiate baseline text splitter -\n",
        "# NOTE!!! The `SimpleTextSplitter` below is my wrapper around Langchain RecursiveCharacterTextSplitter!!!!\n",
        "# (see module for the code if needed)\n",
        "new_baseline_text_splitter = \\\n",
        "    SimpleTextSplitter(chunk_size=new_chunk_size, chunk_overlap=new_chunk_overlap, documents=documents)\n",
        "\n",
        "# split text for baseline case\n",
        "new_baseline_text_splits = new_baseline_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "936"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(new_baseline_text_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chunk Documents Using Semantic Chunking - NOTE Using OpenAI Embeddings Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded 137 to be split \n",
            "returning docs split into 265 chunks \n"
          ]
        }
      ],
      "source": [
        "# instantiate semantic text splitter\n",
        "#  NOTE!!!! SemanticTextSplitter is my wrapper around Langchain SemanticChunker\n",
        "#  see my module for code if needed\n",
        "# NOTE!!! I use openai large embeddings model to get the best possible representation of the semantics of sentences\n",
        "# and to ensure high-quality semantic chunking\n",
        "new_sem_text_splitter = \\\n",
        "    SemanticTextSplitter(llm_embeddings=openai_embeddings_large, threshold_type=\"interquartile\", documents=documents)\n",
        "\n",
        "# split text for semantic-chunking case\n",
        "new_sem_text_splits = new_sem_text_splitter.split_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vibe Check on My Test Questions - Read This First!!!\n",
        "\n",
        "NOTE:  Four RAG Pipelines are run below!!!  These are:\n",
        "\n",
        "1.  `Baseline_Arctic_Original`: uses baseline chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "2.  `Baseline_Arctic_Finetuned`: uses baseline chunking and `Finetuned_Arctic` model embeddings.\n",
        "\n",
        "3.  `Semantic_Arctic_Original`: uses semantic chunking and `Snowflake/snowflake-arctic-embed-m` model embeddings.\n",
        "\n",
        "4.  `Semantic_Arctic_Finetuned`: uses semantic chunking and `Finetuned_arctic` model embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, as outlined in the \"Blueprint for an AI Bill of Rights,\" is a framework designed to ensure that automated systems work for the American people while upholding civil rights, civil liberties, and privacy. It includes principles and guidelines for the responsible use of automated systems, aiming to assist both governments and the private sector in protecting these values. The document emphasizes the importance of evaluating and addressing the harms of automated systems at both individual and community levels.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include the following:\n",
            "\n",
            "1. **Legal Protections**: Algorithmic discrimination may violate existing legal protections based on classifications such as race, gender, age, disability, and other protected categories. \n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are encouraged to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Audits and Impact Assessments**: There should be practices in place for audits and impact assessments to identify potential algorithmic discrimination and provide transparency to the public.\n",
            "\n",
            "4. **Equitable Design and Use**: Automated systems should be designed and used in an equitable way, with safeguards against abuse, bias, and discrimination.\n",
            "\n",
            "5. **Bias Testing**: Some organizations are implementing bias testing as part of their product quality assessment to ensure that products do not perpetuate discrimination.\n",
            "\n",
            "These rights and measures aim to protect individuals in both their physical and digital lives from the impacts of algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how data about you is used, which includes the ability to give or withhold permission for data collection.\n",
            "\n",
            "2. **Built-in Protections**: You should be protected from abusive data practices through built-in protections that are included by default in systems.\n",
            "\n",
            "3. **Reasonable Expectations**: Data collection should conform to reasonable expectations, meaning that only data strictly necessary for a specific context should be collected.\n",
            "\n",
            "4. **Transparency and Reporting**: You should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.\n",
            "\n",
            "5. **Legal Protections**: The use of your data is governed by legal protections that help to protect civil liberties and may include limits on data retention.\n",
            "\n",
            "6. **Consumer Data Privacy Regimes**: Many states have enacted consumer data privacy protection laws to address harms related to data use, although these are not yet standard practices across the United States.\n",
            "\n",
            "These rights aim to ensure that your data privacy is respected and protected.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. These systems must undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective for their intended use. If a system poses safety violations or unintended consequences, it should not be used until the risks are mitigated, which may require rollback or significant modifications to the system.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed consistently and across sectors if an automated system is being used in a way that impacts your rights. This includes receiving adequate explanations for any system impacts or inferences made by AI systems. The reporting should be provided in clear, plain language and in a machine-readable format.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. You should also have access to a person who can quickly consider and remedy any problems you encounter. The determination of appropriateness for opting out should be based on reasonable expectations in a given context, ensuring broad accessibility and protection from harmful impacts. In some cases, a human or alternative may be required by law.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing privacy-enhancing technologies, establishing fine-grained permissions and access control mechanisms, and adhering to conventional system security protocols. Additionally, they should ensure that the use of data is governed by legal protections that help protect civil liberties, including limits on data retention. Organizations can also adopt policies and practices that align with existing laws, such as the Privacy Act of 1974, which requires privacy protections for personal information in federal records systems. Furthermore, they should provide individuals with access to reporting that confirms their data decisions have been respected and assess the potential impact of surveillance technologies on rights, opportunities, or access.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by taking proactive and continuous measures to protect individuals and communities. This includes designing and deploying automated systems with built-in protections against bias, conducting bias testing as part of product quality assessments, and ensuring that governance structures are in place to oversee these systems. Additionally, organizations should be prepared to update the operation of their systems to mitigate any discriminatory effects that may arise.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI can be categorized into several areas:\n",
            "\n",
            "1. **Technical / Model Risks**: This includes risks from malfunction such as confabulation, dangerous or violent recommendations, data privacy issues, value chain and component integration problems, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses malicious uses such as the dissemination of chemical, biological, radiological, and nuclear (CBRN) information or capabilities, data privacy concerns, human-AI configuration issues, and the generation of obscene, degrading, or abusive content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These risks include data privacy, environmental impacts, and intellectual property concerns.\n",
            "\n",
            "Some risks are noted to be cross-cutting between these categories.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "The provided context mentions \"environmental\" as one of the risks associated with generative AI, but it does not provide specific details about the nature or extent of these environmental impacts. Therefore, based on the context, I cannot definitively say whether generative AI has bad environmental impacts.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement governance structures that can oversee and update the operation of AI systems. This includes addressing risks such as algorithmic discrimination, harmful bias, and the potential for misuse by humans. Additionally, a socio-technical perspective should be taken to identify and manage these risks effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is important to implement governance structures that can oversee and update the operation of AI systems to mitigate these biases. Additionally, a socio-technical perspective should be taken to identify and manage AI bias effectively.\n"
          ]
        }
      ],
      "source": [
        "## Testing \n",
        "new_baseline_arctic_original_retrieval_chain, new_baseline_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_baseline_text_splits, # <- NEW baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The context does not provide specific details about the process followed to generate the AI Bill of Rights. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, as outlined in the \"Blueprint for an AI Bill of Rights,\" is a framework designed to ensure that automated systems work for the American people while upholding civil rights, civil liberties, and privacy. It includes principles and guidelines for the responsible use of automated systems, aiming to assist both governments and the private sector in protecting these values. The document emphasizes the importance of evaluating and addressing the harms of automated systems at both individual and community levels.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Legal Protections**: Algorithmic discrimination may violate existing legal protections based on classifications such as race, gender, age, disability, and more. This means you can seek recourse if you believe you have been discriminated against by an automated system.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are encouraged to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination. This includes implementing practices for audits and impact assessments to identify potential biases.\n",
            "\n",
            "3. **Transparency**: There should be transparency in the design and deployment of automated systems, allowing the public to understand how these systems operate and how they mitigate biases.\n",
            "\n",
            "4. **Equitable Design**: Protections should be built into the design, deployment, and ongoing use of automated systems to ensure they operate in an equitable manner.\n",
            "\n",
            "5. **Bias Testing**: Some organizations are already implementing bias testing as part of their product quality assessments, which can lead to changes in products to prevent discrimination.\n",
            "\n",
            "These rights and measures aim to safeguard individuals from discrimination in both their digital and daily lives.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Agency Over Data Use**: You should have control over how data about you is used, which includes the ability to give or withhold permission for data collection.\n",
            "\n",
            "2. **Built-in Protections**: You should be protected from abusive data practices through built-in protections that are included by default in systems.\n",
            "\n",
            "3. **Reasonable Expectations**: Data collection should conform to reasonable expectations, meaning that only data strictly necessary for a specific context should be collected.\n",
            "\n",
            "4. **Transparency and Reporting**: You should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.\n",
            "\n",
            "5. **Legal Protections**: The use of your data is governed by legal protections that help to protect civil liberties and provide limits on data retention in some cases.\n",
            "\n",
            "6. **Consumer Data Privacy Regimes**: Many states have enacted consumer data privacy protection laws to address harms related to data use, although these are not yet standard practices across the United States.\n",
            "\n",
            "These rights aim to ensure that your personal data is handled with respect and care, minimizing risks to your privacy.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. Additionally, these systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective for their intended use. If safety violations or unintended consequences are identified, those systems should not be used until the risks can be mitigated. Ongoing risk mitigation may require significant modifications or rollbacks of launched automated systems.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to be informed about the use of AI systems that impact your rights. This includes receiving adequate explanations regarding how these systems operate and the effects they may have on you. The information should be provided in clear, plain language and in a machine-readable format. Additionally, there are existing notice and explanation requirements in some sectors, and there is a push for consistent application of these requirements across all sectors to ensure the public is aware when automated systems are being used in ways that affect their rights.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. You should also have access to a person who can quickly consider and remedy any problems you encounter. The determination of appropriateness for opting out should be based on reasonable expectations in a given context, ensuring broad accessibility and protection from harmful impacts. In some cases, a human or other alternative may be required by law.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing privacy-enhancing technologies, establishing fine-grained permissions and access control mechanisms, and adhering to conventional system security protocols. Additionally, they should ensure that the use of data is governed by legal protections that help protect civil liberties and provide limits on data retention. Organizations can also adopt laws and policies that promote privacy protections, such as those outlined in the Privacy Act of 1974, which requires limits on data retention and provides individuals with rights regarding their personal information. Furthermore, organizations should ensure that sensitive data is subject to extra oversight and that any reuse of such data is legally authorized and beneficial, with appropriate risk mitigation measures in place.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by taking proactive and continuous measures to protect individuals and communities. This includes designing and deploying automated systems with built-in protections against bias, conducting bias testing as part of product quality assessments, and ensuring that governance structures are in place to oversee these systems. Additionally, organizations should be prepared to update the operation of their systems to mitigate any discriminatory effects that may arise.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "According to NIST, the major risks of generative AI can be categorized into several areas:\n",
            "\n",
            "1. **Technical / Model Risks**: This includes risks from malfunction such as confabulation, dangerous or violent recommendations, data privacy issues, value chain and component integration problems, harmful bias, and homogenization.\n",
            "\n",
            "2. **Misuse by Humans**: This encompasses malicious uses such as the dissemination of chemical, biological, radiological, and nuclear (CBRN) information or capabilities, data privacy violations, human-AI configuration issues, and the generation of obscene, degrading, or abusive content.\n",
            "\n",
            "3. **Ecosystem / Societal Risks**: These risks include data privacy concerns, environmental impacts, and intellectual property issues.\n",
            "\n",
            "Some risks are noted to be cross-cutting between these categories.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "The provided context mentions \"Ecosystem / societal risks (or systemic risks): Environmental\" but does not provide specific details about the environmental impacts of generative AI. Therefore, I don't know if generative AI has bad environmental impacts based on the context provided.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement governance structures that can oversee and update the operation of AI systems. This includes addressing risks such as algorithmic discrimination, harmful bias, and the potential for misuse by humans. Additionally, a socio-technical perspective should be taken to identify and manage these risks effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is important to implement governance structures that can oversee and update the operation of AI systems to mitigate these biases. Additionally, a socio-technical perspective should be taken to identify and manage AI bias effectively.\n"
          ]
        }
      ],
      "source": [
        "new_baseline_arctic_original_retrieval_chain, new_baseline_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_baseline_text_splits, # <- NEW baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The AI Bill of Rights was generated through extensive consultation with the American public. It consists of five principles and associated practices designed to guide the design, use, and deployment of automated systems, ensuring they align with democratic values and protect civil rights, civil liberties, and privacy. The process involved collaboration among various stakeholders, including industry, civil society, researchers, policymakers, technologists, and the public.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, specifically referred to as the \"Blueprint for an AI Bill of Rights,\" is a set of five principles and associated practices designed to guide the design, use, and deployment of automated systems in a way that protects the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems are aligned with democratic values and safeguard civil rights, civil liberties, and privacy. The document was developed through extensive consultation with the American public and serves as a framework for organizations to uphold these values. However, it does not create any legal rights or enforceable benefits against the United States or its entities.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have the right to protection against algorithmic discrimination, which occurs when automated systems contribute to unjustified different treatment or impacts that disfavor individuals based on various protected characteristics, including race, color, ethnicity, sex, religion, age, national origin, disability, and more. Designers, developers, and deployers of automated systems are expected to take proactive measures to protect individuals and communities from such discrimination. Depending on the specific circumstances, algorithmic discrimination may also violate legal protections. Additionally, there should be practices for audits and impact assessments to identify potential algorithmic discrimination and ensure transparency in mitigating biases.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Built-in Protections**: You should be protected from abusive data practices through built-in privacy protections.\n",
            "2. **Agency Over Data Use**: You should have control over how your data is used, including the right to give or withhold permission for data collection.\n",
            "3. **Data Minimization**: Only data that is strictly necessary for a specific context should be collected.\n",
            "4. **Legal Protections**: Your data use is governed by legal protections that help protect civil liberties and provide limits on data retention.\n",
            "5. **Access and Correction**: Under laws like the Privacy Act of 1974, you have a general right to access and correct your data.\n",
            "6. **Transparency and Control**: You are entitled to clear mechanisms to control access to and use of your data, including your metadata, in a proactive and informed manner.\n",
            "\n",
            "These principles aim to ensure that your privacy is respected and that you have a say in how your personal information is handled.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. These systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective for their intended use. Additionally, there should be safeguards in place to prevent the systems from being designed in a way that could foreseeably endanger your safety or the safety of your community. If necessary, there should also be the possibility of not deploying or removing a system from use if it does not meet safety standards.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice regarding the use of AI systems that impact your rights. This includes being informed in advance of the use of such systems and receiving explanations about how and why decisions are made by these systems. The entity responsible for using the automated system should provide documentation that describes the system in plain language, making it easy to understand how it works and how it affects you. Additionally, you should be notified of any changes in the use case or key functionalities of the system.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. You should also have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to a fallback and escalation process to appeal or contest its impacts on you. This human consideration and fallback should be accessible, equitable, effective, and not impose an unreasonable burden on the public. In some cases, a human or other alternative may be required by law.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing built-in protections against abusive data practices, ensuring that individuals have agency over how their data is used. This includes designing systems that conform to reasonable expectations regarding data collection, only collecting data that is strictly necessary for specific contexts, and seeking permission from individuals before collecting their data. Additionally, organizations should connect their data privacy policies and procedures to existing governance frameworks, legal compliance, and risk management activities. They can also document their data curation policies and ensure that their data practices are governed by legal protections that safeguard civil liberties and limit data retention.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key strategies:\n",
            "\n",
            "1. **Bias Testing**: Companies can incorporate bias testing as part of their product quality assessment and launch procedures. This involves evaluating algorithms for potential biases before they are deployed, which can lead to necessary changes or even the decision not to launch a product.\n",
            "\n",
            "2. **Audits and Impact Assessments**: Organizations should conduct regular audits and impact assessments to identify potential algorithmic discrimination. This helps in understanding how automated systems may affect different groups and ensures transparency in addressing any biases.\n",
            "\n",
            "3. **Equitable Design and Deployment**: It is crucial for organizations to design and deploy automated systems in an equitable manner. This means considering the diverse impacts of these systems on various demographics and ensuring that they do not contribute to unjustified different treatment based on protected characteristics.\n",
            "\n",
            "4. **Ongoing Monitoring**: Continuous monitoring of algorithms and their outcomes is essential to detect and mitigate any emerging biases over time.\n",
            "\n",
            "5. **Public Transparency**: Providing transparency to the public regarding the measures taken to mitigate algorithmic discrimination can help build trust and accountability.\n",
            "\n",
            "By adopting these practices, organizations can better protect individuals and communities from algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI systems to create both text-based disinformation and highly realistic \"deepfakes.\" These systems can produce synthetic audiovisual content and photorealistic images that can manipulate human and machine perception. By making subtle changes to text or images, malicious actors can deceive or cause harm to others at scale. Additionally, generative AI can assist in creating compelling imagery and propaganda to support disinformation campaigns, which can enhance the reach and engagement of such campaigns on social media platforms. This capability allows for the deliberate production and dissemination of false or misleading information, thereby eroding public trust in valid evidence and information.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "The provided context does not specify concrete strategies or measures that US entities can take to counter the use of AI for spreading misinformation during elections. Therefore, I don't know how US entities can counter this issue based on the given information.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by applying explainable AI (XAI) techniques as part of ongoing continuous improvement processes. This includes methods such as analysis of embeddings, model compression/distillation, gradient-based attributions, occlusion/term reduction, counterfactual prompts, and word clouds. Additionally, documenting how pre-trained models have been adapted for specific generative tasks can also help mitigate risks related to unexplainable generative AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, the Blueprint for an AI Bill of Rights suggests implementing specific frameworks for ethical use of AI systems, developing laws and policies that protect rights, opportunities, and access, and incorporating technical and sociotechnical approaches. It emphasizes the importance of addressing issues such as bias in AI through careful consideration of datasets, testing, evaluation, and human factors. Additionally, it advocates for community involvement and the reinforcement of societal values in the design and deployment of AI technologies.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI can have bad environmental impacts. The context indicates that generative tasks, such as text summarization, are more energy- and carbon-intensive compared to non-generative tasks like text classification. While methods exist to create smaller models that could reduce environmental impacts during inference, the training and tuning of these models may still contribute to their overall environmental footprint. Additionally, there is currently no agreed-upon method to estimate the environmental impacts of generative AI.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, we can consider the following approaches based on the context provided:\n",
            "\n",
            "1. **Model Distillation and Compression**: Implement methods such as model distillation or compression to create smaller versions of trained models. This can help reduce the environmental impacts during inference.\n",
            "\n",
            "2. **Assess Environmental Impact**: Regularly assess and document the environmental impact and sustainability of AI model training and management activities. This includes evaluating energy and carbon emissions associated with different tasks (e.g., pre-training, fine-tuning, inference).\n",
            "\n",
            "3. **Optimize Resource Utilization**: Focus on optimizing the compute resources used in training and operating GAI models to minimize their energy consumption and environmental footprint.\n",
            "\n",
            "4. **Diverse Training Data**: Ensure that the training data is not overly homogeneous or solely GAI-produced to mitigate risks of model collapse and to promote a more sustainable approach to AI development.\n",
            "\n",
            "5. **Develop Agreed Methods**: Work towards establishing agreed-upon methods to estimate the environmental impacts of generative AI (GAI) systems, which can guide better practices and policies.\n",
            "\n",
            "These strategies can help mitigate the environmental impacts associated with AI technologies.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Content Filters**: Implement content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content. These filters can be rule-based or utilize additional machine learning models to flag problematic inputs and outputs.\n",
            "\n",
            "2. **User Feedback Integration**: Evaluate user-reported problematic content and integrate this feedback into system updates to improve the AI's performance and reduce harmful outputs.\n",
            "\n",
            "3. **Monitoring and Evaluation**: Establish real-time monitoring systems to evaluate the AI-generated content against established guidelines and principles. This includes documenting training data sources to trace the origin of AI-generated content.\n",
            "\n",
            "4. **Bias Mitigation Techniques**: Employ techniques such as re-sampling, re-ranking, or adversarial training to mitigate biases in the generated content.\n",
            "\n",
            "5. **Due Diligence**: Engage in due diligence to analyze AI output for harmful content, misinformation, and other sensitive material.\n",
            "\n",
            "6. **Human Review**: Implement feedback loops between AI system content provenance and human reviewers, updating the system as needed based on their evaluations.\n",
            "\n",
            "By combining these strategies, organizations can better manage the risks associated with AI-generated content.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. This bias can stem from various factors, including systemic, statistical, and human biases, as well as non-representative training data. To prevent this, it is important to address challenges related to datasets, testing and evaluation, and human factors. Implementing preliminary guidance for managing AI bias from a socio-technical perspective can also help mitigate these issues.\n"
          ]
        }
      ],
      "source": [
        "## Testing \n",
        "new_baseline_arctic_finetuned_retrieval_chain, new_baseline_arctic_finetuned_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Finetuned\",\n",
        "                                        embeddings=arctic_finetuned_embeddings, # <- arctic finetuned embeddings\n",
        "                                        embed_dim=arctic_finetuned_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_baseline_text_splits, # <- NEW baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The AI Bill of Rights was generated through extensive consultation with the American public. It consists of five principles and associated practices designed to guide the design, use, and deployment of automated systems, ensuring they align with democratic values and protect civil rights, civil liberties, and privacy. The process involved collaboration among various stakeholders, including industry, civil society, researchers, policymakers, technologists, and the public.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights, as outlined in the \"Blueprint for an AI Bill of Rights,\" is a set of five principles and associated practices designed to guide the design, use, and deployment of automated systems. Its purpose is to protect the rights of the American public in the age of artificial intelligence. The framework was developed through extensive consultation with the American public and aims to ensure that automated systems align with democratic values while safeguarding civil rights, civil liberties, and privacy.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The set of five principles in the AI Bill of Rights includes:\n",
            "\n",
            "1. Safe and Effective Systems\n",
            "2. Algorithmic Discrimination Protections\n",
            "3. Data Privacy\n",
            "4. Notice and Explanation\n",
            "5. Human Alternatives, Consideration, and Fallback\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have the right to protection against algorithmic discrimination, which includes the assurance that automated systems should be designed and used in an equitable manner. This means you should not face unjustified different treatment or negative impacts based on characteristics such as race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Additionally, depending on the specific circumstances, algorithmic discrimination may violate legal protections, and designers, developers, and deployers of automated systems are expected to take proactive measures to prevent such discrimination.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, including:\n",
            "\n",
            "1. **Protection from Abusive Data Practices**: You should be protected from harmful data practices through built-in protections and have agency over how your data is used.\n",
            "\n",
            "2. **Consent and Control**: Designers and developers of automated systems should seek your permission before collecting or using your data. You should have clear mechanisms to control access to and use of your data, including your metadata.\n",
            "\n",
            "3. **Data Minimization**: Only data that is strictly necessary for a specific context should be collected, and there should be limitations on data use and collection.\n",
            "\n",
            "4. **Legal Protections**: The Privacy Act of 1974 provides privacy protections for personal information in federal records systems, including limits on data retention and a general right to access and correct your data.\n",
            "\n",
            "5. **Transparency**: You should be informed about how your data is being collected, used, and shared.\n",
            "\n",
            "These rights are part of a broader framework aimed at protecting your privacy and ensuring that data practices are ethical and transparent. However, it's important to note that the current legal landscape may vary, and comprehensive protections may not be uniformly applied across all contexts.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective systems. Automated systems should be developed with input from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts. These systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring to ensure they are safe and effective based on their intended use. Additionally, there should be safeguards in place to prevent the systems from endangering your safety or the safety of your community, including the possibility of not deploying or removing a system from use if it poses risks.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice regarding the use of AI systems that impact you. This includes:\n",
            "\n",
            "1. **Documentation**: The entity responsible for the AI system must provide documentation that describes how the system works in plain language, making it easy to find and understand.\n",
            "\n",
            "2. **Accountability**: Notices should clearly identify the entities responsible for designing and using the AI system.\n",
            "\n",
            "3. **Timeliness**: You should be notified of the use of automated systems in advance or while being impacted by the technology. Explanations should be available at the time of the decision or soon thereafter, and notices should be kept up-to-date.\n",
            "\n",
            "4. **Explanations**: You are entitled to receive explanations about how and why decisions were made by the AI system, which should be clear and valid.\n",
            "\n",
            "These rights are part of a framework aimed at ensuring transparency and accountability in the use of AI systems.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. You should also have access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. This human consideration should be accessible, equitable, effective, and not impose an unreasonable burden on the public. In some cases, a human or other alternative may be required by law.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adhering to Legal Frameworks**: Following laws such as the Privacy Act of 1974, which mandates privacy protections for personal information, including limits on data retention.\n",
            "\n",
            "2. **Designing with Privacy in Mind**: Ensuring that systems are designed to include built-in protections for users, such as collecting only the data that is strictly necessary for specific contexts and conforming to reasonable expectations of privacy.\n",
            "\n",
            "3. **Obtaining User Consent**: Seeking permission from users before collecting or using their data, and providing clear information about the risks associated with data sharing.\n",
            "\n",
            "4. **Implementing Privacy-Preserving Security Practices**: Utilizing best practices in privacy and security to prevent data leaks, such as employing privacy-enhancing technologies and fine-grained permissions and access control mechanisms.\n",
            "\n",
            "5. **Training and Documentation**: Documenting data curation policies and connecting new data privacy policies to existing governance and compliance frameworks.\n",
            "\n",
            "By integrating these practices, organizations can better protect user data and uphold privacy rights.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key measures:\n",
            "\n",
            "1. **Bias Testing**: Companies can institute bias testing as part of their product quality assessment and launch procedures. This involves evaluating algorithms for potential biases before they are deployed, which can lead to necessary changes or even the decision not to launch a biased product.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination. This includes ensuring that systems are designed and used in an equitable manner.\n",
            "\n",
            "3. **Audits and Impact Assessments**: Organizations should establish practices for conducting audits and impact assessments to identify potential algorithmic discrimination. This helps in providing transparency to the public regarding how biases are mitigated.\n",
            "\n",
            "4. **Ongoing Monitoring**: Continuous monitoring of algorithms and their impacts is essential to ensure that they do not contribute to unjustified different treatment based on protected classifications.\n",
            "\n",
            "By integrating these practices into their operations, organizations can better safeguard against algorithmic discrimination.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI systems to create both text-based disinformation and highly realistic deepfakes. These systems can produce misleading content at scale, either intentionally (disinformation) or unintentionally (misinformation), manipulating human and machine perception through subtle changes to text or images. Additionally, generative AI can assist in creating compelling imagery and propaganda that enhances the reach and engagement of disinformation campaigns on social media platforms. This capability allows malicious actors to impersonate others and disseminate fraudulent content, further eroding public trust in valid information.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "The provided context does not specify how US entities can counter the use of AI to spread misinformation during elections. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by applying explainable AI (XAI) techniques as part of ongoing continuous improvement processes. This includes methods such as analysis of embeddings, model compression/distillation, gradient-based attributions, occlusion/term reduction, counterfactual prompts, and word clouds. Additionally, documenting how pre-trained models have been adapted for specific generative tasks can help mitigate risks related to unexplainable AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, the Blueprint for an AI Bill of Rights suggests several measures, including:\n",
            "\n",
            "1. **Building Protections into Policy and Practice**: Communities, industries, and governments should implement policies that incorporate protections against potential harms from AI technologies.\n",
            "\n",
            "2. **Guiding Principles for Design and Use**: Establishing principles that guide the design, use, and deployment of automated systems to protect the public.\n",
            "\n",
            "3. **Ethical Frameworks**: Developing specific frameworks for the ethical use of AI systems, as seen in initiatives by U.S. government agencies.\n",
            "\n",
            "4. **Transparency and Public Consultation**: Ensuring ongoing transparency in AI systems and engaging with impacted communities to understand potential harms and incorporate their feedback into the design process.\n",
            "\n",
            "5. **Value Sensitive and Participatory Design**: Focusing on design processes that are sensitive to societal values and involve participation from various stakeholders.\n",
            "\n",
            "These steps can help mitigate the risks associated with AI technologies and promote their use in ways that align with societal values.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has been found to have bad environmental impacts. Specifically, emissions for large language model (LLM) inference and generative tasks, such as text summarization, are more energy- and carbon-intensive compared to non-generative tasks like text classification. While methods exist to create smaller versions of trained models to reduce environmental impacts at inference time, the training and tuning of these models can still contribute to their overall environmental footprint. Currently, there is no agreed-upon method to estimate the environmental impacts from generative AI.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, several strategies can be employed:\n",
            "\n",
            "1. **Model Distillation and Compression**: Creating smaller versions of trained models through techniques like model distillation or compression can reduce the energy and carbon intensity associated with inference tasks.\n",
            "\n",
            "2. **Assessing Environmental Impact**: It is important to assess and document the environmental impact and sustainability of AI model training and management activities. This includes evaluating the energy and resource utilization during training, fine-tuning, and inference.\n",
            "\n",
            "3. **Optimizing Resource Use**: Implementing practices that optimize the use of computational resources can help minimize the environmental footprint of AI systems.\n",
            "\n",
            "4. **Developing Agreed-upon Methods**: Establishing standardized methods to estimate the environmental impacts of generative AI (GAI) can help in understanding and mitigating these impacts effectively.\n",
            "\n",
            "By focusing on these areas, the negative environmental effects of AI can be significantly reduced.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, the following measures can be implemented:\n",
            "\n",
            "1. **Content Filters**: Implement content filters to prevent the generation of inappropriate, harmful, false, illegal, or violent content. These filters can be rule-based or utilize additional machine learning models to flag problematic inputs and outputs.\n",
            "\n",
            "2. **User Feedback Integration**: Evaluate user-reported problematic content and integrate this feedback into system updates to improve the AI's performance and reduce harmful outputs.\n",
            "\n",
            "3. **Monitoring and Evaluation**: Establish real-time monitoring systems to evaluate the AI-generated content against established guidelines and principles. This includes documenting training data sources to trace the origin of AI-generated content.\n",
            "\n",
            "4. **Bias Mitigation Techniques**: Use techniques such as re-sampling, re-ranking, or adversarial training to mitigate biases in the generated content.\n",
            "\n",
            "5. **Due Diligence**: Engage in due diligence to analyze AI output for harmful content, misinformation, and other sensitive material.\n",
            "\n",
            "By implementing these strategies, organizations can better manage the risks associated with generative AI and reduce the likelihood of producing harmful content.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. This bias can stem from various factors, including systemic, statistical, and human biases, as well as non-representative training data. To prevent this, it is important to address challenges related to datasets, testing and evaluation, and human factors. Implementing preliminary guidance for managing AI bias from a socio-technical perspective can also help mitigate these issues.\n"
          ]
        }
      ],
      "source": [
        "new_baseline_arctic_finetuned_retrieval_chain, new_baseline_arctic_finetuned_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Baseline_Arctic_Finetuned\",\n",
        "                                        embeddings=arctic_finetuned_embeddings, # <- arctic finetuned embeddings\n",
        "                                        embed_dim=arctic_finetuned_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_baseline_text_splits, # <- NEW baseline chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to gather input from various stakeholders, including impacted communities, industry representatives, technology developers, experts from different fields, and policymakers. This input was collected through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The discussions highlighted both the transformative potential of AI and the necessity of preventing its harms, which played a central role in shaping the Blueprint for an AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to align automated systems with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to incorporate these protections into their policies and practices.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not provide the specific set of five principles in the AI Bill of Rights. Therefore, I don't know the answer.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "Based on the provided context, you have the right to ensure safe and effective systems through the following means:\n",
            "\n",
            "1. **Risk Management**: You can mitigate, transfer, or avoid risks that exceed your organization's risk tolerances, as indicated in the NIST document.\n",
            "\n",
            "2. **Human Oversight**: The context emphasizes the importance of extensive human oversight in settings involving artificial intelligence, which suggests that you have the right to demand such oversight to ensure safety and effectiveness.\n",
            "\n",
            "These rights are part of broader principles aimed at protecting individuals and organizations in the context of artificial intelligence.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by ensuring that data is only used for its intended purpose, as highlighted in the context. For example, if data is provided to an entity like a health insurance company for payment facilitation, it should only be utilized for that specific purpose. Additionally, organizations should implement measures to mitigate, transfer, or avoid risks that exceed their risk tolerances, which can help in managing data privacy effectively. Regular information sharing, change management records, and maintaining version history and metadata can also support organizations in managing data privacy and responding to incidents.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "The context does not provide specific measures to prevent AI from being used to harm society. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement reasonable measures that can prevent, flag, or take other actions in response to outputs that reproduce particular training data. This includes monitoring for outputs that may be plagiarized, trademarked, patented, licensed content, or trade secret material. Regular information sharing, change management records, version history, and metadata can also empower AI actors in responding to and managing AI incidents effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. To prevent this, it is critically important to ensure extensive human oversight in AI systems. Additionally, implementing regular information sharing, change management records, and maintaining version history and metadata can help manage and respond to AI incidents effectively.\n"
          ]
        }
      ],
      "source": [
        "## Testing\n",
        "new_sem_arctic_original_retrieval_chain, new_sem_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_sem_text_splits, # <- NEW semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to gather input from various stakeholders, including impacted communities, industry representatives, technology developers, experts from different fields, and policymakers. This input was collected through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The discussions highlighted both the potential benefits and harms of AI technologies, which played a central role in shaping the Blueprint for an AI Bill of Rights.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to incorporate these protections into their policies and practices.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not provide the specific set of five principles in the AI Bill of Rights. Therefore, I don't know the answer.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "Based on the provided context, you have the right to ensure safe and effective systems through the following means:\n",
            "\n",
            "1. **Mitigation of Risks**: You have the right to mitigate, transfer, or avoid risks that exceed your organization's risk tolerances, as stated in the NIST AI Risk Management Framework.\n",
            "\n",
            "2. **Human Oversight**: The context emphasizes the importance of extensive human oversight in settings involving artificial intelligence, which suggests that you have the right to demand such oversight to ensure safety and effectiveness.\n",
            "\n",
            "These rights are aimed at promoting a safe and effective use of AI systems.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by ensuring that data is only used for its intended purpose, as highlighted in the context. For example, if data is provided to an entity like a health insurance company for payment facilitation, it should only be utilized for that specific purpose. Additionally, organizations should implement measures to mitigate, transfer, or avoid risks that exceed their risk tolerances, which can help in managing data privacy effectively. Regular information sharing, maintaining change management records, and keeping version history and metadata can also support organizations in managing data privacy and responding to incidents.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, regular information sharing, change management records, version history, and metadata can empower AI actors to respond to and manage AI incidents effectively. Additionally, implementing principles and practices outlined in frameworks like an AI Bill of Rights can help ensure that automated systems work for the benefit of the public and protect their rights.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, it is important to implement reasonable measures that can prevent, flag, or take other actions in response to outputs that reproduce particular training data. This includes monitoring for and addressing issues related to plagiarized, trademarked, patented, licensed content, or trade secret material. Regular information sharing, change management records, version history, and metadata can also empower AI actors in responding to and managing AI incidents effectively.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "new_sem_arctic_original_retrieval_chain, new_sem_arctic_original_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Original\",\n",
        "                                        embeddings=arctic_original_embeddings, # <- arctic original embeddings\n",
        "                                        embed_dim=arctic_original_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_sem_text_splits, # <- NEW semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to gather input from various stakeholders, including impacted communities, industry representatives, technology developers, experts across different fields, and policymakers. This input was collected through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The feedback received played a central role in shaping the Blueprint for an AI Bill of Rights, highlighting both the transformative potential of AI and the necessity of preventing its harms.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to implement protections in policy and practice.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The context does not specify the exact set of five principles in the AI Bill of Rights. Therefore, I don't know the specific principles.\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "The context does not specify who led the formulation of the AI Bill of Rights. It mentions that the White House Office of Science and Technology Policy conducted a year-long process to seek input from various stakeholders, but it does not name a specific individual or leader. Therefore, I don't know.\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Protection from Discrimination**: You should not face discrimination by algorithms, and systems should be designed and used in an equitable manner.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are required to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design to identify potential discrimination and its effects on equity.\n",
            "\n",
            "4. **Use of Representative Data**: Systems should use representative data and protect against proxies for demographic features to avoid biased outcomes.\n",
            "\n",
            "5. **Accessibility**: Ensuring accessibility for people with disabilities in the design and development of automated systems.\n",
            "\n",
            "6. **Disparity Testing**: There should be pre-deployment and ongoing disparity testing and mitigation to identify and address any discriminatory impacts.\n",
            "\n",
            "7. **Organizational Oversight**: Clear organizational oversight should be established to monitor and address algorithmic discrimination.\n",
            "\n",
            "8. **Independent Evaluation**: Independent evaluations and plain language reporting, including algorithmic impact assessments and disparity testing results, should be performed and made public whenever possible to confirm these protections.\n",
            "\n",
            "These rights aim to ensure that automated systems do not unjustly treat individuals based on protected classifications such as race, gender, age, disability, and more.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, as outlined in the context provided:\n",
            "\n",
            "1. **Right to Access and Correct Data**: Under the Privacy Act of 1974, you have the right to access your personal information stored in federal records systems and to correct any inaccuracies.\n",
            "\n",
            "2. **Limitations on Data Retention**: The Privacy Act also limits the retention of your data to what is \"relevant and necessary\" for the agency's purpose, which helps protect your data from being held longer than needed.\n",
            "\n",
            "3. **Transparency and Reporting**: Entities that collect, use, or share sensitive data are expected to provide public reports on data security breaches, the types of data shared, and how they assess risks related to sensitive data.\n",
            "\n",
            "4. **Local Control**: Access to sensitive data should be limited based on necessity, meaning that individuals closest to the data subject (like teachers with their students' data) have more access than those who are less proximate.\n",
            "\n",
            "5. **Legal Recourse**: If a federal agency does not comply with the Privacy Act, you have the right to seek legal relief, which may include having your data corrected or receiving monetary damages for adverse effects caused by inaccurate records.\n",
            "\n",
            "6. **Enhanced Protections for Sensitive Domains**: There are additional protections for data related to sensitive domains such as health, employment, and education, which are designed to ensure that such data is used appropriately and only in narrowly defined contexts.\n",
            "\n",
            "These rights are part of a broader framework aimed at protecting your privacy and ensuring that your data is handled responsibly.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective automated systems. This includes the following specific rights:\n",
            "\n",
            "1. **Consultation**: You should be involved in the design, implementation, deployment, acquisition, and maintenance of automated systems, particularly through early-stage consultations that engage diverse communities and stakeholders.\n",
            "\n",
            "2. **Pre-deployment Testing**: Automated systems should undergo thorough testing to identify risks and ensure they are safe and effective for their intended use.\n",
            "\n",
            "3. **Ongoing Monitoring**: There should be continuous monitoring of automated systems to demonstrate their safety and effectiveness, including the ability to mitigate unsafe outcomes.\n",
            "\n",
            "4. **Transparency and Accountability**: The outcomes of safety measures should be made public whenever possible, and independent evaluations should confirm that systems are safe and effective.\n",
            "\n",
            "5. **Protection from Harm**: Automated systems should be designed to proactively protect you from foreseeable harms, including those stemming from unintended uses.\n",
            "\n",
            "6. **Data Use Protections**: You should be safeguarded from inappropriate or irrelevant data use in the design and deployment of automated systems.\n",
            "\n",
            "7. **Possibility of Non-deployment**: There should be mechanisms in place that allow for the possibility of not deploying a system or removing it from use if it is deemed unsafe or ineffective.\n",
            "\n",
            "These rights aim to ensure that automated systems do not endanger your safety or the safety of your community.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice regarding the use of automated systems, including AI systems. This notice should inform you about how and why decisions are made or actions are taken by these systems. Specifically, you can expect:\n",
            "\n",
            "1. **Documentation**: The entity using the automated system must provide easily accessible documentation that describes how the system works and how automated components influence decisions.\n",
            "\n",
            "2. **Accountability**: Notices should clearly identify the responsible entities for both the design and use of the system.\n",
            "\n",
            "3. **Timeliness**: You should be notified in advance of the use of automated systems or while being impacted by them, with explanations available at the time of the decision or shortly thereafter.\n",
            "\n",
            "4. **Clarity**: Notices and explanations should be brief, clear, and assessed for user understanding, ensuring accessibility for individuals with disabilities and appropriate language levels.\n",
            "\n",
            "5. **Tailored Explanations**: Explanations should be tailored to the specific purpose and audience, allowing you to understand the reasoning behind decisions that affect you.\n",
            "\n",
            "These rights are designed to ensure transparency and accountability in the use of AI systems, helping you contest decisions and understand their implications.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This includes having access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. These human alternatives should be accessible, equitable, effective, and not impose an unreasonable burden on the public. Additionally, in sensitive domains such as criminal justice, employment, education, and health, there should be tailored human oversight and consideration for high-risk decisions.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adopting Privacy Frameworks**: Organizations can utilize frameworks like the NIST Privacy Framework, which provides a structured approach to managing privacy risks, identifying privacy goals, and ensuring compliance with applicable laws.\n",
            "\n",
            "2. **Limiting Data Collection**: Organizations should only collect data that is \"relevant and necessary\" for their specific purposes, in line with principles of data minimization.\n",
            "\n",
            "3. **Seeking User Consent**: Organizations should obtain clear and meaningful consent from individuals regarding the collection, use, access, transfer, and deletion of their data. Consent requests should be brief, understandable, and context-specific.\n",
            "\n",
            "4. **Implementing Privacy by Design**: Privacy protections should be integrated into the design of systems and processes from the outset, ensuring that data collection conforms to reasonable expectations and that only necessary data is collected.\n",
            "\n",
            "5. **Enhancing Transparency**: Organizations should provide clear and accessible information about how data is collected, used, and shared, including the potential implications of surveillance technologies.\n",
            "\n",
            "6. **Conducting Impact Assessments**: Before deploying surveillance technologies, organizations should conduct assessments to evaluate potential harms and ensure that privacy and civil liberties are protected.\n",
            "\n",
            "7. **Providing Access and Correction Rights**: Individuals should have the right to access their personal data and contest inaccuracies, as mandated by laws like the Privacy Act.\n",
            "\n",
            "8. **Ensuring Oversight and Accountability**: Organizations should establish mechanisms for oversight of data practices, including reporting on data collection and usage, to ensure compliance with privacy standards.\n",
            "\n",
            "By following these practices, organizations can better protect individual privacy and build trust with their users.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key measures:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct assessments during the design phase of automated systems to ensure they are equitable and do not perpetuate existing biases.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used to train algorithms is representative of diverse populations to avoid biased outcomes.\n",
            "\n",
            "3. **Protection Against Proxies for Demographic Features**: Implement safeguards to prevent the use of demographic proxies that could lead to discriminatory practices.\n",
            "\n",
            "4. **Accessibility for People with Disabilities**: Design and develop systems that are accessible to individuals with disabilities, ensuring inclusivity.\n",
            "\n",
            "5. **Pre-deployment and Ongoing Disparity Testing**: Test algorithms before deployment and continuously monitor their performance to identify and mitigate any disparities in outcomes.\n",
            "\n",
            "6. **Clear Organizational Oversight**: Establish oversight mechanisms within organizations to ensure accountability in the design and deployment of automated systems.\n",
            "\n",
            "7. **Independent Evaluation and Reporting**: Conduct independent evaluations and provide plain language reporting on algorithmic impact assessments, including results from disparity testing and information on mitigation efforts, making this information public whenever possible.\n",
            "\n",
            "By adopting these practices, organizations can work towards minimizing the risk of algorithmic discrimination and promoting equitable outcomes in their automated systems.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI (GAI) systems to produce and disseminate false or misleading content at scale. These systems can create both text-based disinformation and highly realistic synthetic audiovisual content, such as deepfakes. The ability to manipulate text or images subtly allows malicious actors to target specific demographics effectively.\n",
            "\n",
            "Additionally, GAI can assist in creating compelling imagery and propaganda that enhances the reach and engagement of disinformation campaigns on social media platforms. The sophistication of these tools enables the production of fraudulent content that can impersonate others, further complicating the landscape of misinformation. Overall, the capabilities of GAI systems can significantly erode public trust in valid information, leading to harmful downstream effects.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "US entities can counter the use of AI to spread misinformation during elections by implementing several strategies:\n",
            "\n",
            "1. **Fact-Checking Techniques**: Deploy and document fact-checking methods to verify the accuracy and veracity of information generated by generative AI systems, especially when the information comes from multiple or unknown sources.\n",
            "\n",
            "2. **Testing Techniques**: Develop and implement testing techniques to identify content produced by generative AI that may be indistinguishable from human-generated content. This can help in recognizing and flagging potential misinformation.\n",
            "\n",
            "3. **Data Review**: Review and document the accuracy, representativeness, relevance, and suitability of data used at different stages of the AI lifecycle to minimize harmful biases and misinformation.\n",
            "\n",
            "4. **Information Security Practices**: Maintain robust information security practices to protect against the exploitation of AI systems for misinformation campaigns.\n",
            "\n",
            "5. **Public Awareness Campaigns**: Educate the public about the potential for AI-generated misinformation and promote media literacy to help individuals critically evaluate the information they encounter.\n",
            "\n",
            "These measures can help mitigate the risks associated with AI-generated misinformation during elections.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by implementing several strategies:\n",
            "\n",
            "1. **Involve Internal and External Experts**: Regular assessments and updates should include internal experts who did not serve as front-line developers, as well as independent assessors. Engaging domain experts, users, and affected communities can provide valuable insights and feedback.\n",
            "\n",
            "2. **Structured Human Feedback**: Conduct structured human feedback exercises, such as evaluations and GAI red-teaming, in consultation with representative AI actors who have expertise in the context of use. This helps identify potential issues early on.\n",
            "\n",
            "3. **Explainable AI Techniques**: Apply explainable AI (XAI) techniques to analyze and improve the model's outputs. This includes methods like analyzing embeddings, model compression, and counterfactual prompts to enhance understanding and transparency.\n",
            "\n",
            "4. **Document Training Data and Processes**: Maintain thorough documentation of the sources and types of training data, potential biases, and the training process of the models. This transparency can help in identifying and mitigating risks associated with the data.\n",
            "\n",
            "5. **Monitor Pre-trained Models**: Regularly monitor pre-trained models as part of the AI system's maintenance to ensure they are functioning as intended and to catch any emerging issues.\n",
            "\n",
            "6. **Implement Content Filters**: Use content filters to prevent the generation of inappropriate or harmful content, which can help mitigate the risks associated with confabulated outputs.\n",
            "\n",
            "7. **Real-time Monitoring**: Establish real-time monitoring processes to analyze the performance and trustworthiness of generated content, allowing for quick identification of deviations from desired standards.\n",
            "\n",
            "By employing these measures, AI developers can better manage the risks associated with hallucinations in generative AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be implemented:\n",
            "\n",
            "1. **Community Input**: Ensure that community input is included at the beginning of the design process for AI systems. This helps to address the needs and concerns of those who will be affected by the technology.\n",
            "\n",
            "2. **Opt-Out Mechanisms**: Provide ways for individuals to opt out of AI systems and use human-driven alternatives instead, allowing for personal choice and agency.\n",
            "\n",
            "3. **Timeliness of Benefits**: Ensure that benefit payments and services provided through AI systems are timely and accessible to those in need.\n",
            "\n",
            "4. **Transparency and Notice**: Provide clear notice about the use of AI systems, including explanations of how they work and what data they use. This transparency can help build trust and understanding.\n",
            "\n",
            "5. **Bias Testing and Algorithmic Discrimination Protections**: Implement bias testing as part of the product quality assessment and launch procedures to identify and mitigate potential biases in AI systems. Establish protections against algorithmic discrimination to ensure equitable outcomes.\n",
            "\n",
            "6. **Ethical Frameworks**: Develop and adhere to ethical frameworks for the use of AI, as seen in various government agencies that have established principles for responsible AI use.\n",
            "\n",
            "7. **Research and Development Support**: Fund research that focuses on safe, trustworthy, fair, and explainable AI systems, as well as cybersecurity and privacy-enhancing technologies.\n",
            "\n",
            "8. **Regulatory Oversight**: Enact laws and regulations that require AI systems to be validated for bias and transparency before they are deployed, ensuring accountability in their use.\n",
            "\n",
            "By implementing these strategies, the potential harms of AI can be mitigated, promoting its use for positive societal outcomes.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The training, maintenance, and operation of generative AI systems are resource-intensive and can have large energy and environmental footprints. For instance, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive compared to non-generative tasks. While there are methods to reduce environmental impacts, such as model distillation or compression, the training and tuning of these models still contribute to their overall environmental impact.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, the following measures can be taken:\n",
            "\n",
            "1. **Assess Environmental Impact**: Document and assess the environmental impacts of model development, maintenance, and deployment during product design decisions.\n",
            "\n",
            "2. **Measure Resource Consumption**: Measure or estimate the environmental impacts, such as energy and water consumption, for training, fine-tuning, and deploying models. This includes verifying trade-offs between resources used during inference versus those required during training.\n",
            "\n",
            "3. **Evaluate Carbon Offset Programs**: Verify the effectiveness of carbon capture or offset programs related to AI training and applications, and address any concerns regarding green-washing.\n",
            "\n",
            "4. **Diverse Training Data**: Assess the proportion of synthetic to non-synthetic training data to ensure that training data is not overly homogenous or produced by generative AI, which can mitigate concerns of model collapse.\n",
            "\n",
            "5. **Develop Smaller Models**: Explore methods for creating smaller versions of trained models, such as model distillation or compression, to reduce environmental impacts during inference.\n",
            "\n",
            "By implementing these strategies, organizations can work towards minimizing the environmental footprint of AI systems.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, several measures can be implemented:\n",
            "\n",
            "1. **Output Restrictions**: Current systems often restrict model outputs to limit certain types of content. This includes monitoring and controlling responses to specific prompts to prevent harmful recommendations.\n",
            "\n",
            "2. **Robust Testing and Monitoring**: Regularly monitor the effectiveness of risk controls and mitigation plans through methods like red-teaming, field testing, and user feedback mechanisms.\n",
            "\n",
            "3. **Guideline Compliance**: Compare AI system outputs against pre-defined organizational risk tolerance, guidelines, and principles, and review AI-generated content to ensure compliance.\n",
            "\n",
            "4. **Document Training Data**: Maintain transparency by documenting the sources of training data to trace the origin and provenance of AI-generated content.\n",
            "\n",
            "5. **Bias Evaluation**: Evaluate AI content for representational biases and employ techniques such as re-sampling, re-ranking, or adversarial training to mitigate biases in generated content.\n",
            "\n",
            "6. **Engagement in Due Diligence**: Analyze AI output for harmful content, misinformation, and other dangerous material to ensure safety and integrity.\n",
            "\n",
            "7. **User Interaction Studies**: Conduct studies to understand how end users perceive and interact with AI-generated content, assessing whether it aligns with their expectations and how they may act upon the information presented.\n",
            "\n",
            "By implementing these strategies, organizations can better manage the risks associated with AI-generated content and reduce the likelihood of producing harmful outputs.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. The context indicates that AI systems, including generative AI, can perpetuate and amplify harmful biases related to race, gender, disability, and other protected classes. This bias can manifest in various ways, such as underrepresentation in generated content or disparities in performance across different demographic groups.\n",
            "\n",
            "To prevent this bias, the following measures can be taken:\n",
            "\n",
            "1. **Conduct Studies**: Understand how end users perceive and interact with AI-generated content, ensuring it aligns with their expectations and needs.\n",
            "\n",
            "2. **Evaluate Biases**: Use appropriate methodologies, including computational testing and structured feedback, to assess potential biases and stereotypes in AI-generated content.\n",
            "\n",
            "3. **Diverse Training Data**: Ensure that the training data used for AI models is diverse and representative of different demographic groups to minimize bias.\n",
            "\n",
            "4. **Transparency and Accountability**: Implement frameworks that promote transparency in AI systems and hold developers accountable for biased outcomes.\n",
            "\n",
            "5. **Continuous Monitoring**: Regularly monitor AI systems for biased outputs and make necessary adjustments to mitigate any identified biases.\n",
            "\n",
            "By taking these steps, we can work towards reducing bias in AI systems and promoting fairness and inclusivity.\n"
          ]
        }
      ],
      "source": [
        "##Testing\n",
        "new_sem_arctic_finetuned_retrieval_chain, new_sem_arctic_finetuned_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Finetuned\",\n",
        "                                        embeddings=arctic_finetuned_embeddings, # <- arctic finetuned embeddings\n",
        "                                        embed_dim=arctic_finetuned_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_sem_text_splits, # <- NEW semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================\n",
            "=====question number: 0 =============\n",
            "What process was followed to generate the AI Bill of Rights?\n",
            "The process followed to generate the AI Bill of Rights involved extensive consultation with the American public. The White House Office of Science and Technology Policy led a year-long effort to gather input from various stakeholders, including impacted communities, industry representatives, technology developers, experts from different fields, and policymakers. This input was collected through panel discussions, public listening sessions, meetings, a formal request for information, and a publicly accessible email address. The feedback received played a central role in shaping the Blueprint for an AI Bill of Rights, which aims to protect the rights of the American public in the age of artificial intelligence.\n",
            "=================\n",
            "=====question number: 1 =============\n",
            "What is the AI Bill of Rights?\n",
            "The AI Bill of Rights is a framework consisting of five principles and associated practices designed to guide the design, use, and deployment of automated systems in order to protect the rights of the American public in the age of artificial intelligence. It aims to ensure that these systems align with democratic values and safeguard civil rights, civil liberties, and privacy. Developed through extensive public consultation, the framework serves as a national values statement and toolkit for various organizations, including governments and companies, to implement protections in policy, practice, and technological design.\n",
            "=================\n",
            "=====question number: 2 =============\n",
            "What are the set of five principles in the AI bill of Rights?\n",
            "The set of five principles in the AI Bill of Rights includes:\n",
            "\n",
            "1. Safe and Effective Systems\n",
            "2. Algorithmic Discrimination Protections\n",
            "3. Data Privacy\n",
            "4. Notice and Explanation\n",
            "5. Human Alternatives, Consideration, and Fallback\n",
            "=================\n",
            "=====question number: 3 =============\n",
            "Who led the formulation of the AI Bill of Rights?\n",
            "The formulation of the AI Bill of Rights was led by the White House Office of Science and Technology Policy (OSTP).\n",
            "=================\n",
            "=====question number: 4 =============\n",
            "What rights do I have to ensure protection against algorithmic discrimination?\n",
            "You have rights to ensure protection against algorithmic discrimination, which include:\n",
            "\n",
            "1. **Protection from Discrimination**: You should not face discrimination by algorithms, and systems should be designed and used in an equitable manner.\n",
            "\n",
            "2. **Proactive Measures**: Designers, developers, and deployers of automated systems are required to take proactive and continuous measures to protect individuals and communities from algorithmic discrimination.\n",
            "\n",
            "3. **Equity Assessments**: There should be proactive equity assessments as part of the system design to identify potential discrimination and its effects on equity.\n",
            "\n",
            "4. **Use of Representative Data**: Systems should utilize representative data and protect against proxies for demographic features to avoid biased outcomes.\n",
            "\n",
            "5. **Accessibility**: Ensuring accessibility for people with disabilities in the design and development of automated systems is essential.\n",
            "\n",
            "6. **Disparity Testing and Mitigation**: There should be pre-deployment and ongoing disparity testing and mitigation to address any identified biases.\n",
            "\n",
            "7. **Organizational Oversight**: Clear organizational oversight is necessary to ensure compliance with these protections.\n",
            "\n",
            "8. **Independent Evaluation and Reporting**: Independent evaluations and plain language reporting, including algorithmic impact assessments and disparity testing results, should be performed and made public whenever possible to confirm these protections.\n",
            "\n",
            "These rights aim to ensure that automated systems do not contribute to unjustified different treatment based on protected classifications such as race, gender, age, and disability, among others.\n",
            "=================\n",
            "=====question number: 5 =============\n",
            "What rights do I have to ensure that my data stays private?\n",
            "You have several rights to ensure that your data stays private, as outlined in the context provided:\n",
            "\n",
            "1. **Right to Access and Correct Data**: Under the Privacy Act of 1974, you have the right to access your personal information stored in federal records systems and to correct any inaccuracies.\n",
            "\n",
            "2. **Limitations on Data Retention**: The Privacy Act requires that federal agencies only retain data that is \"relevant and necessary\" for their purposes, which helps limit the scope of data retention.\n",
            "\n",
            "3. **Local Control and Access**: Access to sensitive data should be limited based on necessity, meaning that individuals closest to the data subject (like a teacher to a student’s data) have more access than those who are less proximate.\n",
            "\n",
            "4. **Transparency and Reporting**: Entities that collect, use, or share sensitive data are expected to provide public reports on data security lapses, ethical reviews, and any data sold or shared, ensuring transparency in how your data is handled.\n",
            "\n",
            "5. **Legal Recourse**: If a federal agency does not comply with the Privacy Act, you have the right to seek legal relief, which may include having your data corrected or receiving monetary damages for adverse effects caused by inaccurate data.\n",
            "\n",
            "6. **Enhanced Protections for Sensitive Domains**: There are additional protections for data related to sensitive domains such as health, employment, and education, which are recognized as deserving of enhanced privacy protections.\n",
            "\n",
            "These rights are designed to help protect your personal data from unauthorized access and misuse, ensuring that you have control over your own information.\n",
            "=================\n",
            "=====question number: 6 =============\n",
            "What rights do I have to ensure safe and effective systems?\n",
            "You have the right to be protected from unsafe or ineffective automated systems. This includes the following specific rights:\n",
            "\n",
            "1. **Consultation**: You should be involved in the design, implementation, deployment, acquisition, and maintenance of automated systems, particularly through early-stage consultations that engage diverse communities and stakeholders.\n",
            "\n",
            "2. **Pre-deployment Testing**: Automated systems should undergo rigorous testing to identify risks and ensure they are safe and effective for their intended use.\n",
            "\n",
            "3. **Ongoing Monitoring**: There should be continuous monitoring of automated systems to demonstrate their safety and effectiveness, including the ability to mitigate unsafe outcomes.\n",
            "\n",
            "4. **Transparency and Accountability**: The outcomes of safety measures should be made public whenever possible, and independent evaluations should confirm that systems are safe and effective.\n",
            "\n",
            "5. **Protection from Harm**: Automated systems should be designed to proactively protect you from foreseeable harms, including those stemming from unintended uses.\n",
            "\n",
            "6. **Data Use Protections**: You should be safeguarded from inappropriate or irrelevant data use in the design and deployment of automated systems.\n",
            "\n",
            "7. **Possibility of Non-deployment**: There should be a possibility of not deploying a system or removing it from use if it is found to be unsafe or ineffective.\n",
            "\n",
            "These rights aim to ensure that automated systems do not endanger your safety or the safety of your community.\n",
            "=================\n",
            "=====question number: 7 =============\n",
            "What rights do I have to ensure that I am given adequate explanation and notice re the use of AI systems?\n",
            "You have the right to receive clear, timely, understandable, and accessible notice regarding the use of automated systems. This includes:\n",
            "\n",
            "1. **Documentation**: The entity responsible for the automated system must provide documentation that describes how the system works in plain language, including any human components involved.\n",
            "\n",
            "2. **Accountability**: Notices should clearly identify the entities responsible for designing and using the system.\n",
            "\n",
            "3. **Timeliness**: You should be notified of the use of automated systems in advance or while being impacted by the technology, with explanations available at the time of the decision or soon thereafter.\n",
            "\n",
            "4. **Clarity**: Notices and explanations should be brief and assessed for user understanding, ensuring they are accessible to all, including individuals with disabilities.\n",
            "\n",
            "5. **Tailored Explanations**: Explanations should be tailored to the specific purpose and audience, allowing you to understand how and why decisions were made by the automated system.\n",
            "\n",
            "These rights are designed to ensure transparency and accountability in the use of AI systems, allowing you to contest decisions and understand their implications.\n",
            "=================\n",
            "=====question number: 8 =============\n",
            "What rights do I have to ensure recourse to alternatives and remedy problems that I encounter?\n",
            "You have the right to opt out of automated systems in favor of a human alternative, where appropriate. This includes having access to a person who can quickly consider and remedy any problems you encounter. If an automated system fails or produces an error, you should have access to timely human consideration and a fallback and escalation process to appeal or contest its impacts on you. These human alternatives should be accessible, equitable, effective, and not impose an unreasonable burden on the public. Additionally, in sensitive domains such as criminal justice, employment, education, and health, there should be tailored human oversight and consideration for high-risk decisions.\n",
            "=================\n",
            "=====question number: 9 =============\n",
            "How can organizations put data privacy into practice?\n",
            "Organizations can put data privacy into practice by implementing several key strategies:\n",
            "\n",
            "1. **Adopting Legal Frameworks**: Organizations can follow laws such as the Privacy Act of 1974, which requires limits on data retention and provides individuals the right to access and correct their data. This includes retaining only data that is \"relevant and necessary\" for specific purposes.\n",
            "\n",
            "2. **Utilizing Privacy Frameworks**: The NIST Privacy Framework offers a comprehensive approach for managing privacy risks. Organizations can use this framework to identify and communicate their privacy risks and goals, supporting ethical decision-making in the design and deployment of systems, products, and services.\n",
            "\n",
            "3. **Implementing Privacy by Design**: Organizations should integrate privacy protections into their products and services by default. This includes minimizing data collection, ensuring transparency about data use, and allowing users to control their data.\n",
            "\n",
            "4. **Enhancing User Consent Mechanisms**: Consent requests should be clear, brief, and understandable, allowing users to make informed decisions about their data. Organizations should respect user choices regarding data collection, use, access, transfer, and deletion.\n",
            "\n",
            "5. **Conducting Impact Assessments**: Before deploying surveillance technologies, organizations should conduct assessments to evaluate potential harms and ensure that privacy and civil liberties are protected.\n",
            "\n",
            "6. **Providing Transparency and Reporting**: Organizations should offer reporting mechanisms that confirm users' data decisions have been respected and assess the impact of surveillance technologies on their rights and opportunities.\n",
            "\n",
            "7. **Ensuring Compliance with Regulations**: Organizations must comply with applicable laws and regulations regarding data privacy, including reporting requirements related to surveillance and data collection practices.\n",
            "\n",
            "By implementing these practices, organizations can better protect individual privacy and foster trust with their users.\n",
            "=================\n",
            "=====question number: 10 =============\n",
            "How can organizations put into practice protection against algorithmic discrimination\n",
            "Organizations can put into practice protection against algorithmic discrimination by implementing several key measures:\n",
            "\n",
            "1. **Proactive Equity Assessments**: Conduct assessments during the design phase of automated systems to ensure they are equitable and do not perpetuate existing biases.\n",
            "\n",
            "2. **Use of Representative Data**: Ensure that the data used to train algorithms is representative of diverse populations to avoid biased outcomes.\n",
            "\n",
            "3. **Protection Against Proxies for Demographic Features**: Implement safeguards to prevent the use of indirect indicators that could lead to discrimination based on protected characteristics.\n",
            "\n",
            "4. **Accessibility for People with Disabilities**: Design and develop systems that are accessible to individuals with disabilities, ensuring inclusivity.\n",
            "\n",
            "5. **Pre-deployment and Ongoing Disparity Testing**: Test algorithms before deployment and continuously monitor their performance to identify and mitigate any disparities in outcomes.\n",
            "\n",
            "6. **Clear Organizational Oversight**: Establish oversight mechanisms to ensure accountability in the design and deployment of automated systems.\n",
            "\n",
            "7. **Independent Evaluation and Reporting**: Conduct independent evaluations and provide plain language reporting on algorithmic impact assessments, including results from disparity testing and mitigation efforts, making this information public whenever possible.\n",
            "\n",
            "By integrating these practices, organizations can work towards minimizing the risk of algorithmic discrimination and promoting equitable outcomes.\n",
            "=================\n",
            "=====question number: 11 =============\n",
            "How can foreign actors spread misinformation through the use of AI?\n",
            "Foreign actors can spread misinformation through the use of AI by leveraging generative AI (GAI) systems to produce and disseminate false or misleading content at scale. These systems can create both text-based disinformation and highly realistic synthetic media, such as deepfakes, which can manipulate human and machine perception. \n",
            "\n",
            "Malicious actors can use GAI to craft targeted disinformation campaigns aimed at specific demographics, enhancing the sophistication and reach of their efforts. For instance, subtle alterations to text or images can lead to significant misinterpretations, while the ability to generate compelling imagery and propaganda can increase engagement on social media platforms. \n",
            "\n",
            "Additionally, GAI can assist in creating fraudulent content that impersonates others, further complicating the landscape of information integrity. The potential for GAI to produce content that is indistinguishable from human-generated material poses a significant risk, as it can erode public trust in valid information and evidence.\n",
            "=================\n",
            "=====question number: 12 =============\n",
            "How can US entities counter the use of AI to spread misinformation during the elections?\n",
            "The provided context does not specify concrete strategies for US entities to counter the use of AI to spread misinformation during elections. Therefore, I don't know how US entities can specifically counter this issue based on the given information.\n",
            "=================\n",
            "=====question number: 13 =============\n",
            "According to NIST, what are the major risks of generative AI?\n",
            "I don't know.\n",
            "=================\n",
            "=====question number: 14 =============\n",
            "How can AI developers reduce the risk of hallucinations?\n",
            "AI developers can reduce the risk of hallucinations by implementing several strategies, including:\n",
            "\n",
            "1. **Applying Explainable AI (XAI) Techniques**: Utilizing methods such as analysis of embeddings, model compression, gradient-based attributions, and counterfactual prompts can help in understanding and mitigating the risks associated with unexplainable generative AI systems.\n",
            "\n",
            "2. **Monitoring Pre-trained Models**: Regular monitoring and maintenance of pre-trained models used in development can help identify and address issues related to hallucinations.\n",
            "\n",
            "3. **Documenting Model Adaptations**: Keeping detailed records of how pre-trained models have been adapted for specific tasks, including any modifications made, can aid in debugging and understanding the model's behavior.\n",
            "\n",
            "4. **Evaluating Training Data**: Documenting the sources and types of training data, as well as potential biases, can help in assessing the integrity of the outputs generated by the AI system.\n",
            "\n",
            "5. **Integrating User Feedback**: Evaluating user-reported problematic content and incorporating that feedback into system updates can help improve the accuracy and reliability of the AI outputs.\n",
            "\n",
            "6. **Implementing Content Filters**: Using content filters to prevent the generation of inappropriate or false content can help mitigate the risks associated with hallucinations.\n",
            "\n",
            "7. **Real-time Monitoring**: Establishing processes for real-time monitoring of generated content can help identify deviations from desired standards and trigger alerts for human intervention when necessary.\n",
            "\n",
            "By employing these strategies, AI developers can work towards minimizing the occurrence of hallucinations in generative AI systems.\n",
            "=================\n",
            "=====question number: 15 =============\n",
            "What can be done to prevent AI from being used to harm society?\n",
            "To prevent AI from being used to harm society, several measures can be taken:\n",
            "\n",
            "1. **Community Input**: Ensure that community input is integrated at the beginning of the design process for AI systems. This helps to identify potential harms and address them proactively.\n",
            "\n",
            "2. **Opt-Out Mechanisms**: Provide ways for individuals to opt out of AI systems and utilize human-driven alternatives when necessary.\n",
            "\n",
            "3. **Timeliness of Benefits**: Ensure that benefit payments and services provided through AI systems are timely and reliable.\n",
            "\n",
            "4. **Transparency**: Offer clear notice about the use of AI systems, including explanations of how they operate and the data they use. This transparency can help build trust and understanding among users.\n",
            "\n",
            "5. **Algorithmic Discrimination Protections**: Implement protections against algorithmic discrimination by incorporating bias testing and audits into the design and deployment of AI systems. This includes developing standards and best practices to identify and mitigate biases.\n",
            "\n",
            "6. **Public Consultation**: Engage with impacted communities to understand the potential harms of technologies and incorporate their feedback into the design and implementation of AI systems.\n",
            "\n",
            "7. **Focus on Positive Outcomes**: Use technology to enhance benefits for individuals rather than to take away support. Automated decision-making systems should aim to provide positive outcomes for users.\n",
            "\n",
            "By adopting these strategies, the risks associated with AI can be mitigated, and its benefits can be maximized for society.\n",
            "=================\n",
            "=====question number: 16 =============\n",
            "Does generative AI have bad environmental impacts?\n",
            "Yes, generative AI has bad environmental impacts. The training, maintenance, and operation of generative AI systems are resource-intensive and can have large energy and environmental footprints. For instance, training a single transformer large language model (LLM) can emit as much carbon as 300 round-trip flights between San Francisco and New York. Additionally, generative tasks are found to be more energy- and carbon-intensive compared to non-generative tasks. While methods exist to reduce environmental impacts at inference time, the training and tuning of models still contribute to their overall environmental footprint.\n",
            "=================\n",
            "=====question number: 17 =============\n",
            "How can we prevent the bad environmental impact of AI?\n",
            "To prevent the bad environmental impact of AI, the following measures can be taken:\n",
            "\n",
            "1. **Assess Environmental Impact**: Document and assess the environmental impacts of AI model training, maintenance, and deployment. This includes evaluating energy and water consumption associated with these activities.\n",
            "\n",
            "2. **Evaluate Training Data**: Assess the proportion of synthetic to non-synthetic training data to ensure that the training data is not overly homogenous, which can mitigate concerns of model collapse.\n",
            "\n",
            "3. **Measure Resource Consumption**: Measure or estimate the environmental impacts, such as energy and carbon emissions, for training, fine-tuning, and deploying models. It's important to verify trade-offs between resources used during inference versus those required during training.\n",
            "\n",
            "4. **Carbon Capture and Offsetting**: Verify the effectiveness of carbon capture or offset programs related to AI training and applications, and address any concerns regarding green-washing.\n",
            "\n",
            "5. **Develop Smaller Models**: Explore methods for creating smaller versions of trained models, such as model distillation or compression, which could reduce environmental impacts during inference.\n",
            "\n",
            "By implementing these strategies, organizations can work towards minimizing the environmental footprint of AI systems.\n",
            "=================\n",
            "=====question number: 18 =============\n",
            "How can we safeguard against AI generating toxic or harmful content?\n",
            "To safeguard against AI generating toxic or harmful content, several measures can be implemented:\n",
            "\n",
            "1. **Output Restrictions**: Current systems often restrict model outputs to limit certain types of content. This involves setting up filters that prevent the generation of harmful recommendations in response to explicit prompts.\n",
            "\n",
            "2. **Prompt Management**: Addressing the issue of \"jailbreaking,\" where users manipulate prompts to bypass output controls, is crucial. This may involve developing more sophisticated prompt handling techniques to detect and mitigate such attempts.\n",
            "\n",
            "3. **User Interaction Studies**: Conducting studies to understand how end users perceive and interact with AI-generated content can help assess whether the content aligns with user expectations and how they may act upon it.\n",
            "\n",
            "4. **Bias and Stereotype Evaluation**: Evaluating potential biases and stereotypes in AI-generated content using computational testing methods and structured feedback can help identify and mitigate harmful outputs.\n",
            "\n",
            "5. **Monitoring and Response Mechanisms**: Implementing reasonable measures to prevent, flag, or take action in response to harmful outputs is essential. This includes monitoring for outputs that reproduce sensitive or harmful content.\n",
            "\n",
            "By combining these strategies, it is possible to reduce the risk of AI generating toxic or harmful content.\n",
            "=================\n",
            "=====question number: 19 =============\n",
            "Is AI likely to be biased against certain types of people?  If so, how can we prevent this?\n",
            "Yes, AI is likely to be biased against certain types of people. The context highlights that AI systems can perpetuate and amplify harmful biases, particularly against women, racial minorities, and people with disabilities. These biases can manifest in various ways, such as underrepresentation in generated content or discriminatory decision-making based on race, gender, or other protected classes.\n",
            "\n",
            "To prevent this bias, the following measures can be taken:\n",
            "\n",
            "1. **Conduct Studies**: Understand how end users perceive and interact with AI-generated content, ensuring it aligns with their expectations and needs.\n",
            "\n",
            "2. **Evaluate Biases**: Use appropriate methodologies, including computational testing and structured feedback, to assess potential biases and stereotypes in AI-generated content.\n",
            "\n",
            "3. **Diverse Training Data**: Ensure that the training data used for AI systems is diverse and representative of different demographic groups to minimize bias.\n",
            "\n",
            "4. **Transparency and Accountability**: Implement transparent processes for AI development and decision-making, allowing for accountability in cases of bias.\n",
            "\n",
            "5. **Continuous Monitoring**: Regularly monitor AI systems for biased outcomes and make necessary adjustments to algorithms and training data.\n",
            "\n",
            "By implementing these strategies, the risk of bias in AI systems can be mitigated.\n"
          ]
        }
      ],
      "source": [
        "new_sem_arctic_finetuned_retrieval_chain, new_sem_arctic_finetuned_q_and_a = \\\n",
        "    get_vibe_check_on_list_of_questions(collection_name=\"Semantic_Arctic_Finetuned\",\n",
        "                                        embeddings=arctic_finetuned_embeddings, # <- arctic finetuned embeddings\n",
        "                                        embed_dim=arctic_finetuned_embeddings_dimension,\n",
        "                                        prompt=rag_prompt,\n",
        "                                        llm=openai_chat_gpt4omini,\n",
        "                                        text_splits=new_sem_text_splits, # <- NEW semantic chunking\n",
        "                                        list_of_questions=my_test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Summary of The Anecdotal Responses to My Questions Above\n",
        "\n",
        "Anecdotally, I see fewer `I don't know` responses with the finetuned model.  This is for both chunking strategies, compared with the original model.\n",
        "\n",
        "I also see that the finetuned model's are probably the only ones to actually articulate an answer to the question `What are the set of five principles in the AI bill of Rights?`.  Even the OpenAI model embeddings struggle to retrieve the relevant context here and as a result the most common answer is `I don't know`.  But both the finetuned pipelines, baseline chunking as well as semantic chunking, are able to answer the question well.\n",
        "\n",
        "Based on these anecdotal results, I would expect that the finetuned model performs much better than the original model in the full RAGAS evaluations below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate RAG Pipeline Using RAGAS Generated Synthetic Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:51<00:00,  1.56it/s]\n"
          ]
        }
      ],
      "source": [
        "##Testing\n",
        "baseline_arctic_original_results, baseline_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_baseline_arctic_original_retrieval_chain, # <- baseline chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:27<00:00,  2.86it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_original_results, baseline_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_baseline_arctic_original_retrieval_chain, # <- baseline chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [01:17<00:00,  1.04it/s]\n"
          ]
        }
      ],
      "source": [
        "##Testing\n",
        "baseline_arctic_finetuned_results, baseline_arctic_finetuned_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_baseline_arctic_finetuned_retrieval_chain, # <- baseline chunking + arctic finetuned embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:40<00:00,  1.96it/s]\n"
          ]
        }
      ],
      "source": [
        "baseline_arctic_finetuned_results, baseline_arctic_finetuned_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_baseline_arctic_finetuned_retrieval_chain, # <- baseline chunking + arctic finetuned embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:47<00:00,  1.69it/s]\n"
          ]
        }
      ],
      "source": [
        "##Testing\n",
        "sem_arctic_original_results, sem_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_sem_arctic_original_retrieval_chain, # <- semantic chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:22<00:00,  3.57it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_original_results, sem_arctic_original_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_sem_arctic_original_retrieval_chain, # <- semantic chunking + arctic orig embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [01:23<00:00,  1.05s/it]\n"
          ]
        }
      ],
      "source": [
        "##Testing\n",
        "sem_arctic_finetuned_results, sem_arctic_finetuned_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_sem_arctic_finetuned_retrieval_chain, # <- semantic chunking + arctic finetuned embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 80/80 [00:39<00:00,  2.03it/s]\n"
          ]
        }
      ],
      "source": [
        "sem_arctic_finetuned_results, sem_arctic_finetuned_results_df = \\\n",
        "    ragas_pipeline.ragas_eval_of_rag_pipeline(new_sem_arctic_finetuned_retrieval_chain, # <- semantic chunking + arctic finetuned embeddings\n",
        "                                              ragas_test_questions, \n",
        "                                              ragas_test_groundtruths, \n",
        "                                              ragas_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compare The Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>BaselineChunkArcticOrig</th>\n",
              "      <th>BaselineChunkArcticFinetuned</th>\n",
              "      <th>SemanticChunkArcticOrig</th>\n",
              "      <th>SemanticChunkArcticFinetuned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.735739</td>\n",
              "      <td>0.877788</td>\n",
              "      <td>0.290917</td>\n",
              "      <td>0.896071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.869516</td>\n",
              "      <td>0.970543</td>\n",
              "      <td>0.297116</td>\n",
              "      <td>0.969438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.745694</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.391042</td>\n",
              "      <td>0.933403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.728333</td>\n",
              "      <td>0.879167</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric  BaselineChunkArcticOrig  BaselineChunkArcticFinetuned  \\\n",
              "0       faithfulness                 0.735739                      0.877788   \n",
              "1   answer_relevancy                 0.869516                      0.970543   \n",
              "2  context_precision                 0.745694                      0.985000   \n",
              "3     context_recall                 0.728333                      0.879167   \n",
              "\n",
              "   SemanticChunkArcticOrig  SemanticChunkArcticFinetuned  \n",
              "0                 0.290917                      0.896071  \n",
              "1                 0.297116                      0.969438  \n",
              "2                 0.391042                      0.933403  \n",
              "3                 0.204167                      0.916667  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##Testing\n",
        "df_baseline_arctic_original = pd.DataFrame(list(baseline_arctic_original_results.items()), columns=['Metric', 'BaselineChunkArcticOrig'])\n",
        "df_baseline_arctic_finetuned = pd.DataFrame(list(baseline_arctic_finetuned_results.items()), columns=['Metric', 'BaselineChunkArcticFinetuned'])\n",
        "df_merged_arctic_baseline = pd.merge(df_baseline_arctic_original, df_baseline_arctic_finetuned, on='Metric')\n",
        "\n",
        "df_sem_arctic_original = pd.DataFrame(list(sem_arctic_original_results.items()), columns=['Metric', 'SemanticChunkArcticOrig'])\n",
        "df_sem_arctic_finetuned = pd.DataFrame(list(sem_arctic_finetuned_results.items()), columns=['Metric', 'SemanticChunkArcticFinetuned'])\n",
        "df_merged_arctic_sem = pd.merge(df_sem_arctic_original, df_sem_arctic_finetuned, on='Metric')\n",
        "\n",
        "df_all_merged = pd.merge(df_merged_arctic_baseline, df_merged_arctic_sem, on='Metric')\n",
        "\n",
        "df_all_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>BaselineChunkArcticOrig</th>\n",
              "      <th>BaselineChunkArcticFinetuned</th>\n",
              "      <th>SemanticChunkArcticOrig</th>\n",
              "      <th>SemanticChunkArcticFinetuned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.709273</td>\n",
              "      <td>0.892459</td>\n",
              "      <td>0.223940</td>\n",
              "      <td>0.887186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.869910</td>\n",
              "      <td>0.968701</td>\n",
              "      <td>0.296473</td>\n",
              "      <td>0.974746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.699861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.408750</td>\n",
              "      <td>0.956736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric  BaselineChunkArcticOrig  BaselineChunkArcticFinetuned  \\\n",
              "0       faithfulness                 0.709273                      0.892459   \n",
              "1   answer_relevancy                 0.869910                      0.968701   \n",
              "2  context_precision                 0.699861                      1.000000   \n",
              "3     context_recall                 0.720000                      0.875000   \n",
              "\n",
              "   SemanticChunkArcticOrig  SemanticChunkArcticFinetuned  \n",
              "0                 0.223940                      0.887186  \n",
              "1                 0.296473                      0.974746  \n",
              "2                 0.408750                      0.956736  \n",
              "3                 0.204167                      0.916667  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_baseline_arctic_original = pd.DataFrame(list(baseline_arctic_original_results.items()), columns=['Metric', 'BaselineChunkArcticOrig'])\n",
        "df_baseline_arctic_finetuned = pd.DataFrame(list(baseline_arctic_finetuned_results.items()), columns=['Metric', 'BaselineChunkArcticFinetuned'])\n",
        "df_merged_arctic_baseline = pd.merge(df_baseline_arctic_original, df_baseline_arctic_finetuned, on='Metric')\n",
        "\n",
        "df_sem_arctic_original = pd.DataFrame(list(sem_arctic_original_results.items()), columns=['Metric', 'SemanticChunkArcticOrig'])\n",
        "df_sem_arctic_finetuned = pd.DataFrame(list(sem_arctic_finetuned_results.items()), columns=['Metric', 'SemanticChunkArcticFinetuned'])\n",
        "df_merged_arctic_sem = pd.merge(df_sem_arctic_original, df_sem_arctic_finetuned, on='Metric')\n",
        "\n",
        "df_all_merged = pd.merge(df_merged_arctic_baseline, df_merged_arctic_sem, on='Metric')\n",
        "\n",
        "df_all_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways from these results\n",
        "\n",
        "1.  Finetuning helps across the board.  It obviously starts with retrieval.  Regardless of the chunking strategy used, finetuning helps to improve retrieval-based measures like context_precision and context_recall tremendously.  The improvements are extremely significant!\n",
        "\n",
        "2.  The improvements in retrieval carry over to the generation realm.  Both measures – faithfulness and answer_relevancy – are significantly improved.  The improvements are much more stark with semantic chunking, where the original model performs particularly poorly.\n",
        "\n",
        "3.  Comparing the two finetuning results above with those of OpenAI embeddings shows that this modest amount of finetuning allows the model to achieve the same level of performance across all these measures, an incredible feat indeed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendation for the Demo App\n",
        "\n",
        "No hesitation in recommending the `finetuned_arctic` finetuned model embeddings in the RAG pipeline that is used in the demo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which one is best and why?\n",
        "-------------------------\n",
        "Overall, the “finetuned_arctic” model embeddings are quite good.  \n",
        "1.  The RAGAS metrics show that their performance is at about the same level as the OpenAI embeddings.  \n",
        "2.  Further, some anecdotal results on test-questions (documented n my notebook) show that the finetuned model is better able to grasp the nuances of the content of the documents in the collection.\n",
        "\n",
        "RECOMMENDATION\n",
        "--------------\n",
        "I would recommend using the ‘finetuned-arctic’ model embeddings in the final version of the app.  In addition to the points above, given that the purpose of the app is to showcase the advances in AI, it makes sense to use the “partially homegrown” embeddings as it can be another illustration of the reach of AI and its potential.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STORY TO CEO\n",
        "------------\n",
        "\n",
        "##### Preamble\n",
        "\n",
        "In focus groups as well as in water-cooler conversations, many employees have shared that they’d like to understand how AI is evolving.  And, how we as a company can arm ourselves with the knowledge of AI’s potential but also its risks.\n",
        "\n",
        "##### Rapid progress\n",
        "\n",
        "Most people, even experts in the field, were caught by surprise at the rapid progress made in the field in the past few years.  Partly due to the sheer pace of innovative work, but also due to the statistical machinery deployed in these models, we have to move thoughtfully but also rapidly to understand the potential of AI as well as its drawbacks.\n",
        "\n",
        "##### What we’ve done\n",
        "\n",
        "What better way to help us all understand the implications of AI than \"use AI to answer questions about AI\"?  We in Technology have worked hard to create a chatbot.  We've used a few key policy and framework proposals from the US government that this chatbot can search for a response to employees’ questions around the risks of AI, how to measure AI risks, and best practices to manage these risks in an enterprise setting.\n",
        "\n",
        "##### What we’d like to do\n",
        "\n",
        "We’d like to roll out the chatbot to at least 50 different internal stakeholders over the next month.  Our own application, just like the recent advances in AI, are somewhat brittle.  Occasionally, the chatbot may respond with \"I don't know\".  If it does that, try a more specific variation of your question.  Our chatbot is only designed to answer questions about AI’s risks, framework for measuring and managing its risks and mitigating/reducing the likelihood of adverse societal outcomes from poor management of AI tools and models.  To that end, we’d like the stakeholders that we recruit to help us on this double mission: educate themselves on the risks and educate themselves on how we as a company can adopt ideas from AI into our own business units.  And, in turn, educate us all!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Incorporate New Information Into Our RAG Pipeline\n",
        "\n",
        "In this application, I used an in-memory instance of the vector database to store content from the two PDFs provided for this assignment.  If we need to add more documents with the exact same pipeline, we will literally need to re-instantiate the entire pipeline by rebuilding the vector database: (a) because the vector DB is in memory and (b) there is a single monolithic block of code that does everything from the creation of the vector indexing as well as the RAG querying.  Clearly this is not a scalable way of doing things.\n",
        "\n",
        "How we can augment this approach:\n",
        "\n",
        "a.\t*Implement persistent on-disk vector databases.*   All major vector database providers offer this capability.\n",
        "\n",
        "b.\t*Build separate pipelines to manage the process to ingest documents and other information into vector databases.*   Separate this part of the pipeline from the part that deals with querying the database (eg RAG applications).\n",
        "\n",
        "c.\t*Improve the architecture of the retrieval process itself.*  For example, if there are new versions of previously released documents, then we may need our vector DB to maintain, for audit and other reasons, the older versions of these documents.  In that case, we can put the metadata to work – e.g., identify documents by their release date, or date-added to our vector DB.  Additionally, use metadata in the retrieval process e.g., we select out more recent versions of documents to search for and phase out older documents from the search and retrieval process.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Clyykfe6xOIo"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00afceb39c074975b6b88d6d0d4d2901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9932859168ad436e9aeef09279b534b1",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_474f44771cb04a4693585273a03a5548",
            "value": 95
          }
        },
        "0267d8c4d9cc48b0a4d60d206de62a91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09c3173c05f54539ae025937b1525e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0be98b57b4894cf9a92818ae1dd72976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda4d1ec0f0043c8b4d254a4ada3e9bf",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b24ed8b36764c39aef39c92430fdc1d",
            "value": 20
          }
        },
        "126c30cc07c4452ab73fefce09dab617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1535c2c75a104f3abb262c5fb7859c14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b24ed8b36764c39aef39c92430fdc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c0f9aeab5de4e32af8bfef423a64f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0267d8c4d9cc48b0a4d60d206de62a91",
            "placeholder": "​",
            "style": "IPY_MODEL_126c30cc07c4452ab73fefce09dab617",
            "value": "Generating: 100%"
          }
        },
        "1e2026abc1314d3caf37d74af7a407e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32bc4bb09af4ac5a608e56f87317596",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b53095cea92740dfb967120a77310283",
            "value": 95
          }
        },
        "22c5f6324de545ba814402c3f71d84f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353b6b9a974048499d854774fe4c882c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2e50139c234d19ac3e32515e575883": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "474f44771cb04a4693585273a03a5548": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d9ba78dc78040f494df9122ddc7ba1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4e76e5d4fba404a9ed4ff059f3a0c04",
              "IPY_MODEL_1e2026abc1314d3caf37d74af7a407e7",
              "IPY_MODEL_fb306876e3244dc69312e2af46c4da02"
            ],
            "layout": "IPY_MODEL_b319ae78e30d437c81f07d5a062ba805"
          }
        },
        "63d6044414e24c5ea55efa925f7a3b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754827da55fa4240bce3710048d1645b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd96dd318c1b4e1481c039321e052081",
            "placeholder": "​",
            "style": "IPY_MODEL_3b2e50139c234d19ac3e32515e575883",
            "value": "embedding nodes: 100%"
          }
        },
        "764b7b6827c9437b90c9c948b9f1037b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "771597df670f417794f66408b05a7eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ab80823e1344b638ddd1646367a6ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b61421d62964b00ba440ecba21f4b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353b6b9a974048499d854774fe4c882c",
            "max": 1248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09c3173c05f54539ae025937b1525e90",
            "value": 1248
          }
        },
        "8025a0f161d3475794daa9cd88209d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "824fe37b12d4414a9376e266ddd086f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9496fc3f26cb42ec9ace36175eb14906",
            "placeholder": "​",
            "style": "IPY_MODEL_8025a0f161d3475794daa9cd88209d5c",
            "value": "Evaluating: 100%"
          }
        },
        "90af75e58cef440a8d38ee6621e0f4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f2e2d3123c4cd88d7c5755342ae154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9496fc3f26cb42ec9ace36175eb14906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9932859168ad436e9aeef09279b534b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a40d4ba626f4563b062a5765325d8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c0f9aeab5de4e32af8bfef423a64f3b",
              "IPY_MODEL_0be98b57b4894cf9a92818ae1dd72976",
              "IPY_MODEL_c7550f460273484a913d211381630626"
            ],
            "layout": "IPY_MODEL_ba8f638b7f6343d9b07cce6e54e9be1c"
          }
        },
        "9ccac42dd9f04713b0ed9fe09c35b5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "b319ae78e30d437c81f07d5a062ba805": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53095cea92740dfb967120a77310283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba8f638b7f6343d9b07cce6e54e9be1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96dd318c1b4e1481c039321e052081": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bda4d1ec0f0043c8b4d254a4ada3e9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d31c6cf07143aea1bbe76aa13fbca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_754827da55fa4240bce3710048d1645b",
              "IPY_MODEL_7b61421d62964b00ba440ecba21f4b52",
              "IPY_MODEL_c67b66e1f2d34ce4b10789fc2fca5843"
            ],
            "layout": "IPY_MODEL_9ccac42dd9f04713b0ed9fe09c35b5b0"
          }
        },
        "c67b66e1f2d34ce4b10789fc2fca5843": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faa998b429774e4eb6aaaa5477bb6977",
            "placeholder": "​",
            "style": "IPY_MODEL_7ab80823e1344b638ddd1646367a6ce6",
            "value": " 1248/1248 [07:00&lt;00:00, 49.86s/it]"
          }
        },
        "c7550f460273484a913d211381630626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1535c2c75a104f3abb262c5fb7859c14",
            "placeholder": "​",
            "style": "IPY_MODEL_92f2e2d3123c4cd88d7c5755342ae154",
            "value": " 20/20 [01:17&lt;00:00, 12.75s/it]"
          }
        },
        "ce0b10aca9064bc092cf3305eb0dab04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ced3689d335c4f1ca62d39b908d6cb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_824fe37b12d4414a9376e266ddd086f5",
              "IPY_MODEL_00afceb39c074975b6b88d6d0d4d2901",
              "IPY_MODEL_e8c20cb22ecb40dbaf61959fc7d087cb"
            ],
            "layout": "IPY_MODEL_771597df670f417794f66408b05a7eb9"
          }
        },
        "d020211480b149cab1761b14ae631eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e32bc4bb09af4ac5a608e56f87317596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e76e5d4fba404a9ed4ff059f3a0c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c5f6324de545ba814402c3f71d84f1",
            "placeholder": "​",
            "style": "IPY_MODEL_764b7b6827c9437b90c9c948b9f1037b",
            "value": "Evaluating: 100%"
          }
        },
        "e8c20cb22ecb40dbaf61959fc7d087cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90af75e58cef440a8d38ee6621e0f4d1",
            "placeholder": "​",
            "style": "IPY_MODEL_ce0b10aca9064bc092cf3305eb0dab04",
            "value": " 95/95 [00:30&lt;00:00,  1.25it/s]"
          }
        },
        "faa998b429774e4eb6aaaa5477bb6977": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb306876e3244dc69312e2af46c4da02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d020211480b149cab1761b14ae631eb1",
            "placeholder": "​",
            "style": "IPY_MODEL_63d6044414e24c5ea55efa925f7a3b56",
            "value": " 95/95 [00:24&lt;00:00,  1.20it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
